

We begin our synthesis journey by designing a synthesis tool for the
linear-base target language we introduced in chapter~\ref{chapter:background}.
Recall from~\ref{chapter:intro} that we can derive a synthesis judgement as an
inversion of our typing judgement.
\jnote{blah blah blah}

To do this we must overcome the problem of \textit{resource management}. This
issue was touched on in~\ref{chapter:intro}. In
section~\ref{sec:resource-management}, we expand on the problem and provide an
overview of its history. We identify two feasible approaches to resource
management named \textit{additive} and \textit{subtractive}, and implement a
synthesis calculus for both in section~\ref{sec:core-synth-calculi}. Both
approaches follow a similar structure; inverting the typing rules to derive a
a set of synthesis rules, yet they differ significantly in how resources are
managed.

These differences carry implications with regard to performance and
implementation. Both calculi are implemented as part of a synthesis tool for Granule~\cite{}. Having outlined these two solutions to the resource-management problem, we then
evaluate the performance of our implementations against a set of benchmarks.


\section{A practical target language}
Our linear base calculus presented in chapter~\ref{chapter:background} contains
only the absolute esssential core of a functional programming
language with graded modalities. We extend the syntax with multiplicatve
product types $\otimes$, additive coproduct types $\oplus$, and a multiplicative unit
$1$. The syntax for these extensions is given by the following grammar, which
extends the linear-base syntax of section~\ref{sec:linear-base}:
\begin{align*}
\hspace{-0.8em} [[ t ]] ::= \;
       & [[ x ]]
  \mid [[ \x . t ]]
  \mid [[ t1 t2 ]]
  \mid [[ [t] ]]
  \mid [[ let [ x ] = t1 in t2 ]]
\mid [[ pair t1 t2 ]]
  \mid [[ letpair x1 x2 = t1 in t2 ]] \\
\hspace{-0.9em}  \mid \; & () \mid [[ let () = t1 in t2 ]]
\mid [[ inl t ]] \mid [[ inr t ]] \mid \textbf{case} \ t_{1}\ \textbf{of}\ \textbf{inl}\ x_{1} \rightarrow t_{2};\ \textbf{inr}\ x_{2} \rightarrow t_{3}
{\small{\tag{terms}}}
\end{align*}
We use the syntax $()$ for the inhabitant of  the
multiplicative unit $1$. Pattern matching via a $\textbf{let}$
is used to eliminate products and unit types; for sum types,
$\textbf{case}$ is used to distinguish the constructors.

\begin{figure}[H]
\begin{align*}
\hspace{-0.5em}
  \begin{array}{c}
\inferrule*[right = Pair]
  {[[ G1 |- t1 : A ]] \\ [[ G2 |- t2 : B ]]}
  {[[ G1 + G2 |- pair t1 t2 : Tup A B]]}
\\[0.9em]
\inferrule*[right = LetPair]
  {[[ G1  |- t1 : Tup A B ]] \;\; [[ G2, x1 : A, x2 : B |- t2 : C ]]}
  {[[ G1 + G2 |- letpair x1 x2 = t1 in t2 : C  ]]}
\\[0.9em]
\inferrule*[right = Inl]
  {[[ G |- t : A ]]}
  {[[ G |- inl t : Sum A B ]]}
\;\;\;
\inferrule*[right = Inr]
  {[[ G |- t : B ]]}
  {[[ G |- inr t : Sum A B]]}
\\[0.9em]
\inferrule*[right = Case]
  {[[ G1 |- t1 : Sum A B ]] \\ [[ G2, x1 : A |- t2 : C]] \\ [[ G3, x2 : B |- t3 : C]]}
    {\Gamma + (\Gamma_{2} \sqcup \Gamma_{3}) \vdash\ \textbf{case} \ t_{1}\ \textbf{of}\ \textbf{inl}\ x_{1} \rightarrow t_{2};\ \textbf{inr}\ x_{2} \rightarrow t_{3} : C }
\\[0.9em]
\inferrule*[right = 1]
 {\quad}{[[ . |- () : Unit ]]}
\;\;\;
\inferrule*[right = Let$1$]
 {[[G1 |- t1 : Unit ]] \quad [[ G2 |- t2 : A ]]}
 {[[ G1 + G2 |- let () = t1 in t2 : A ]]}
\end{array}
\end{align*}
\vspace{-1.25em}
  \caption{Typing rules of for $\otimes$, $\oplus$, and $1$}
\label{fig:typing-prod-sum-unit}
 \end{figure}

Figure~\ref{fig:typing-prod-sum-unit} gives the typing rules.  Rules
for multiplicative products (pairs) and additive coproducts (sums) are routine, where
pair introduction ($\textsc{Pair}$)
adds the contexts used to type the pair's constituent subterms. Pair
elimination ($\textsc{LetPair}$) binds a pair's components to two
linear variables in the scope of the body $[[t2]]$. The
$\textsc{Inl}$ and $\textsc{Inr}$ rules handle the typing of
constructors for the sum type $[[Sum A B]]$. Elimination of sums
($\textsc{Case}$) takes the least upper bound (defined above) of the contexts used to
type the two branches of the case.

In the typing of $\mathbf{case}$ expressions, the \emph{least-upper
  bound} of the two contexts used to type each branch is used, defined:

\begin{definition}[Partial least-upper bounds of
  contexts]\label{def:context-lub}
For all $[[ G1 ]]$, $[[ G2 ]]$:
\begin{align*}
\label{def:lub}
[[G1]] \sqcup [[G2]] =
%%
\left\{\begin{matrix}
\begin{array}{lll}
%% Both empty case
\emptyset
  & [[ G1 ]] = \emptyset & \wedge \; [[ G2 ]] = \emptyset
\\
%
%% Left empty
(\emptyset \sqcup [[ G2' ]]), [[ x : [ A ] {lub 0 s} ]]
  & [[ G1 ]] = \emptyset & \wedge \; [[G2]] = [[ G2',x : [A] s]]
\\
%
%% Left is left linear
([[G1']] \sqcup [[(G2',G2'')]]), [[x : A]]
 & [[G1]] = [[{G1', x : A} ]] & \wedge \; [[ G2 ]] = [[ {G2', x : A},, G2'' ]]
\\
%
%% Left is graded
([[G1']] \sqcup [[(G2',G2'')]]), [[x : [A] {lub r s}]]\;\;
 & [[G1]] = [[ G1',x : [A] r]] & \wedge \; [[ G2 ]] = [[{G2', x : [A] s}, G2'']]
\end{array}
\end{matrix}\right.
\end{align*}
where $r\!\sqcup\!s$ is the least-upper bound of grades $[[r]]$
and $[[s]]$ if it exists, derived from $\sqsubseteq$.
\end{definition}
%
As an example of the partiality of $\sqcup$, if one branch of a \textbf{case} uses a linear variable,
then the other branch must also use it to maintain linearity overall,
otherwise the upper-bound of the two contexts for these branches is not defined.

 With these extensions in place, we now have the
 capacity to write more idiomatic functional programs in our target language.
 As a demonstration of this, and to showcase how graded modalities interact with
 these new type extensions, we provide two further examples of different graded
 modalities which complement these new types.

\begin{example}%[Intervals]
\label{exm:or3}
Exact usage analysis is less useful when control-flow is involved, e.g.,
eliminating sum types where each control-flow branch
uses variables differently. The above $\mathbb{N}$-semiring can be imbued with
a notion of \emph{approximation} via less-than-equal ordering, providing
upper bounds. A more expressive semiring is that of
natural number intervals~\cite{DBLP:journals/pacmpl/OrchardLE19}, given by pairs
$\mathbb{N} \times \mathbb{N}$ written $[[ Intrv r s ]]$ here for
the lower-bound $r \in \mathbb{N}$ and upper-bound usage $s \in
\mathbb{N}$ with $0 = [[ Intrv 0 0 ]]$ and $1 = [[ Intrv 1 1 ]]$,
addition and multiplication defined pointwise,
and ordering $[[ Intrv r s ]] \sqsubseteq [[ Intrv r' s' ]] = [[ r' ]] \leq [[ r ]]
\wedge [[ s ]] \leq [[ s' ]]$. Thus a coproduct elimination function
can be written and typed:
%
\begin{align*}
\oplus_e & : [[ {[] {Intrv 0 1} (A -o C)} -o {{[] {Intrv 0 1} (B -o C)} -o {(Sum A B) -o C}} ]] \\
\oplus_e & =
\lambda x' . \lambda y' . \lambda z. \textbf{let}\ [x] = x'\ \textbf {in}\
\textbf{let}\ [y] = y'\ \textbf{in}\ \textbf{case}\ z\ \textbf{of}\
\textbf{inl}\ u \rightarrow x\ u\ |\ \textbf{inr}\ v\ \rightarrow y\ v
\end{align*}
\end{example}

\begin{example}%[Information flow]
\label{exm:security}
%
Graded modalities can capture a form of information-flow
security, tracking the flow of labelled data through a
program~\cite{DBLP:journals/pacmpl/OrchardLE19},
with a lattice-based semiring on
$\mathcal{R} = \{[[ Irrelevant ]] \sqsubseteq [[ Private ]] \sqsubseteq  [[ Public ]]\}$
where $0 = [[ Irrelevant ]]$, $1 = [[ Private ]]$, $+ = \sqcup$ and
if $r = [[ Irrelevant ]]$ or $s = [[ Irrelevant ]]$ then $[[r * s ]] = [[
Irrelevant ]]$ otherwise $[[ r * s ]] = \sqcup$. This
allows the following well-typed program, eliminating a pair of
$[[ Public ]]$ and $[[ Private ]]$ security values, picking the left
one to pass to a continuation expecting a $[[ Public ]]$ input:
%
\begin{align*}
\textit{noLeak} & : [[ (Tup {[] Public A} {[] Private A}) -> {({[]
                  Public (Tup A Unit)} -> B) -> B} ]] \\
\textit{noLeak} & = [[ \z . {\z' . {letpair x' y' = z in {let [x] = x' in
                  {let [y] = y' in {z' [ pair x () ]}}}}} ]]
\end{align*}
\end{example}


\section{The resource management problem}
\label{sec:resource-management}

Chapter~\ref{chapter:introduction} discussed a rule for synthesising pairs and
highlighted how graded types could be use to control the number of times
assumptions are used in the synthesising term. In a linear or graded
context, synthesis needs to handle the problem of \emph{resource management}~\cite{harlandpym,CERVESATO2000133}: how do we give a resourceful accounting to the context during synthesis
so that we respect its constraints. Before explicating our synthesis approach,
we give an overview of the resource management problem here.

Chapter~\ref{chapter:introduction} considered (Cartesian) product types
$\times$, but in our target language we switch to the \emph{multiplicative product} of
linear types, given in figure~\ref{fig:typing-prod-sum-unit}%
Each subterm is typed by a different context $[[ G1 ]]$ and $[[ G2 ]]$ which are then combined via \emph{disjoint} union: the pair cannot be formed if variables are shared between $[[ G1 ]]$ and $[[ G2 ]]$. This prevents the structural behaviour of \emph{contraction} (where a variable appears in multiple subterms). Na\"{i}vely inverting this typing rule into a synthesis rule yields:
%
\begin{align*}
  \inferrule*[Right=$\otimes_{\textsc{Intro}}$]{ \Gamma_1 \vdash [[ A ]] \Rightarrow [[ t1 ]] \\ \Gamma_2 \vdash [[ B ]] \Rightarrow [[ t2 ]]}{ \Gamma_1, \Gamma_2 \vdash [[ A ]] \otimes [[ B ]] \Rightarrow ([[ t1 ]], [[ t2 ]] )}
\end{align*}
%
As a declarative specification, the $\otimes_{\textsc{Intro}}$ synthesis rule is sufficient.
However, this rule embeds a considerable amount of non-determinism
when considered from an algorithmic perspective. Reading `clockwise'
starting from the bottom-left, given some context $[[ G ]]$ and a goal
$A \otimes B$, we have to split the context into disjoint subparts $[[ G1 ]]$ and $[[ G2
]]$ such that $[[ G ]] = [[ G1 , G2 ]]$ in order to pass the $[[ G1
]]$ and $[[ G2 ]]$ to the subgoals for $A$ and $B$. For a context of
size $n$ there are $2^n$ possible such partitions! This quickly
becomes intractable. Instead, Hodas and
Miller developed a technique for linear logic
programming~\cite{HODAS1994327}, refined by Cervasto et
al.~\cite{CERVESATO2000133}, where proof search for linear logic has
both an \emph{input context} of available resources and
an \emph{output context} of the remaining resources, which we
write as judgements of the form $[[ G |- A =>- t | G' ]]$ for input
context $[[ G ]]$ and output context $[[ G' ]]$. Synthesis for multiplicative
products then becomes:
%
\begin{align*}
  \inferrule*[right=$\otimes_{\textsc{Intro}}^{-}$]{\Gamma_1 \vdash [[ A ]] \Rightarrow^- [[t1]] \ |\ \Gamma_{2} \\ \Gamma_{2} \vdash [[ B ]] \Rightarrow^- [[ t2 ]] \ | \ \Gamma_{3} }{ \Gamma_1 \vdash [[ A Prod B ]] \Rightarrow^- [[ pair t1 t2 ]] \ | \ \Gamma_{3}}
\end{align*}
%
where the remaining resources after synthesising for $A$ the first term $[[ t1 ]]$ are
$[[ G2 ]]$ which are then passed as the resources for synthesising the second term $B$.
There is an ordering implicit here in `threading through' the contexts between
the premises. For example, starting with a context $[[ x : A, y : B ]]$, then this
rule can be instantiated as:
%
\begin{align}
\tag{\footnotesize{example}}
  \inferrule*[right=$\otimes_{\textsc{Intro}}^{-}$]{x : A, y : B \vdash [[ A ]] \Rightarrow^- [[x]] \ |\ y : B \\ y : B \vdash [[ B ]] \Rightarrow^- [[ y ]] \ | \ \emptyset }{ x : A, y : B \vdash [[ A Prod B ]] \Rightarrow^- [[ pair x y ]] \ | \ \emptyset}
\end{align}
%
Thus this approach neatly avoids the problem of having to split the input context,
and facilitates efficient proof search for linear types.
This idea was adapted by Hughes and Orchard to graded types to facilitate
the synthesis of programs in Granule~\cite{DBLP:journals/pacmpl/OrchardLE19}. Hughes and Orchard termed the
above approach \textit{subtractive} resource management (in a style similar to
\textit{left-over} type-checking for linear type systems~\cite{allais2018typing,zalakain2020pi}). In a graded setting however, this approach was shown to be costly.

Graded type systems, as we consider them here, have typing contexts
in which free-variables are assigned a type, and a grade (usually
drawn from some semiring structure parameterising the calculus). A
useful example is the semiring of natural numbers which is used to
describe exactly how many times an assumption can be used (in contrast
to linear assumptions which must be used exactly once). For example,
the context $[[ x : [A] 2, y : [B] 0 ]]$ explains that $x$ must be
used twice but $y$ must be used not at all. The literature contains
many other examples of semirings for tracking other properties in this way, such
as security
labels~\cite{DBLP:journals/pacmpl/OrchardLE19,DBLP:conf/icfp/GaboardiKOBU16,DBLP:journals/pacmpl/AbelB20},
intervals of usage~\cite{DBLP:journals/pacmpl/OrchardLE19}, or
hardware schedules~\cite{DBLP:conf/esop/GhicaS14}. In a graded
setting, the subtractive approach is problematic as there is not
necessarily a notion of actual subtraction for grades. Consider a
version of the above example for subtractively synthesising a pair,
but now for a context with some grades $r$ and
$s$ on the input variables. Using a variable to synthesise a subterm
now does not result in that variable being left out of the output
context. Instead a new grade must be assigned in the output context
that relates to the first by means of an additional constraint describing
that some usage took place:
%
\begin{align}
\tag{\footnotesize{example}}
\!\!\!\!\!\inferrule*[right=$\otimes_{\textsc{Intro}}^{-}$]{
\exists r' . r' + 1 = r \\ \!\!\!\! \exists s' . s' + 1 = s \\
[[ x : [A] r, y : [B] s ]] \vdash [[ A ]] \Rightarrow^-\! [[x]] \ |\ [[ x : [A] r', y : [B] s ]] \\ \!\!\!\! [[ x : [A] r', y : [B] s ]] \vdash [[ B ]] \Rightarrow^-\! [[ y ]] \ | \ [[ x : [A] r', y : [B] s' ]]  }{ [[ x : [A] r, y : [B] s ]]  \vdash [[ A Prod B ]] \Rightarrow^- \![[ pair x y ]] \ | \ [[ x : [A] r', y : [B] s' ]]}\!\!\!
\end{align}
%
In the first synthesis premise, $x$ has grade $r$ in the input context,
$x$ is synthesised for the goal, and thus the output context has some grade $r'$
where $r' + 1 = r$, denoting that some usage of $x$ occurred (which is
represented by the $1$ element of the semiring in graded systems).

For the natural numbers semiring, with $r = 1$ and $s = 1$ then the
constraints above are satisfied with $r' = 0$ and $s' = 0$. In a general setting, this
subtractive approach to synthesis for graded types requires solving many such
existential equations over semirings, which also introduces a new
source of non-determinism is there is more than one solution.

% Hughes and Orchard implemented this approach,
% leveraging off-the-shelf SMT solving in the context of the Granule language, but
% show that a dual \emph{additive} approach has much better performance.
% In the additive approach, output contexts describe
% what was \emph{used} not what was is \emph{left}.
% In the case of synthesising a term with multiple subterms
% (like pairs), the output context from each premise is then added together using
% the semiring addition operation applied pointwise on contexts to produce
% the final output in the conclusion. For pairs this looks like:
% %
% \begin{align*}
%   \inferrule*[right=$\otimes_{\textsc{Intro}}^{+}$]{\Gamma \vdash [[ A ]] \Rightarrow^+ [[t1]] \ |\ \Delta_{1} \\ \Gamma \vdash [[ B ]] \Rightarrow^+ [[ t2 ]] \ |\ \Delta_{2} }{ \Gamma \vdash [[ A Prod B ]] \Rightarrow^+ [[ pair t1 t2 ]] \ |\ \Delta_{1} + \Delta_{2}}
% \end{align*}
% %
% The entirety of $[[G]]$ is used to synthesise both premises. For example,
% for a goal of $[[ A Prod A ]]$:
% %
% \begin{align}
% \tag{\footnotesize{example}}
%   \inferrule*[right=$\otimes_{\textsc{Intro}}^{+}$]{
% [[ x : [A] r, y : [B] s ]] \vdash [[ A ]] \Rightarrow^+ [[x]] \ |\ [[ x : [A] 1, y : [B] 0 ]] \\ [[ x : [A] r, y : [B] s ]] \vdash [[ A ]] \Rightarrow^+ [[ x ]] \ | \ [[ x : [A] 1, y : [B] 0 ]]  }{ [[ x : [A] r, y : [B] s ]]  \vdash [[ A Prod A ]] \Rightarrow^+ [[ pair x x ]] \ | \ [[ x : [A] {1 + 1} , y : [B] 0 ]]}
% \end{align}
% %
% Later checks in synthesis then determine whether the output context
% describes usage that is within the grades given by $[[ G ]]$, i.e.,
% that the synthesised terms are \emph{well-resourced}.

% Both the subtractive and additive approaches avoid having to split the incoming
% context $[[ G ]]$ into two prior to
% synthesising subterms. We evaluate both resource
% management strategies on a synthesis tool for Granule, finding that in most
% cases, the additive strategy was more efficient for use in program synthesis
% with grades as it involves less complex predicates to be solved as part of
% synthesis; the subtractice approach typically incurs higher overhead due
% to the existentially-derived notion of subtraction seen above.


\section{Synthesis calculi}
\label{sec:core-synth-calculi}

We present two linear-base synthesis calculi with subtractive and additive resource
management schemes, extending an input-output context management approach to graded
modal types. The structure of the synthesis calculi mirrors a
cut-free sequent calculus, with
\textit{left} and \textit{right} rules for each type constructor. Right rules
synthesise an introduction form for the goal type. Left rules
eliminate (deconstruct) assumptions so that they may be
used inductively to synthesise subterms. Each type in the
core language has right and left
rules corresponding to its constructors and destructors respectively.

% MOVE TO FOCUSING
%Our approach to synthesis ensures that it is restricted to generating programs in
%$\beta$-normal form, which eliminates a class of redundant programs for which
%behaviourally equivalent $\beta$-normal forms can be synthesised in
%less steps.

\subsection{Subtractive Resource Management}
  Our subtractive approach follows the philosophy of earlier work on
  linear logic proof search~\cite{HODAS1994327,CERVESATO2000133},
  structuring synthesis rules around an input context of the available
  resources and an output context of the remaining resources that
  can be used to synthesise subsequent subterms. Synthesis rules
  are read bottom-up, with judgments $\Gamma \vdash A \Rightarrow^{-} t\ |\ \Delta$
  meaning from the \emph{goal type} $[[A]]$ we can synthesise a term $[[t]]$ using
  assumptions in $[[G]]$, with output context $[[D]]$. We describe
  the rules in turn to aid understanding. Appendix~\ref{app:subtractive} collects the
  rules for reference.

  \paragraph{Variables}
Variable terms can be synthesised from linear or graded assumptions by rules:
%
  \begin{align*}
  \subLinVar
  \;\;
  \subGrVar
  \end{align*}
%
On the left, a variable $[[x]]$ may be synthesised for the goal
$[[ A ]]$ if a linear assumption $[[ x : A ]]$ is present
  in the input context. The input context without $[[x]]$ is then returned as
  the output context, since $x$ has been used. On the right,
  we can synthesise a variable $x$ for $A$ we have a graded
  assumption of $x$ matching the type. However, % to synthesise $[[ x ]]$
  %it may be the case that
  %using $x$ violates the constraints placed by the assumption's grade. For example,
  %the input context may contain assumptions graded by 0, either because this was
  %specified by the type or the variable has been used as far as it's grade
  %permits already. For this reason, the ability to synthesise a
  %graded variable $[[x]]$
%  requires that $[[x]]$'s grade $r$ can be factored into some grade
%  $s + 1$
  the grading $[[ r ]]$ must permit $[[ x ]]$ to
  be used once here. Therefore, the premise states that there exists
  some grade $s$ such that grade $r$ approximates $s + 1$. The grade $s$
  represents the use of $x$ in the rest of the synthesised term, and
  thus $[[ x : [ A ] s ]]$ is in the output context. For the natural
  numbers semiring, this constraint is satisfied by $s = r - 1$ whenever $r \neq
  0$, e.g., if $r = 3$ then $s = 2$. For
  intervals, the role of approximation is more apparent: if $r = [[
  Intrv 0 3]]$ then this rule is satisfied by $s = [[ Intrv 0 2 ]]$
  where $s + 1 = [[ Intrv 0 2 ]] + [[ Intrv 1 1 ]] = [[ Intrv 1 3 ]]
  \sqsubseteq [[ Intrv 0 3 ]]$.
  This is captured by the instantiation of a new
  existential variable representing the new grade for $[[x]]$ in the output
  context of the rule. In the natural numbers semiring, this could be done by
  simply subtracting $1$ from the assumption's
  existing grade $r$. However, as not all semirings have an
  additive inverse, this is instead handled via a constraint on the new grade
  $s$, requiring that $ r \sqsupseteq s + 1 $. In the implementation, the constraint is
  discharged via an SMT solver, where an unsatisfiable result terminates
  this branch of synthesis.

  \paragraph{Functions}
In typing, $\lambda$-abstraction binds linear variables to introduce
  linear functions. Synthesis from a linear function type therefore mirrors typing:
%
  \begin{align*}
\subAbs
    \end{align*}
%
  Thus, $\lambda x . t$ can be synthesised given that
  $t$ can be synthesised from $B$ in the context of $[[G]]$ extended with a fresh linear assumption $[[ x
  : A]]$. To ensure that $[[x]]$ is used linearly
  by $[[t]]$ we must therefore check that it is not present in
  $[[D]]$.

The left-rule for linear function types then synthesises applications
(as in~\cite{HODAS1994327}):
%
  \begin{align*}
    \subApp
    \end{align*}
%
  %The left rule equivalent of $\textsc{R}\multimap^{-}$ for
  %synthesising an abstraction is the rule for
  %synthesising an application $\textsc{L}\multimap^{+}$.
  The rule synthesises a term for type $[[ C ]]$ in a context that
  contains an assumption $[[ x1 : A -o B ]]$.
%
%, we can apply some value
%  of type $[[A]]$ to this to obtain a value of type $[[B]]$ to use in the
  %synthesis of $[[t1]]$ from the goal type $[[C]]$.
  The first premise synthesises a term $[[t1]]$ for $[[C]]$ under the context
  extended with a fresh linear assumption $[[x2
  : B]]$, i.e., assuming the result of $[[ x1 ]]$. This produces an output context $[[D1]]$ that must not contain
  $[[x2]]$, i.e., $[[x2]]$ is used by $[[t1]]$. The remaining
  assumptions $[[D1]]$ provide the input context to
  synthesise $[[t2]]$ of type $[[A]]$: the argument to the function $[[x1]]$. In the conclusion,
  the application $[[x1 t2]]$ is substituted for $[[x2]]$ inside
  $[[t1]]$, and $[[D2]]$ is the output context.

\paragraph{Dereliction} Note that the above rule synthesises the application of a
function given by a linear assumption. What if we have a graded
assumption of function type? Rather than duplicating every left rule
for both linear and graded assumptions, we mirror the
dereliction typing rule (converting a linear assumption to graded) as:
%
  \begin{align*}
    \subDer
    \end{align*}
%
  Dereliction captures the ability to reuse a graded assumption being
  considered in a left rule. A fresh linear assumption $[[y]]$ is generated that
  represents the graded assumption's use in a left rule, and must be used
  linearly in the subsequent synthesis of $[[t]]$. The output context of this premise then contains $[[x]]$ graded by $s'$, which reflects how $[[x]]$ was used in the synthesis of $[[t]]$, i.e. if $[[x]]$ was not used then $s' = s$. The premise $[[ exists s . r >= s + 1 ]]$ constrains the number of times dereliction can be applied so that it does not exceed $x$'s
  original grade $r$.

  \paragraph{Graded modalities}
  For a graded modal goal type $[[ [] r A ]]$, we synthesise a promotion
$[[ [ t ] ]]$ if we can synthesise the `unpromoted' $[[t]]$ from $[[A]]$:
%Synthesis of promoted values is captured by the
%  rule $\textsc{R}\square^{-}$.
  \begin{align*}
    \subBox
    \end{align*}
%
  A non-graded value $[[t]]$ may be promoted to a graded value using
  the box syntactic construct.
  Recall that typing of a promotion $[[ [ t ] ]]$
  scales all the graded assumptions used to type $[[ t ]]$ by $r$. Therefore,
  to compute the output context we must ``subtract'' $r$-times the use of the variables in $[[
  t ]]$. However, in the subtractive model $[[ D ]]$ tells us what is
  left, rather than what is used. Thus we first compute the
  \textit{context subtraction} of $[[G]]$ and $[[D]]$
  yielding the variables usage information about $[[ t ]]$:
 %
  \begin{definition}[Context subtraction]\label{def:contextSub}
  For all $[[ G1 ]], [[ G2 ]]$ where $ [[G2]] \subseteq [[G1]]$:
\begin{align*}
[[G1 - G2]] =
\left\{\!\begin{matrix}
\begin{array}{lll}
% Base case
[[G1]]
  & [[G2]] = \emptyset
\\[0.25em]
([[G1']], [[G1'']]) - [[G2']]
  & [[G2]] = [[ G2', x : A]] & \wedge\ [[G1]] = [[G1', x : A]], [[G1'']]
\\[0.25em]
(([[G1']], [[G1'']]) - [[G2']]), [[x : [A] q]]
  & [[ G2]] = [[G2', x : [A] s]] & \wedge\ [[G1]] = [[ G1',x : [A]
                                   r]],[[G1'']] \\[0em]
          & \wedge \ [[ exists q . r >= q + s]]
          & \!\! \wedge \ \maximal{q}{q'}{r}{q' + s}
\end{array}
\end{matrix}\right.
\end{align*}
%
\end{definition}
As in graded variable synthesis, context subtraction existentially quantifies a
variable $q$ to express the relationship
between grades on the right being ``subtracted'' from those on the
left. The last conjunct states
$q$ is the greatest element (wrt.
to the pre-order) satisfying this constraint, i.e., for all
other $q' \in \mathcal{R}$ satisfying the subtraction constraint
then $[[ q >= q']]$ e.g., if $r = [[ Intrv 2 3 ]]$
and $s = [[ Intrv 0 1]]$ then $q = [[ Intrv 2 2 ]]$ instead of, say,
$[[ Intrv 0 1]]$. This \emph{maximality} condition is
important for soundness (that synthesised programs are well-typed).
%, which does not satisfy
%maximality.

%There may be many such existentially quantified variables coming from
%a subtraction, and we sometimes write $\exists \Sigma . [[ G1 - G2 ]]$
%to denote the set $\Sigma$ of existential variables introduced by such
%a context subtraction.

Thus for \subBoxName, $[[ G - D ]]$ is multiplied by the goal type grade $r$ to obtain how these
  variables are used in $[[t]]$ after promotion. This is then subtracted from
  the original input context $[[G]]$ giving an output context
  containing the left-over variables and grades. Context
  multiplication
  requires that $[[G - D]]$ contains only graded variables,
  preventing the incorrect use of linear variables from $[[G]]$ in
  $[[t]]$.

Synthesis of graded modality elimination, is handled by the
  \subUnboxName\ left rule:
  \begin{align*}
    \subUnbox
    \end{align*}
%
  Given an input context comprising $[[ G ]]$ and a linear
  assumption $[[ x1 ]]$ of graded modal type, we can synthesise an unboxing of
  $[[x1]]$ if we can synthesise a term $[[t]]$ under $[[G]]$
  extended with a graded assumption $[[x2 : [A] r]]$. This returns an output
  context that must contain $[[x2]]$ graded by $s$
  with the constraint that $s$ must approximate $0$. This enforces
  that $x_2$ has been used as much as stated by the grade $r$.

\paragraph{Products}
The right rule for products \subPairIntroName\ behaves similarly to the
\subAppName\ rule, passing the entire input context $[[ G ]]$ to the first
premise. This is in then used to synthesise the first sub-term of the pair
$[[ t1 ]]$, yielding an output context $ [[ D1 ]]$, which is passed to the
second premise. After synthesising the second sub-term $[[ t2 ]]$, the output
context for this premise becomes the output context of the rule's conclusion.

The left rule equivalent \subPairElimName\  binds two assumptions
$[[ x1 : A ]]$ $[[ x2 : B ]]$ in the premise, representing the constituent sides
of the pair. As with \subAppName, we also ensure that these bound assumptions must not
present in the premise's output context $[[ D ]]$.

\begin{align*}
\begin{array}{c}
  \subPairIntro
\\[0.75em]
  \subPairElim
\end{array}
\end{align*}
\paragraph{Sums}
The \subSumElimName\ rule synthesises the left and
right branches of a case statement that may use resources
differently. The output context therefore takes the \textit{greatest
lower bound} ($\sqcap$) of $[[ D1 ]]$ and $[[ D2 ]]$, given by definition~\ref{def:context-glb},

\begin{definition}[Partial greatest-lower bounds of
  contexts]\label{def:context-glb}
For all $[[ G1 ]]$, $[[ G2 ]]$:
\begin{align*}
\label{def:lub}
[[G1]] \sqcap [[G2]] =
%%
\left\{\begin{matrix}
\begin{array}{lll}
%% Both empty case
\emptyset
  & [[ G1 ]] = \emptyset & \wedge \; [[ G2 ]] = \emptyset
\\
%
%% Left empty
(\emptyset \sqcap [[ G2' ]]), [[ x : [ A ] {glb 0 s} ]]
  & [[ G1 ]] = \emptyset & \wedge \; [[G2]] = [[ G2',x : [A] s]]
\\
%
%% Left is left linear
([[G1']] \sqcap [[(G2',G2'')]]), [[x : A]]
 & [[G1]] = [[{G1', x : A} ]] & \wedge \; [[ G2 ]] = [[ {G2', x : A}, G2'' ]]
\\
%
%% Left is graded
([[G1']] \sqcap [[(G2',G2'')]]), [[x : [A] {glb r s}]]\;\;
 & [[G1]] = [[ G1',x : [A] r]] & \wedge \; [[ G2 ]] = [[{G2', x : [A] s}, G2'']]
\end{array}
\end{matrix}\right.
\end{align*}
where $r\!\sqcap\!s$ is the greatest-lower bound of grades $[[r]]$
and $[[s]]$ if it exists, derived from $\sqsubseteq$.
\end{definition}
%
%
\begin{align*}
\begin{array}{c}
  \subSumIntroL
  \subSumIntroR
\\[0.75em]
  \subSumElim
\end{array}
\end{align*}

As an example of $\sqcap$, consider the semiring of intervals over natural numbers and two
judgements that could be used as premises for the (\subSumElimName) rule:
%
\begin{align*}
& [[ G, y : [A'] Intrv 0 5, x2 : A |- C =>- t1 ; y : [A'] Intrv 2 5 ]] \\
& [[ G, y : [A'] Intrv 0 5, x3 : B |- C =>- t2 ; y : [A'] Intrv 3 4 ]]
\end{align*}
%
where $t_1$ uses $y$ such that there are $2$-$5$ uses remaining
and $t_2$ uses $y$ such that there are $3$-$4$
uses left. To synthesise $\textbf{case} \ x_{1}\ \textbf{of}\ \textbf{inl}\ x_{2} \rightarrow t_{1};\ \textbf{inr}\ x_{3} \rightarrow t_{2}$
the output context must be pessimistic about what resources are left,
thus we take the greatest-lower bound yielding the interval $[
2\dots4 ]$ here: we know $y$ can be used at least twice and at most
$4$ times in the rest of the synthesised program.

\paragraph{Unit}
The right and left rules for units are then
self-explanatory following the subtractive resource model:
%
\begin{align*}
\begin{array}{c}
  \subUnitIntro
  \subUnitElim
\end{array}
\end{align*}

%

This completes subtractive synthesis. We conclude
with a key result, that synthesised terms are well-typed at the type from which they
were synthesised:
%
\begin{restatable}[Subtractive synthesis soundness]{lemma}{subSynthSound}
\label{lemma:subSynthSound}
For all $[[ G ]]$ and $[[ A ]]$
then:
\begin{align*}
[[ G |- A =>- t ; D ]] \quad \implies \quad [[ G - D |- t : A ]]
\end{align*}
i.e. $[[ t ]]$ has type $[[ A ]]$
under context $[[ G - D ]]$,
that contains just those linear and
graded variables with grades reflecting their use in $[[ t ]]$.
Appendix~\ref{sec:linear-proofs} provides the proof.
\end{restatable}
%
\iffalse
\subsubsection{Alternative promotion}
We consider an alternate synthesis for graded modal terms,
replacing (R$\square^{-}$) (repeated on the left) with an alternate
version (R${\square'^{-}}$):
%
  \begin{align*}
    \subBox
    \;\;
    \subBoxAlt
    \end{align*}
%
  In this rule, the input context to the
  premise consists of the context $[[G]]$ ``divided'' by the grade of the goal type
  $[[r]]$ where division is defined:
%
%\begin{definition}[Scalar context division]\label{def:contextDiv}
\begin{align*}
   [[ . / r ]] = [[ . ]]
    \qquad\qquad
  [[ (G , x : [ A ] s) / r ]] = [[ G' / r, x : [A] s' ]]\ where\ [[ exists s' . s'
  * r = s ]]
\end{align*}
%\end{definition}
  Similarly to context subtraction, context division existentially quantifies
  over a grade $s'$ to express the relationship between the grades
  in the context $[[G]]$ being ``divided'' by $r$. %Equations of the form
  %$[[ exists s' . s' * r = s]]$ may only be satisfiable if a
  %multiplicative inverse exists, requiring some constraint solving.
  The output context for the synthesis of $[[t]]$ is then multiplied
  by $r$ in the conclusion of the rule. Such constraints require the
  SMT solver to compute a factorisation, which is typically expensive.
  Section~\ref{sec:evaluation} considers the cost implications, comparing
  performance of (R$\square^{-}$) versus (R${\square^{-}}'$).
\fi

\subsection{Additive Resource Management}
We now present the dual to subtractive resource management --- the
\emph{additive} approach.
Additive synthesis also uses the input-output context approach, but where
output contexts describe exactly which assumptions were used to synthesise
a term, rather than which assumptions are still available. Additive
   synthesis rules are read bottom-up, with $[[G |- A =>+ t; D]]$
  meaning that from the type $[[A]]$ we synthesise a term $[[t]]$ using
  exactly the assumptions $[[D]]$ that originate from the input
  context $[[G]]$.

  \paragraph{Variables}
  We unpack the rules, starting with variables:
%
\begin{align*}
  \addLinVar
  \addGrVar
  \end{align*}
%
For a linear assumption, the output context contains
just the variable that was synthesised. For a graded assumption $[[x : [A] r]]$, the output
context contains the assumption graded by $1$. To synthesise a
variable from a graded assumption, we must check that the use is
compatible with the grade.

\paragraph{Graded modalities}
The subtractive approach handled the \textsc{GrVar$^{-}$}
by a constraint $[[ exists s . r >= s + 1]]$. Here however, the
point at which we check that a graded assumption has been used
according to the grade takes place in the \addUnboxName rule, where graded
assumptions are bound:
%
\begin{align*}
  \addUnbox
  \end{align*}
%
Here, $[[t]]$ is synthesised under a fresh graded assumption
$[[ x2 : [A] r]]$. This produces an output context containing $[[x2]]$ with
some grade $s$ that describes how $[[x2]]$ is used in $[[t]]$. An
additional premise requires that the original grade $r$ approximates either $s$
if $[[x2]]$ appears in $[[D]]$ or $0$ if it does not,
ensuring that $[[x2]]$ has been used correctly. For the
$\mathbb{N}$-semiring with equality as the ordering, this would
ensure that a variable has been used exactly the number of times
specified by the grade.

The synthesis of a promotion is considerably simpler in the additive
approach. In subtractive resource management it was necessary to calculate how
resources were used in the synthesis of $[[t]]$ before then applying the
scalar context multiplication by the grade $r$ and subtracting this from the
original input $[[G]]$. In additive resource management, however, we can simply
apply the multiplication directly to the output context $[[D]]$ to obtain how
our assumptions are used in $[[ [t] ]]$:
%
\begin{align*}
  \addBox
\end{align*}

\paragraph{Functions}
Right and left rules for $\multimap$ have a similar shape to the
subtractive calculus:
%
\begin{align*}
\begin{array}{c}
\addAbs
\\[0.8em]
\addApp
\end{array}
\end{align*}
%
Synthesising an abstraction (\addAbsName) requires that $[[x : A]]$ is in
the output context of the premise, ensuring that linearity is preserved.
Likewise for application (\addAppName), the output
context of the first premise must contain the linearly bound $[[x2 :
B]]$ and the final output context must contain the assumption being used in the
application $[[ x1 : A -o B ]]$. This output context computes the \emph{context
addition} (Def.~\ref{def:contextAdd}) of both output contexts of the premises $[[D1 + D2]]$. If $[[D1]]$
describes how assumptions were used in $[[t1]]$ and $[[D2]]$ respectively for
$[[t2]]$, then the addition of these two contexts describes the usage of
assumptions for the entire subprogram. Recall, context addition
ensures that a linear assumption may not appear in both $[[D1]]$ and
$[[D2]]$, preventing us from synthesising terms that violate linearity.



\paragraph{Dereliction}
As in the subtractive calculus,
%the additive equivalent of dereliction ($\textsc{Der}$) also allows us to
%reuse graded assumptions in a left rule:
we avoid duplicating left rules to
match graded assumptions by giving a synthesising version of dereliction:
\begin{align*}
  \addDer
  \end{align*}
%
The fresh linear assumption $[[ y : A ]]$ must
appear in the output context of the premise, ensuring it is used. The final
context therefore adds to $[[ D ]]$ an assumption of $[[x]]$ graded by
$1$, accounting for this use of $[[ x ]]$ (temporarily renamed to
$y$).

\paragraph{Products}
The right rule for products \addPairIntroName\ follows the same structure as its
subtractive equivalent, however, here $[[ G ]]$ is passed to both premises.
The conclusion's output context is then formed by taking the context addition of
the $[[ D1 ]]$ and $[[ D2 ]]$. The left rule, \addPairElimName\ follows fairly
straightforwardly from the resource scheme.
\begin{align*}
\begin{array}{c}
  \addPairIntro
\\[0.8em]
  \addPairElim
\end{array}
  \end{align*}

\paragraph{Sums}
In contrast to the subtractive rule, the rule \addSumElimName\ takes the least-upper bound of
the premise's output contexts (see definition~\ref{def:context-lub}). Otherwise,
the right and left rules for synthesising programs from sum types are straightforward.
\begin{align*}
\begin{array}{c}
  \addSumIntroL
  \addSumIntroR
\\[0.8em]
{\small{\!\!\addSumElim}}
\end{array}
  \end{align*}

\paragraph{Unit}
As in the subtractive approach, the right and left rules for unit types, are
as expected.
\begin{align*}
\begin{array}{c}
  \addUnitIntro
  \addUnitElim
\end{array}
  \end{align*}


  Thus concludes the rules for additive synthesis. As with subtractive, we
  have prove that this calculus is sound.
  \begin{restatable}[Additive synthesis soundness]{lemma}{addSynthSound}
\label{lemma:addSynthSound} For all $[[ G ]]$ and $[[ A ]]$:
%
\begin{align*}
[[ G |- A =>+ t ; D ]] \quad \implies \quad [[ D |- t : A ]]
\end{align*}
Appendix~\ref{sec:soundness-proofs} gives the proof.
\end{restatable}
Thus, the synthesised term $[[ t ]]$ is well-typed
at $[[ A ]]$ using only the assumptions $[[ D ]]$.
, where $[[D]]$ is a
subset of $[[G]]$.
i.e., synthesised terms are well typed at the type from which they
were synthesised.

\subsubsection{Additive pruning}
%
As seen above, the additive approach delays checking
whether a variable is used according to its linearity/grade
until it is bound. We hypothesise that this can lead additive synthesis to explore
many ultimately ill-typed (or \emph{ill-resourced})
paths for too long. Subsequently, we define a ``pruning''
variant of any additive rules with multiple sequenced
premises. For \addPairIntroName\ this is:
%
\begin{align*}
  \begin{array}{c}
    \addPrunePairIntro
  \end{array}
\end{align*}
%
Instead of passing $[[G]]$ to both
premises, $[[G]]$ is the input only for
the first premise. This premise outputs context $[[D1]]$ that
is subtracted from $[[G]]$ to give the input context
of the second premise. This provides an opportunity to
terminate the current branch of synthesis early if $[[ G - D1
]]$ does not contain the necessary resources to attempt the
second premise.
The \addAppName\ rule is similarly adjusted:

\begin{align*}
  \begin{array}{c}
    \addPruneApp
  \end{array}
\end{align*}

\begin{restatable}[Additive pruning synthesis soundness]{lemma}{addPruningSynthSound}
\label{lemma:addPruningSynthSound} For all $[[ G ]]$ and $[[ A ]]$:
%
\begin{align*}
[[ G |- A =>+ t ; D ]] \quad \implies \quad [[ D |- t : A ]]
\end{align*}
Appendix~\ref{sec:soundness-proofs} gives the proof.
\end{restatable}

\section{Focusing}
\jnote{expand here with diagram and full focusing calculi - maybe talk through
  the subtractive rules}
\jnote{possibly make this a section about implementaiton with a focusing subsection}

\section{Evaluation}

\newcommand{\stderr}[1]{\textcolor{gray}{${#1}$}} % \pm{#1}
\newcommand{\fail}{\textcolor{mypink3}{$\times$}}
\newcommand{\success}{\checkmark}
\newcommand{\highlight}[1]{%
{\setlength{\fboxsep}{0pt}\colorbox{yellow!50}{$\displaystyle#1$}}}

Prior to evaluation, we made the following hypotheses about the
relative performance of the additive versus subtractive approaches:
%
\begin{enumerate}[itemsep=0em]
\item Additive synthesis should make fewer calls to the solver, with lower
complexity theorems (fewer quantifiers). Dually,
subtractive synthesis makes more calls to the solver with
higher complexity theorems (more quantifiers);

\item For complex problems, additive synthesis will
explore more paths as it cannot tell whether a variable is not
well-resourced until closing a binder; additive pruning and subtractive will
explore fewer paths as they can fail sooner.

\item A corollary of the above two: simple examples will
likely be faster in additive mode, but more complex examples will be
faster in subtractive mode.
\end{enumerate}

\subsubsection{Methodology}
We implemented our approach as a synthesis tool for
Granule, integrated with its core tool. Granule features
ML-style polymorphism (rank-0 quantification) but we do not address polymorphism here.
Instead, programs are synthesised from type schemes treating universal
type variables as logical atoms. %Multiplicative products are
%primitive in Granule, although additives coproducts are provided via
%ADTs, from which we define a core sum type to use here.

Constraints on resource usage are handled via Granule's existing
symbolic engine, which compiles constraints on grades (for various semirings)
to the SMT-lib format for Z3~\cite{z3}.
%In the case of graded variable synthesis in the subtractive
%scheme, the kind of the assumption's grade (i.e., what semiring it
%belongs to) is inferred using Granule's type
%checker, which is used to generate an existential variable representing
%the remaining available usage of the graded assumption.
We use the LogicT
monad for backtracking search~\cite{logict}
and the Scrap Your Reprinter library for
splicing synthesised code into syntactic ``holes'',
preserving the rest of the program text~\cite{clarke2017scrap}.

To evaluate our synthesis tool we developed a suite of benchmarks comprising
Granule type schemes for a variety of operations using linear and graded modal
types. We divide our benchmarks into several classes of problem:
%
\begin{itemize}[itemsep=0em,leftmargin=1.1em]
\item \textbf{Hilbert}: the Hilbert-style axioms of
  intuitionistic logic (including SKI combinators), with appropriate $\mathbb{N}$ and $\mathbb{N}$-intervals
  grades where needed (see, e.g., $S$ combinator in
  Example~\ref{ex:s-comb} or coproduct elimination in Example~\ref{exm:or3}).

\item \textbf{Comp}: various translations of function composition
into linear logic: multiplicative, call-by-value and
call-by-name using $!$~\cite{girard1987linear}, I/O using $!$~\cite{liang2009focusing},
and coKleisli composition over $\mathbb{N}$ and arbitrary semirings:
e.g. $\forall r, s \in \mathcal{R}$:
%
\begin{equation*}
\textit{comp-}\textit{coK}_{\mathcal{R}} : [[ {[] r ({[] s A} -> B)} -> {({[] r B} -> C) -> {{[] {r * s} A} -> C}} ]]
\end{equation*}
%
\item \textbf{Dist}: distributive laws of various graded
modalities over functions, sums, and products~\cite{hughes2020},
e.g., $\forall r \in \mathbb{N}$, or
$\forall r \in \mathcal{R}$ in any semiring, or $r = [[ Intrv 0 Inf ]]$:
%
\begin{equation*}
\textit{pull}_\oplus : [[ (Sum {[] r A} {[] r B}) -> [] r (Sum A B) ]]
\quad\;\;\;
\textit{push}_\multimap : [[ {[] r (A -> B)} -> {{[] r A} -> [] r B} ]]
\end{equation*}
%
%% data type and vice vera as our second class of programs. In the latter case, we have programs
%which \textit{pull} natural number, !, and general modalities out of sum,
%product, and vector data types. In the former case, we have programs
%which distribute (\textit{push}) natural number, !, and general graded modalities over a
%function type. The ability to synthesise \textit{push} over other data types is
%further work (see section \ref{sec:futurework}).

\item \textbf{Vec}: map operations on
vectors of fixed size encoded as products, e.g.:
%
\begin{equation*}
\!\!
\textit{vmap}_5 : [[ {[] 5 (A -> B)} -> {(Tup (Tup (Tup (Tup A A) A) A) A) -> (Tup (Tup (Tup (Tup B B) B) B) B)} ]]
\end{equation*}
%

\item \textbf{Misc}: includes Example~\ref{exm:security}
  (information-flow security) and  functions which must share or split resources
between graded modalities, e.g.:
%one problem requires two graded values (the first two parameters)
%to be shared between two applications of a function given as the third input:
%
\begin{equation*}
\!\!
\textit{share}: [[ {{[] 4 A} -> {{[] 6 A} -> {[] 2 {({(Tup {(Tup {(Tup {(Tup A A)} A)} A)} A)} ->
    B)} }}} -> {(Tup B B)}  ]]
\end{equation*}
%
\end{itemize}
%
Appendix~\ref{app:list-of-types} lists the type schemes for these
synthesis problems (32 in total).

We found that Z3 is highly variable in its solving time, so timing
measurements are computed as the mean of 20 trials. We used
Z3 version 4.8.8 on a Linux laptop with an Intel i7-8665u @ 4.8 Ghz
and 16 Gb of RAM.

\subsubsection{Results and analysis}
%
For each synthesis problem, we recorded whether synthesis
was successful or not (denoted $\success$ or \fail), the mean
total synthesis time ($\mu{T}$), the mean total time spent by
the SMT solver ($\mu\textsc{smt}$), and the number of
calls made to the SMT solver (\textsc{N}).
Table~\ref{tab:results} summarises the results with the fastest case for each
benchmark highlighted.
For all benchmarks that used
the SMT solver, the solver accounted for $91.73\%-99.98\%$
of synthesis time, so we report only the mean
total synthesis time $\mu{T}$. % as this gives a good proxy of the solver time.
We set a timeout of 120 seconds.

\begin{table}[t]
{\small{
\begin{center}
\setlength{\tabcolsep}{0.3em}
% this is wide enough to show 4 modes worth of data
\begin{tabular}{p{2.5em}r|p{0.75em}rr|p{0.5em}rr|p{0.5em}rr}
 & & \multicolumn{3}{c|}{Additive}&\multicolumn{3}{c|}{Additive (pruning)}&\multicolumn{3}{c|}{Subtractive}\\ \hline
\multicolumn{2}{c|}{{Problem}} &  & \multicolumn{1}{c}{$\mu{T}$ (ms)} & \multicolumn{1}{r|}{\textsc{N}} & & \multicolumn{1}{c}{$\mu{T}$ (ms)} & \multicolumn{1}{r|}{\textsc{N}} & & \multicolumn{1}{c}{$\mu{T}$ (ms)} & \multicolumn{1}{r|}{\textsc{N}} \\ \hline\hline
\multirow{5}{*}{{\rotatebox{90}{\textbf{Hilbert}}}}
& $\otimes{}$Intro & \success{} &   {\highlight{$6.69 (\stderr{  0.05})$}} &   2  &    \success{}
                                             &   9.66 (\stderr{  0.23}) &   2
                                                                     &     \success{}   &  10.93 (\stderr{  0.31}) &   2 \\
& $\otimes{}$Elim                     & \success{} &   0.22 (\stderr{  0.01}) &   0       & \success{} &   {\highlight{$0.05 (\stderr{  0.00})$}} &   0       & \success{} &   0.06 (\stderr{  0.00}) &   0      \\
& $\oplus{}$Intro & \success{} &   0.08 (\stderr{  0.00}) &   0    &  \success{}
                                          &   {\highlight{$0.07 (\stderr{  0.00})$}} &   0
                                                                   &    \success{}
                                                                                 &
                                                                                   {\highlight{$0.07
                                                                                   (\stderr{
                                                                                   0.00})$}}
                                                                                                                   &   0 \\
& $\oplus{}$Elim                       & \success{} &   {\highlight{$7.26 (\stderr{  0.30})$}} &   2       & \success{} &  13.25 (\stderr{  0.58}) &   2       & \success{} & 204.50 (\stderr{  8.78}) &  15      \\ %SmtT/T = 93.85304985917998%, 91.73667044352199%, 99.1533306157868%
& SKI & \success{} &   {\highlight{$8.12 (\stderr{  0.25})$}} &   2  &    \success{} &  24.98
                                                                      (\stderr{
                                                                      1.19}) &
                                                                               2
                                                                             &  \success{}
                                                                                 &
                                                                                   41.92
                                                                                   (\stderr{
                                                                                   2.34})
                                                                                                                   &
                                                                                                                     4
                 \\
\hline
\multirow{6}{*}{{\rotatebox{90}{\textbf{Comp}}}}
& 01                        & \success{} &  {\highlight{$28.31 (\stderr{  3.09})$}} &   5       & \success{} &  41.86 (\stderr{  0.38}) &   5  & \fail{} & Timeout & -    \\
& cbn                       & \success{} &  {\highlight{$13.12 (\stderr{  0.84})$}} &   3       & \success{} &  26.24 (\stderr{  0.27}) &   3  & \fail{} & Timeout & -    \\
& cbv                       & \success{} &  {\highlight{$19.68 (\stderr{  0.98})$}} &   5       & \success{} &  34.15 (\stderr{0.98}) &   5   & \fail{} & Timeout & -   \\
& $\circ\textit{coK}_\mathcal{R}$                 & \success{} &  33.37 (\stderr{  2.01}) &   2       & \success{} &  {\highlight{$27.37$}} (\stderr{  0.78}) &   2       & \fail{}  &  92.71 (\stderr{  2.37}) &   8      \\  % SmtT/T = 98.56608024136683%, 98.56541178986083%, 98.70614421865372%
& $\circ\textit{coK}_\mathbb{N}$                 & \success{} &  27.59 (\stderr{  0.67}) &   2       & \success{} &  {\highlight{$21.62$}} (\stderr{  0.59}) &   2       & \fail{}  &  95.94 (\stderr{  2.21}) &   8      \\ % SmtT/T = 98.36019681512148%, 98.35210841341276%, 98.63478123527435%
& mult                      & \success{} &   0.29 (\stderr{  0.02}) &   0       & \success{} &   0.12 (\stderr{  0.00}) &   0       & \success{} &   {\highlight{$0.11 (\stderr{  0.00})$}} &   0      \\     % SmtT/T = 0.0%, 0.0%, 0.0%
\hline
\multirow{9}{*}{{\rotatebox{90}{\textbf{Dist}}}}
& $\otimes$-!                 & \success{} &  {\highlight{$12.96 (\stderr{  0.48})$}} &   2       & \success{} &  32.28 (\stderr{  1.32}) &   2       & \success{} & 10487.92 (\stderr{  4.38}) &   7      \\ % SmtT/T = 96.84305139487837%, 99.16605247629178%, 99.98233353925326%
& $\otimes$-$\mathbb{N}$                  & \success{} &  {\highlight{$24.83 (\stderr{  1.01})$}} &   2       & \fail{}  &  32.18 (\stderr{  0.80}) &   2       & \fail{}  &  31.33 (\stderr{  0.65}) &   2      \\  % SmtT/T = 99.16249211706345%, 98.19366472714205%, 97.68400765522344
& $\otimes$-$\mathcal{R}$                  & \success{} &  {\highlight{$28.17 (\stderr{  1.01})$}} &   2       & \fail{}  &  29.72 (\stderr{  0.90}) &   2       & \fail{}  &  31.91 (\stderr{  1.02}) &   2      \\ % SmtT/T = 99.26176085615197%, 97.2013820814111%, 97.92618319348946%
& $\oplus$-!                & \success{} &   {\highlight{$7.87 (\stderr{  0.23})$}} &   2       & \success{} &  16.54 (\stderr{  0.43}) &   2       & \success{} & 160.65 (\stderr{  2.26}) &   4      \\ % SmtT/T = 96.52232766433309%, 96.54651122326587%, 99.69538508877449
& $\oplus$-$\mathbb{N}$                & \success{} &  {\highlight{$22.13 (\stderr{  0.70})$}} &   2       & \success{} &  30.30 (\stderr{  1.02}) &   2       & \fail{}  &  23.82 (\stderr{  1.13}) &   1      \\  % SmtT/T = 99.00528362933007%, 99.08548651040508%, 98.49944141606836%
& $\oplus$-$\mathcal{R}$                & \success{} &  {\highlight{$22.18 (\stderr{  0.60})$}} &   2       & \success{} &  31.24 (\stderr{  1.40}) &   2       & \fail{}  &  16.34 (\stderr{  0.40}) &   1      \\ % SmtT/T = 99.08179689945528%, 99.16435051551996%, 98.4221935356417%
& $\multimap$-!                & \success{} &   {\highlight{$6.53 (\stderr{  0.16})$}} &   2       & \success{} &  10.01 (\stderr{  0.25}) &   2       & \success{} & 342.52 (\stderr{  2.64}) &   4      \\% SmtT/T = 96.72718832940333%, 96.60077128502638%, 99.698687842732%
& $\multimap$-$\mathbb{N}$                  & \success{} &  29.16 (\stderr{  0.82}) &   2       & \success{} &  {\highlight{$28.71 (\stderr{  0.67})$}} &   2       & \fail{}  &  54.00 (\stderr{  1.53}) &   4      \\% SmtT/T = 99.14821593847735%, 99.13267964321408%, 99.00055370249412%
& $\multimap$-$\mathcal{R}$                  & \success{} &  29.31 (\stderr{  1.84}) &   2       & \success{} &  {\highlight{$27.44 (\stderr{  0.60})$}} &   2       & \fail{}  &  61.33 (\stderr{  2.28}) &   4      \\% SmtT/T = 99.22644411928067%, 99.20991868100477%, 99.01094953081872%
\hline
\multirow{4}{*}{{\rotatebox{90}{\textbf{Vec}}}}
& vec5                      & \success{} &   {\highlight{$4.72 (\stderr{  0.07})$}} &   1       & \success{} &  14.93 (\stderr{  0.21}) &   1       & \success{} &  78.90 (\stderr{  2.25}) &   6      \\% SmtT/T = 80.52034229302626%, 95.98937410635435%, 98.89325740352668%
& vec10                     & \success{} &   {\highlight{$5.51 (\stderr{  0.36})$}} &   1       & \success{} &  20.81 (\stderr{  0.77}) &   1       & \success{} & 142.87 (\stderr{  5.86}) &  11      \\  % SmtT/T = 57.23088868771888%, 91.78964923101526%, 98.76365835388931%
& vec15                     & \success{} &   {\highlight{$9.75 (\stderr{  0.25})$}} &   1       & \success{} &  22.09 (\stderr{  0.24}) &   1       & \success{} & 195.24 (\stderr{  3.20}) &  16      \\ % SmtT/T = 37.28829884651822%, 88.79604744779657%, 98.11179611018657%
& vec20                     & \success{} &  {\highlight{$13.40 (\stderr{  0.46})$}} &   1       & \success{} &  30.18 (\stderr{  0.20}) &   1       & \success{} & 269.52 (\stderr{  4.25}) &  21      \\ % SmtT/T = 25.881885439958207%, 82.374721744386%, 97.48691949245291%
\hline
\multirow{4}{*}{{\rotatebox{90}{\textbf{Misc}}}}
& split$\oplus$            & \success{} &   {\highlight{$3.79 (\stderr{  0.04})$}} &   1       & \success{} &   5.10 (\stderr{  0.16}) &   1       & \success{} & 10732.65 (\stderr{  8.01}) &   6      \\ % SmtT/T = 94.19385210967563%, 94.14708972520134%, 99.97155844397255%
& split$\otimes$                      & \success{} &  {\highlight{$14.07 (\stderr{  1.01})$}} &   3       & \success{} &  46.27 (\stderr{  2.04}) &   3       & \fail{} & Timeout & -                            \\ % SmtT/T = 87.40012646315907%, 97.15716368724911%
& share                     & \success{} & 292.02 (\stderr{ 11.37}) &  44       & \success{} & {\highlight{$100.85 (\stderr{  2.44})$}} &   6       & \success{} & 193.33 (\stderr{  4.46}) &  17      \\ % SmtT/T = 94.3058504587738%, 97.15508002763923%, 99.19701142373343%
& exm.~\ref{exm:security}                 & \success{} &   {\highlight{$8.09 (\stderr{  0.46})$}} &   2       & \success{} &  26.03 (\stderr{  1.21}) &   2       & \success{} & 284.76 (\stderr{  0.31}) &   3      \\ % SmtT/T = 96.70245619318075%, 99.14227225428877%, 99.83905641669199%
\end{tabular}
\end{center}}}

\caption{Results. $\mu{T}$ in \emph{ms} to 2 d.p.
with standard sample error in brackets}
\label{tab:results}
\vspace{-2.5em}
\end{table}

% \begin{table}[t]
% {\small{
% \begin{center}
% \setlength{\tabcolsep}{0.3em}
% \begin{tabular}{p{2.5em}r|p{0.75em}rcc|p{0.75em}rcc} & &
% \multicolumn{4}{c|}{Graded}&\multicolumn{4}{c|}{Cartesian}\\ \hline
% \multicolumn{1}{c}{{Problem}}& \multicolumn{1}{c|}{Exs} & & \multicolumn{1}{c}{$\mu{T}$ (ms)} & \multicolumn{1}{c}{Examples} & \multicolumn{1}{r|}{Size} & & \multicolumn{1}{c}{$\mu{T}$ (ms)} & \multicolumn{1}{c}{Examples} & \multicolumn{1}{r|}{Size}\\ \hline
% \hline \multirow{1}{*}{{\rotatebox{90}{\textbf{Misc}}}} &
% compose & \success{} & {\highlight{$  46.39 (\stderr{  2.91}) $}} &   0       &  38       & \success{} &  55.58 (\stderr{  3.76}) &   0       &  38      \\
% %
% \end{tabular}
% \end{center}}}
% \caption{Results. $\mu{T}$ in \emph{ms} to 2 d.p. with standard sample error in brackets}
% \label{tab:results}
% \vspace{-2.5em}
% \
% end{table}
%



\paragraph{Additive versus subtractive}
As expected, the additive approach generally synthesises programs faster
than the subtractive. Our first hypothesis (that the additive approach in general
makes fewer calls to the SMT solver) holds for almost all benchmarks, with the
subtractive approach often far exceeding the number made by the additive. This is explained by
the difference in graded variable synthesis between approaches. In the
additive, a constant grade $1$ is given for graded assumptions in the output
context, whereas in the subtractive, a fresh grade
variable is created with a constraint on its usage which
is checked immediately. As the
total synthesis time is almost entirely spent in the SMT solver (more than 90\%), solving constraints is by far the most costly part of synthesis
leading to the additive approach synthesising most examples in a shorter amount of time.

Graded variable synthesis in the subtractive case also results
in several examples failing to synthesise. In some cases, e.g.,
the first three \textit{comp} benchmarks, the subtractive approach
times-out as synthesis diverges with constraints growing in size
due to the maximality condition and absorbing behaviour of
$[[ Intrv 0 Inf ]]$ interval. In the case of $\textit{coK-$\mathcal{R}$}$
and $\textit{coK-$\mathbb{N}$}$, the generated constraints
have the form $\forall r. \exists s. r \sqsupseteq s + 1 $ which
is not valid $\forall r \in \mathbb{N}$ (e.g., when $r = 0$),
which suggests that the subtractive approach does not work well for
polymorphic grades. As further work, we are
considering an alternate rule for synthesising promotion with
constraints of the form $\exists s . s = s' * r$, i.e.,
a multiplicative inverse constraint.
%solving approach cannot tell this holds for all semirings. Indeed, it
%is false for $\ma
%which is not satisfiable for all semirings, including \textsc{N}, causing examples
%with arbitrary \textsc{N} grades to also fail.

In more complex examples we see evidence to support
our second hypothesis. The \textit{share} problem requires a lot
of graded variable synthesis which is problematic for the additive
approach, for the reasons described in the second hypothesis. In contrast, the subtractive approach
performs better, with $\mu{T} = 193.3\textit{ms}$ as opposed to additive's
$292.02\textit{ms}$. However, additive pruning outperforms both.

% Not as important
%Notably, on examples which are purely linear such as \textit{andElim} from
%Hilbert's axioms or \textit{mult} for function composition, the subtractive
%approach generally performs better. Linear programs without graded modalities
%can be synthesised without the need to interface with Z3 at all, making the
%differences here somewhat negligible as solver time generally makes up for the
%vast proportion of total synthesis time.

\paragraph{Additive pruning}
The pruning variant of additive synthesis (where subtraction
takes place in the premises of multiplicative rules) had mixed results
compared to the default. In simpler examples, the overhead of pruning
(requiring SMT solving) outweighs
the benefits obtained from reducing the space. However, in more
complex examples which involve synthesising many graded variables (e.g. \textit{share}), pruning is
especially powerful, performing better than the subtractive
approach. However, additive pruning failed to synthesis two
 examples which are polymorphic in their grade
 ($\otimes$-$\mathbb{N}$) and in the semiring/graded-modality ($\otimes$-$\mathcal{R}$).


% Overall, the additive approach outperforms the subtractive and is
% successful at synthesising more examples, including ones polymorphic
% in grades and even the semiring itself. Given that the literature on linear logic theorem proving
% is typically subtractive, this is an interesting result. Going forward, a mixed
% approach between additive and additive pruning may be possible,
% selecting the algorithm, or even the rules, depending on the class of
% problem. Exploring this, and further optimisations and improvements,
% is further work.

\iffalse
\paragraph{Subtractive with Division}
Subtractive synthesis with the alternate rule for graded modality introduction
is generally slower on simpler examples, often requiring more calls to the
solver.
\fi

\section{Conclusion}

\jnote{set up deriving work: problem with synthesising push + examples}
