\chapter{A Core Synthesis Calculus}
\label{chapter:core}
We begin our first exploration into program synthesis with a synthesis system for the
graded linear $\lambda$-calculus of Section~\ref{sec:linear-base}. %More about story and evaluation
The primary aim of this chapter is to introduce the core concepts of
type-directed program synthesis in a resourceful setting, in particular, the
problem of \textit{resource management}. We therefore prioritise simplicity over
expressivity for our target language, with the core typing calculus of
Section~\ref{sec:linear-base} forming an ideal candidate. Much of the work in
this chapter is derived from~\citet{DBLP:conf/lopstr/HughesO20}. 

In Chapter~\ref{chapter:intro} we posed the question ``how do we harness [linear
and graded types] to make writing programs automatically easier?''. This chapter
goes some way to providing an answer, by showing how we can  
incorporate these types into the design of a synthesis tool, using resource
constraints in synthesis to prune the search space of programs. 

As mentioned in Chapter~\ref{chapter:intro}, type-directed program synthesis can
be framed as an inversion of type checking. In type checking, we have a
judgement of the form: 
\begin{align*}
    \Gamma \vdash t : A {\small{\tag{type checking}}}
\end{align*}
which states that under some context of assumptions $\Gamma$ we can assign the
program term $t$ the type $A$. 
In type-checking, we typically think of starting with a term and 
generating its type. Synthesis inverts this interpretation, leaving us
with a \textit{synthesis judgement} form:
\begin{align*}
    \Gamma \vdash A \Rightarrow t {\small{\tag{synthesis}}}
\end{align*}
which states that we can construct a program term $t$ from the type $A$, using
the assumptions in $\Gamma$. Program synthesis then becomes a task of
inductively enumerating programs ``bottom-up'' starting from the goal type $A$:
we break $A$ into sub-goals, from which sub-terms are synthesised until the goal
cannot be broken into further sub-goals. At this point, we either synthesise a
usage of a variable from $\Gamma$ if possible, a constructor for a null-ary data
type if one is available, or synthesis fails. This is the essence of type-directed program
synthesis. 

Resourceful types introduce another dimension to synthesis: how do we ensure
that the assumptions in $\Gamma$ are used according to their resource
constraints as described by their linearity or grades in the synthesised term $t$? I.e. if $x : A$
is a linear assumption in $\Gamma$ that is used in some way to construct $t$,
then the synthesis algorithm must synthesise a $t$ which uses $x$ exactly once.
Likewise, if $x :_r A$ is a graded assumption, then it must be used in $t$ in a
way which satisfies its grade $r$.
 
The linear side of this problem has been explored before in the context of
automated theorem proving for linear logic, and has been termed the
\textit{resource management problem}. We describe this problem in detail in
Section~\ref{sec:resource-management} and propose two candidate solutions which
incorporate grades as well as linearity, basing our approach on the
\textit{input-output context management} model described
by~\citet{HODAS1994327}, and further developed by~\citet{CERVESATO2000133}. 

The challenges posed by ensuring the well-resourcedness of synthesised programs
are best illustrated by the inclusion in our target language of multiplicative
conjunction, and additive disjunction. Therefore, prior to fully describing the
problem of resource management and our proposed solutions, we first expand our
target language with multiplicative product ($\otimes$), and unit types
($\mathsf{Unit}$), as well as disjunctive sum types ($\oplus$). These extensions
are detailed in Section~\ref{sec:linear-base-calculus}, which will be the target
language of the synthesis calculi of this chapter. As well as helping to
conceptualise the challenges posed by program synthesis in a resourceful
setting, these have the added benefit of allowing the synthesis of more
expressive programs, without introducing unnecessary complexity at this stage.

Having outlined both a suitable target language and two approaches to dealing
with the issue of resource management, we then present two synthesis calculi in
Section~\ref{sec:linear-base-synthesis} as augmented inversions of the typing
rules. Each calculus is based on a one of our proposed solutions to the resource
management problem.  

Both calculi are implemented as a synthesis tool for
Granule.\footnote{The exact implementation of the rules as they stand is
deprecated, but may be found in Granule release v0.7.8.0:
https://github.com/granule-project/granule/releases/tag/v0.7.8.0} The calculi
are turned into an algorithm written in Haskell. In order to do so, we apply an
important technique from proof search literature to our calculi:
\emph{focusing}~\citep{focusing}. Focusing removes much of the unnecessary
non-determinism present in our synthesis rules by fixing an ordering on the
application of rules. We present the two \textit{focused} forms of our original
synthesis calculi in Section~\ref{sec:linear-base-focusing}, which form the
basis of our Granule implementation. Following this, we provide some details of
the implementation in Section~\ref{sec:linear-base-implementation}

Each synthesis calculus implementation is then evaluated and contrasted against
each other in Section~\ref{sec:linear-base-evaluation}, which measures the
synthesis calculi on a suite of simple Granule benchmark programs. Finally we
conclude with a discussion of related work in
Section~\ref{sec:linear-base-related}, and highlight some limitations of our
system in Section~\ref{sec:linear-base-conclusion}, showing how we intend to
proceed in subsequent chapters to address these.


\section{A Core Target Language}
\label{sec:linear-base-calculus}
The syntax of types and terms for our extended language are given by the
following two grammars:
\begin{align*}
  \hspace{-0.9em}[[ A ]] , [[ B ]] & ::=
         [[ A -o B ]]
    \mid A \otimes B 
    \mid A \oplus B
    \mid [[ Unit ]]
    \mid [[ [] r A ]]
  {\small{\tag{types}}}
  \\
\hspace{-0.8em} [[ t ]] ::= \;
       & [[ x ]]
  \mid [[ \x . t ]]
  \mid [[ t1 t2 ]]
  \\ \mid \; & [[ [t] ]]
  \mid [[ let [ x ] = t1 in t2 ]]
  \\  \mid \; & [[ pair t1 t2 ]]
  \mid [[ letpair x1 x2 = t1 in t2 ]] \\
  \mid \; & () \mid [[ let () = t1 in t2 ]]
\\ \mid \; & [[ inl t ]] \mid [[ inr t ]] \mid \textbf{case} \ t_{1}\ \textbf{of}\ \textbf{inl}\ x_{1} \rightarrow t_{2};\ \textbf{inr}\ x_{2} \rightarrow t_{3}
{\small{\tag{terms}}}
\end{align*}
Type formers comprise the graded linear $\lambda$-calculus of
Section~\ref{sec:linear-base}, extended with multiplicative products
($\otimes$), additive coproducts ($\oplus$), and multiplicative units $[[ Unit
]]$.

We use the syntax $()$ for the inhabitant of $[[ Unit ]]$. Pattern matching via
a $\textbf{let}$ is used to eliminate products and unit types; for sum types,
$\textbf{case}$ is used to distinguish the constructors.

\begin{figure}[t]
\begin{align*}
  \begin{array}{c}
\inferrule*[right = Pair]
  {[[ G1 |- t1 : A ]] \\ [[ G2 |- t2 : B ]]}
  {[[ G1 + G2 |- pair t1 t2 : Tup A B]]}
\\[1.25em]
\inferrule*[right = LetPair]
  {[[ G1  |- t1 : Tup A B ]] \;\; [[ G2, x1 : A, x2 : B |- t2 : C ]]}
  {[[ G1 + G2 |- letpair x1 x2 = t1 in t2 : C  ]]}
\\[1.25em]
\inferrule*[right = Inl]
  {[[ G |- t : A ]]}
  {[[ G |- inl t : Sum A B ]]}
\;\;\;
\inferrule*[right = Inr]
  {[[ G |- t : B ]]}
  {[[ G |- inr t : Sum A B]]}
\\[1.25em]
\inferrule*[right = Case]
  {[[ G1 |- t1 : Sum A B ]] \\ [[ G2, x1 : A |- t2 : C]] \\ [[ G3, x2 : B |- t3 : C]]}
    {\Gamma + (\Gamma_{2} \sqcup \Gamma_{3}) \vdash\ \textbf{case} \ t_{1}\ \textbf{of}\ \textbf{inl}\ x_{1} \rightarrow t_{2};\ \textbf{inr}\ x_{2} \rightarrow t_{3} : C }
\\[1.25em]
\inferrule*[right = $\mathsf{Unit}$]
 {\quad}{[[ . |- () : Unit ]]}
\;\;\;
\inferrule*[right = LetUnit]
 {[[G1 |- t1 : Unit ]] \quad [[ G2 |- t2 : A ]]}
 {[[ G1 + G2 |- let () = t1 in t2 : A ]]}
\end{array}
\end{align*}
\vspace{-1.25em}
  \caption{Typing rules of for $\otimes$, $\oplus$, and $1$}
\label{fig:typing-prod-sum-unit}
 \end{figure}

Figure~\ref{fig:typing-prod-sum-unit} gives the typing rules.  Rules for
multiplicative products (pairs) and additive coproducts (sums) are routine,
where pair introduction ($\textsc{Pair}$) adds the contexts used to type the
pair's constituent sub-terms. Pair elimination ($\textsc{LetPair}$) binds a
pair's components to two linear variables in the scope of the body $[[t2]]$. The
$\textsc{Inl}$ and $\textsc{Inr}$ rules handle the typing of constructors for
the sum type $[[Sum A B]]$. Elimination of sums ($\textsc{Case}$) takes the
\emph{least upper bound} of
  the two contexts used to type each branch is used, defined:

\begin{definition}[Partial least-upper bounds of
  contexts]\label{def:context-lub} For all $[[ G1 ]]$, $[[ G2 ]]$, $[[G1]] \sqcup [[G2]] =$
\begin{align*}
\label{def:lub}
%%
\left\{\begin{matrix}
\begin{array}{lll}
%% Both empty case
\emptyset
  & [[ G1 ]] = \emptyset & \wedge \; [[ G2 ]] = \emptyset
\\
%
% %% Left empty
% (\emptyset \sqcup [[ G2' ]]), [[ x : [ A ] {lub 0 s} ]]
%   & [[ G1 ]] = \emptyset & \wedge \; [[G2]] = [[ G2',x : [A] s]]
% \\
%
%% Left is left linear
([[G1']] \sqcup [[(G2',G2'')]]), [[x : A]]
 & [[G1]] = [[{G1', x : A} ]] & \wedge \; [[ G2 ]] = [[ {G2', x : A},, G2'' ]]
\\
%
%% Left is graded
([[G1']] \sqcup [[(G2',G2'')]]), [[x : [A] {lub r s}]]\;\;
 & [[G1]] = [[ G1',x : [A] r]] & \wedge \; [[ G2 ]] = [[{G2', x : [A] s}, G2'']]
\end{array}
\end{matrix}\right.
\end{align*}
where $r\!\sqcup\!s$ is the least-upper bound of grades $[[r]]$ and $[[s]]$ if
it exists, derived from $\sqsubseteq$ (given by Definition~\ref{def:def-lub}).
\end{definition}
%
As an example of the partiality of $\sqcup$, if one branch of a \textbf{case}
uses a linear variable, then the other branch must also use it to maintain
linearity overall, otherwise the upper-bound of the two contexts for these
branches is not defined.

With these extensions in place, we now have the capacity to write more 
idiomatic functional programs in our target language. As a demonstration of 
this, and to showcase how graded modalities interact with these new type 
extensions, we provide two further examples of different graded modalities 
which complement these new types.

\begin{example}%[Intervals]
\label{exm:or3}
Exact usage analysis is less useful when control-flow is involved, e.g.,
eliminating sum types where each control-flow branch uses variables differently.
The $\mathbb{N}$-semiring shown on page~\pageref{ex:s-comb} can be imbued with a
notion of \emph{approximation} via less-than-equal ordering, providing upper
bounds. A more expressive semiring is that of natural number
intervals~\citep{DBLP:journals/pacmpl/OrchardLE19}, given by pairs $\mathbb{N}
\times \mathbb{N}$ written $[[ Intrv r s ]]$ here for the lower-bound $r \in
\mathbb{N}$ and upper-bound usage $s \in \mathbb{N}$ with $0 = 0 ... 0$ and $1 =
1 ... 1$, addition and multiplication defined pointwise, and ordering $[[ Intrv
r s ]] \sqsubseteq [[ Intrv r' s' ]] = [[ r' ]] \leq [[ r ]] \wedge [[ s ]] \leq
[[ s' ]]$. Thus a coproduct elimination function can be written and typed:
%
\begin{align*}
\oplus_e & : [[ {[] {Intrv 0 1} (A -o C)} -o {{[] {Intrv 0 1} (B -o C)} -o {(Sum A B) -o C}} ]] \\
\oplus_e & =
\lambda x' . \lambda y' . \lambda z. \textbf{let}\ [x] = x'\ \textbf {in}\ \\ 
          & \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \textbf{let}\ [y] = y'\ \textbf{in}\ \\ 
          & \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \textbf{case}\ z\ \textbf{of}\
\textbf{inl}\ u \rightarrow x\ u\ |\ \textbf{inr}\ v\ \rightarrow y\ v
\end{align*}
\end{example}

Here, $\oplus_{0...1}$ takes two functions as arguments which each have a return
type $C$ and are graded by the interval $0...1$, as well as a linear sum of type $A
\oplus B$. The sum is eliminated via a \textbf{case} statement, where each branch 
uses the appropriate argument function to form an application with type $C$. Despite 
each branch only using one of the argument functions, the program is well-resourced thanks 
to the interval grade $0...1$ giving us the ability to discard the irrelevant function.  

\begin{example}%[Information flow]
\label{exm:security}
%
Graded modalities can capture a form of information-flow security, tracking the
flow of labelled data through a
program~\citep{DBLP:journals/pacmpl/OrchardLE19}, with a 2-point semiring of
security levels where $\mathcal{L} = \{ \text{Private}, \text{Public} \}$ forms a set of
abstract labels, denoting \emph{high} and \emph{low} security permissions
respectively, with a lattice formed by the total order with $\text{Private} \sqsubseteq \text{Public}$.
Multiplication is given by $\sqcup$, and addition by $\sqcap$, with $0$ = $\text{Private}$
and $1$ = $\text{Public}$. 

This allows the following well-typed program, eliminating a pair of $\text{Lo}$
and $\text{Hi}$ security values, picking the left one to pass to a continuation
expecting a $[[ Public ]]$ input:
%
\begin{align*}
\textit{noLeak} & : [[ (Tup {[] Public A} {[] Private A}) -o {({[]
                  Public (Tup A Unit)} -o B) -o B} ]] \\
\textit{noLeak} & = \lambda z . \lambda u . \textbf{let}\ (x',\ y')\ =\ z\ \textbf{in}\ \\
                & \;\;\;\;\;\;\;\;\;\;\; \textbf{let}\ [x]\ =\ x'\ \textbf{in}\ \\ 
                & \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \textbf{let}\ [y]\ =\ y'\ \textbf{in}\ u\ [(x,\ ())]
\end{align*}
\end{example}

\subsection{Metatheory}
Finally, as hinted at on page~\pageref{ref:substitution} of
Chapter~\ref{chapter:background}, the admissibility of substitution is a key
result that holds for this language~\citep{DBLP:journals/pacmpl/OrchardLE19},
which is leveraged in soundness of the synthesis calculi.
%
\begin{restatable}[Admissibility of substitution]{lemma}{linearSubst}
Let $[[ D |- t' : A]]$, then:
\label{lemma:substitution}
\begin{itemize}[leftmargin=1em]
\item (Linear) \hspace{0.04em} If $[[ {G, x : A}
    , { G' } |- t : B]]$ then $[[ G + D + G' |-
[ t' / x ] t : B ]]$
\item (Graded) If $[[ {G, x : [A] r} , { G' } |- t : B]]$
then $[[ G + (r * D) + G' |- [ t' / x ] t : B ]]$
\end{itemize}
\end{restatable}


\section{The Resource Management Problem}
\label{sec:resource-management}

In Chapter~\ref{chapter:intro} we considered a synthesis rule for pairs and
highlighted how graded types could be used to control the number of times
assumptions are used in the synthesised term. 

Chapter~\ref{chapter:intro} considered (Cartesian) product types $\times$, but
in our target language we use the multiplicative product of linear types: 
\begin{align*}
\inferrule*[right = Pair]
  {[[ G1 |- t1 : A ]] \\ [[ G2 |- t2 : B ]]}
  {[[ G1 + G2 |- pair t1 t2 : Tup A B]]}
\end{align*}
Each sub-term is typed by a different
context $[[ G1 ]]$ and $[[ G2 ]]$ which are then combined via context addition,
which equates to \emph{disjoint union} when considering a solely linear setting:
the pair cannot be formed if linear variables are shared between $[[ G1 ]]$ and
$[[ G2 ]]$. This prevents the structural behaviour of \emph{contraction} (where
a variable appears in multiple sub-terms). Na\"{i}vely inverting this typing
rule into a synthesis rule yields:
%
\begin{align*}
  \inferrule*[Right=$\otimes_{\textsc{Intro}}$]{ \Gamma_1 \vdash [[ A ]] \Rightarrow [[ t1 ]] \\ \Gamma_2 \vdash [[ B ]] \Rightarrow [[ t2 ]]}{ \Gamma_1, \Gamma_2 \vdash [[ A ]] \otimes [[ B ]] \Rightarrow ([[ t1 ]],\ [[ t2 ]] )}
\end{align*}
%
As a declarative specification, the $\otimes_{\textsc{Intro}}$ synthesis rule is
sufficient. However, this rule embeds a considerable amount of non-determinism
when considered from an algorithmic perspective. Reading `clockwise' starting
from the bottom-left, given some context $[[ G ]]$ and a goal $A \otimes B$, we
have to split the context into disjoint subparts $[[ G1 ]]$ and $[[ G2 ]]$ such
that $[[ G ]] = [[ G1 , G2 ]]$ in order to pass $[[ G1 ]]$ and $[[ G2 ]]$ to
the sub-goals for $A$ and $B$. For a context of size $n$ there are $2^n$ possible
such partitions! This quickly becomes intractable. Instead,~\cite{HODAS1994327} 
developed a technique for linear logic programming, refined
by~\citet{CERVESATO2000133}, where proof search for linear logic
has both an \emph{input context} of available resources and an \emph{output
context} of the remaining resources, which we write as judgements of the form
$[[ G |- A =>- t | G' ]]$ for input context $[[ G ]]$ and output context $[[ G'
]]$. Synthesis for multiplicative products then becomes:
%
\begin{align*}
  \inferrule*[right=$\otimes_{\textsc{Intro}}^{-}$]{\Gamma_1 \vdash [[ A ]] \Rightarrow^- [[t1]] \ |\ \Gamma_{2} \\ \Gamma_{2} \vdash [[ B ]] \Rightarrow^- [[ t2 ]] \ | \ \Gamma_{3} }{ \Gamma_1 \vdash [[ A Prod B ]] \Rightarrow^- [[ pair t1 t2 ]] \ | \ \Gamma_{3}}
\end{align*}
%
where the remaining resources after synthesising for $A$ the first term $[[ t1
]]$ are $[[ G2 ]]$ which are then passed as the resources for synthesising the
second term $B$. There is an ordering implicit here in `threading through' the
contexts between the premises. For example, starting with a context $[[ x : A, y
: B ]]$, and given the following rule for synthesising a variable usage:
%
\begin{align*}
  \inferrule*[right=$\textsc{Var}$]{\quad}{\Gamma, x : A \vdash A \Rightarrow^{-} x \mid \Gamma}
\end{align*}
%
then the $\otimes^{-}_\textsc{Intro}$ rule may be instantiated as:
%
\begin{align}
\tag{\footnotesize{example}}
  \inferrule*[right=$\otimes_{\textsc{Intro}}^{-}$]{x : A, y : B \vdash [[ A ]] \Rightarrow^- [[x]] \ |\ y : B \\ y : B \vdash [[ B ]] \Rightarrow^- [[ y ]] \ | \ \emptyset }{ x : A, y : B \vdash [[ A Prod B ]] \Rightarrow^- [[ pair x y ]] \ | \ \emptyset}
\end{align}
%
Thus this approach neatly avoids the problem of having to split the input
context, and facilitates efficient proof search for linear types. We extend this
input-output context management model to the graded linear $\lambda$-calculus to
facilitate the synthesis of programs in Granule. We  
term the above approach \textit{subtractive} resource management (in a style
similar to \textit{left-over} type checking for linear type
systems~\citep{allais2018typing,zalakain2020pi}). 

Graded type systems, as we consider them here, have typing contexts in which
free-variables are assigned a type, and a grade. In a graded setting, the
subtractive approach is problematic as there is not necessarily a notion of
actual subtraction for grades. Consider a version of the above example for
subtractively synthesising a pair, but now for a context with some grades $r$
and $s$ on the input variables. Using a variable to synthesise a sub-term now
does not result in that variable being left out of the output context. Instead a
new grade must be assigned in the output context that relates to the first by
means of an additional constraint describing that some usage took place:
%
\begin{align}
\begin{array}{l}
\tag{\footnotesize{example}}
\!\!\!\!\!\inferrule*[right=$\otimes_{\textsc{Intro}}^{-}$]{
\exists r' . r' + 1 = r \\ 
\exists s' . s' + 1 = s \\\\
[[ x : [A] r, y : [B] s ]] \vdash [[ A ]] \Rightarrow^-\! [[x]] \ |\ [[ x : [A] r', y : [B] s ]] \\ 
[[ x : [A] r', y : [B] s ]] \vdash [[ B ]] \Rightarrow^-\! [[ y ]] \ | \ [[ x : [A] r', y : [B] s' ]]}{ [[ x : [A] r, y : [B] s ]]  \vdash [[ A Prod B ]] \Rightarrow^- \![[ pair x y ]] \ | \ [[ x : [A] r', y : [B] s' ]] }\!\!\!
\end{array}
\end{align}
%
In the first synthesis premise, $x$ has grade $r$ in the input context, $x$ is
synthesised for the goal, and thus the output context has some grade $r'$ where
$r' + 1 = r$, denoting that some usage of $x$ occurred (which is represented by
the $1$ element of the semiring in graded systems).

For the natural numbers semiring, with $r = 1$ and $s = 1$ then the constraints
above are satisfied with $r' = 0$ and $s' = 0$. In a general setting, this
subtractive approach to synthesis for graded types requires solving many such
existential equations over semirings, which also introduces a new source of
non-determinism if there is more than one solution. These constraints can be
discharged via an off-the-shelf SMT solver, such as Z3~\citep{z3}. Such calls to an 
external solver are costly, however, and thus efficiency of resource management is 
a key concern. 

We propose a dual approach to the subtractive: the \emph{additive} resource
management scheme. In the additive approach, output contexts describe what was
\emph{used} not what was is \emph{left}. In the case of synthesising a term with
multiple sub-terms (like pairs), the output context from each premise is then
added together using the semiring addition operation applied pointwise on
contexts to produce the final output in the conclusion. For pairs this looks
like:
% %
\begin{align*} \inferrule*[right=$\otimes_{\textsc{Intro}}^{+}$]{\Gamma \vdash
  [[ A ]] \Rightarrow^+ [[t1]] \ |\ \Delta_{1} \\ \Gamma \vdash [[ B ]]
  \Rightarrow^+ [[ t2 ]] \ |\ \Delta_{2} }{ \Gamma \vdash [[ A Prod B ]]
  \Rightarrow^+ [[ pair t1 t2 ]] \ |\ \Delta_{1} + \Delta_{2}} \end{align*}
%
The entirety of $[[G]]$ is used to synthesise both premises. For example, for
a goal of $[[ A Prod A ]]$:
%
\begin{align} \tag{\footnotesize{example}}
\inferrule*[right=$\otimes_{\textsc{Intro}}^{+}$]{ [[ x : [A] r, y : [B] s ]]
\vdash [[ A ]] \Rightarrow^+ [[x]] \ |\ [[ x : [A] 1, y : [B] 0 ]] \\ [[ x :
[A] r, y : [B] s ]] \vdash [[ A ]] \Rightarrow^+ [[ x ]] \ | \ [[ x : [A] 1, y
: [B] 0 ]]  }{ [[ x : [A] r, y : [B] s ]]  \vdash [[ A Prod A ]] \Rightarrow^+
[[ pair x x ]] \ | \ [[ x : [A] {1 + 1} , y : [B] 0 ]]} \end{align}
%
Later checks in synthesis then determine whether the output context describes
usage that is within the grades given by $[[ G ]]$, i.e., that the synthesised
terms are \emph{well-resourced}.

Both the subtractive and additive approaches avoid having to split the incoming
context $[[ G ]]$ into two prior to synthesising sub-terms. 

We adapt the input-output context management model of linear logic synthesis to
graded types, pruning the search space via the quantitative constraints of
grades. It is not immediately apparent which approach has better performance,
thus we implement synthesis calculi based on both the additive and subtractive
approaches, evaluating their performance on a set of benchmarking synthesis
problems. 


\section{The Synthesis Calculi}
\label{sec:linear-base-synthesis}

We now present two synthesis calculi based on the subtractive and additive
resource management schemes, respectively. The structure of the synthesis
calculi mirrors a cut-free sequent calculus, with \textit{left} and
\textit{right} rules for each type constructor. Right rules synthesise an
introduction form for the goal type. Left rules eliminate (deconstruct)
assumptions so that they may be used inductively to synthesise sub-terms. Each
type in the core language has right and left rules corresponding to its
constructors and destructors respectively.

\subsection{Subtractive Resource Management}
\label{subsec:subtractive}
  Our subtractive approach follows the philosophy of earlier work on
  linear logic proof search~\citep{HODAS1994327,CERVESATO2000133},
  structuring synthesis rules around an input context of the available
  resources and an output context of the remaining resources that
  can be used to synthesise subsequent sub-terms. Synthesis rules
  are read bottom-up, with judgments $\Gamma \vdash A \Rightarrow^{-} t\ |\ \Delta$
  meaning from the \emph{goal type} $[[A]]$ we can synthesise a term $[[t]]$ using
  assumptions in $[[G]]$, with output context $[[D]]$. We describe
  the rules in turn to aid understanding. Figure~\ref{fig:sub-rules} collects the
  rules for reference.
  \subsubsection{Variables}
Variable terms can be synthesised from assumptions in the context of linear and
graded assumptions $\Gamma$ by rules:
  \begin{align*}
    \label{sub:grVar}
    \begin{array}{c}
  \hspace{-3em}\subLinVar
  \;\;\;
  \subGrVar
    \end{array}
  \end{align*}
%
On the left, a variable $[[x]]$ may be synthesised for the goal
$[[ A ]]$ if a linear assumption $[[ x : A ]]$ is present
  in the input context. The input context without $[[x]]$ is then returned as
  the output context, since $x$ has been used. On the right,
  we can synthesise a variable $x$ for $A$ if we have a graded
  assumption of $x$ matching the type. However, % to synthesise $[[ x ]]$
  %it may be the case that
  %using $x$ violates the constraints placed by the assumption's grade. For example,
  %the input context may contain assumptions graded by 0, either because this was
  %specified by the type or the variable has been used as far as it's grade
  %permits already. For this reason, the ability to synthesise a
  %graded variable $[[x]]$
%  requires that $[[x]]$'s grade $r$ can be factored into some grade
%  $s + 1$
  the grading $[[ r ]]$ must permit $[[ x ]]$ to be used once here. Therefore,
  the premise states that there exists some grade $s$ such that grade $r$
  approximates $s + 1$. The grade $s$ represents the use of $x$ in further
  synthesis judgements, and thus $[[ x : [ A ] s ]]$ is in the output context.
  For the natural numbers semiring, this constraint is satisfied by $s = r - 1$
  whenever $r \neq 0$, e.g., if $r = 3$ then $s = 2$. For intervals, the role of
  approximation is more apparent: if $r = [[ Intrv 0 3]]$ then this rule is
  satisfied by $s = [[ Intrv 0 2 ]]$ where $s + 1 = [[ Intrv 0 2 ]] + [[ Intrv 1
  1 ]] = [[ Intrv 1 3 ]] \sqsubseteq [[ Intrv 0 3 ]]$. In the natural numbers
  semiring, this existential grade variable could be instantiated by simply
  subtracting $1$ from the assumption's existing grade $r$. However, as not all
  semirings have an additive inverse, this is instead handled via a constraint
  on the new grade $s$, requiring that $ r \sqsupseteq s + 1 $. In the
  implementation, the constraint is discharged via an SMT solver, where an
  unsatisfiable result terminates this branch of synthesis.

  \subsubsection{Functions}
In typing, $\lambda$-abstraction binds linear variables to introduce
  linear functions. Synthesis from a linear function type therefore mirrors typing:
%
  \begin{align*}
\subAbs
    \end{align*}
%
  Thus, $\lambda x . t$ can be synthesised given that
  $t$ can be synthesised from $B$ in the context of $[[G]]$ extended with a fresh linear assumption $[[ x
  : A]]$. To ensure that $[[x]]$ is used linearly
  by $[[t]]$ we must therefore check that it is not present in
  $[[D]]$.

The left-rule for linear function types then synthesises applications
(as in~\cite{HODAS1994327}):
%
  \begin{align*}
    \hspace{-1em}\subApp
    \end{align*}
%
  %The left rule equivalent of $\textsc{R}\multimap^{-}$ for
  %synthesising an abstraction is the rule for
  %synthesising an application $\textsc{L}\multimap^{+}$.
  The rule synthesises a term for type $[[ C ]]$ in a context that
  contains an assumption $[[ x1 : A -o B ]]$.
%
%, we can apply some value
%  of type $[[A]]$ to this to obtain a value of type $[[B]]$ to use in the
  %synthesis of $[[t1]]$ from the goal type $[[C]]$.
  The first premise synthesises a term $[[t1]]$ for $[[C]]$ under the context
  extended with a fresh linear assumption $[[x2 : B]]$, i.e., assuming the
  result of $[[ x1 ]]$. This produces an output context $[[D1]]$ that must not
  contain $[[x2]]$, i.e., $[[x2]]$ is used by $[[t1]]$. The remaining
  assumptions $[[D1]]$ provide the input context to synthesise $[[t2]]$ of type
  $[[A]]$: the argument to the function $[[x1]]$. This structure corresponds to a
  left-elimination rule of function types in sequent calculus. In the
  conclusion, the application $[[x1 t2]]$ is substituted for $[[x2]]$ inside
  $[[t1]]$, and $[[D2]]$ is the output context.

\subsubsection{Dereliction} Note that the above rule synthesises the application of a
function given by a linear assumption. What if we have a graded
assumption of function type? Rather than duplicating every left rule
for both linear and graded assumptions, we mirror the
dereliction typing rule (converting a linear assumption to graded) as:
%
  \begin{gather*}
    {\small{
    \subDer
    }}
    \end{gather*}
%
Dereliction captures the ability to reuse a graded assumption being considered
in a left rule. A fresh linear assumption $[[y]]$ is generated that represents a
use of the graded assumption $x$ when used in a left rule, and must be used
linearly in the subsequent synthesis of $[[t]]$. As with the
$\multimap^{-}_{\textsc{L}}$ rule, the use of $y$ is substituted for $x$ in the
body of the synthesised term $t$. 

The output context of the premise then contains $[[x]]$ graded by $s'$, which
reflects how $[[x]]$ was used in the synthesis of $[[t]]$, i.e. if $[[x]]$ was
not used then $s' = s$. The premise $[[ exists s . r >= s + 1 ]]$ constrains the
number of times dereliction can be applied so that it does not exceed $x$'s
original grade $r$.

One may observe that the $\subDerName$ rule makes the presence of the
$\subGrVarName$ rule admissible. Synthesising the usage of a graded variable can
instead be achieved through the use of dereliction on the graded assumption,
followed by the $\subLinVarName$. For example, the following derivation for
synthesising a value of type $A$ from a context with the graded assumption $x :_1
A$ using $\subDerName$:
\begin{align*}
  \inferrule*[right=\subDerName]
    {\inferrule*[right=\subLinVarName, leftskip=3.8em]{\quad}{x :_s A, y : A \vdash A \Rightarrow^- y \mid x :_s A} \\ y \notin x :_s A \\ \exists s . 1 \sqsupseteq s + 1}
    {x :_1 A \vdash A \Rightarrow^{-} [x / y] y \mid x :_s A}
\end{align*}
is equivalent to using the $\subGrVarName$ rule (after applying the substitution in the $\subDerName$ rule's conclusion): 
\begin{align*}
  \inferrule*[right=\subGrVarName]
      {\exists s . 1 \sqsupseteq s + 1}
      { x :_1 A \vdash A \Rightarrow^{-} x \mid x :_s A}
\end{align*}
Nevertheless, we find the inclusion of
$\subGrVarName$ useful as an explanatory tool and optimisation in the
implementation of the calculus. 


  \subsubsection{Graded modalities}
  For a graded modal goal type $[[ [] r A ]]$, we synthesise a promotion
$[[ [ t ] ]]$ if we can synthesise the `unpromoted' $[[t]]$ from $[[A]]$:
%Synthesis of promoted values is captured by the
%  rule $\textsc{R}\square^{-}$.
  \begin{align*}
    \subBox
    \end{align*}
%
  A non-graded value $[[t]]$ may be promoted to a graded value using
  the box syntactic construct.
  Recall that typing of a promotion $[[ [ t ] ]]$
  scales all the graded assumptions used to type $[[ t ]]$ by $r$. Therefore,
  to compute the output context we must ``subtract'' $r$-times the use of the variables in $[[
  t ]]$. However, in the subtractive model $[[ D ]]$ tells us what is
  left, rather than what is used. Thus we first compute the
  \textit{context subtraction} of $[[G]]$ and $[[D]]$
  yielding the variable usage information about $[[ t ]]$:
 %
  \begin{definition}[Context subtraction]\label{def:contextSub}
  For all $[[ G1 ]], [[ G2 ]]$ where $ [[G2]] \subseteq [[G1]]$, $[[G1 - G2]] =$
\begin{align*}
\hspace{-2em}\left\{\!\begin{matrix}
\begin{array}{lll}
% Base case
[[G1]]
  & [[G2]] = \emptyset
\\[0.25em]
([[G1']], [[G1'']]) - [[G2']]
  & [[G2]] = [[ G2', x : A]] & \wedge\ [[G1]] = [[G1', x : A]], [[G1'']]
\\[0.25em]
(([[G1']], [[G1'']]) - [[G2']]), [[x : [A] q]]
  & [[ G2]] = [[G2', x : [A] s]] & \wedge\ [[G1]] = [[ G1',x : [A]
                                   r]],[[G1'']] \\[0em]
          & \wedge \ [[ exists q . r >= q + s]]
          & \!\! \wedge \ \maximal{q}{q'}{r}{q' + s}
\end{array}
\end{matrix}\right.
\end{align*}
%
\end{definition}
Note that this is an algorithmic definition of context subtraction. As in graded
variable synthesis, context subtraction existentially quantifies a variable $q$
to express the relationship between grades on the right being ``subtracted''
from those on the left. The last conjunct states $q$ is the greatest element
(wrt. to the pre-order) satisfying this constraint, i.e., for all other $q' \in
\mathcal{R}$ satisfying the subtraction constraint then $[[ q >= q']]$ e.g., if
$r = [[ Intrv 2 3 ]]$ and $s = [[ Intrv 0 1]]$ then $q = [[ Intrv 2 2 ]]$
instead of, say, $[[ Intrv 0 1]]$. This \emph{maximality} condition is important
for soundness (i.e. that synthesised programs are well-typed); we discuss soundness further on page \pageref{soundness-ref}.

Thus for \subBoxName, $[[ G - D ]]$ is multiplied by the goal type grade $r$ to obtain how these
  variables are used in $[[t]]$ after promotion. This is then subtracted from
  the original input context $[[G]]$ giving an output context
  containing the left-over variables and grades. Context
  multiplication
  requires that $[[G - D]]$ contains only graded variables,
  preventing the incorrect use of linear variables from $[[G]]$ in
  $[[t]]$. For example, the synthesis derivation:
\begin{align*}
  \inferrule*[right=\subBoxName]
    {\inferrule*[right=\subGrVarName]
      {\exists s . 0...2 \sqsupset s + 1}
      {x :_{0...2} A \vdash A \Rightarrow^- x \mid  x :_s A}
    }
    {x :_{0...2} \vdash \Box_0..1 \Rightarrow^- [ x ] \mid x :_{0...2} A - {0...1} \cdot (x :_{0...2} A - x :_s A)}
\end{align*}
would be valid. Here, the subtraction $x :_{0...2} A - x :_s A$ yields a grade
variable $q_1$ with the constraint $\exists q_1 . {0...2} \sqsupseteq q_1 + {0...1}$
and $q'_1$ with the maximality constraint $\forall q'_1 . {0...2} \sqsupseteq q'_1 +
{0...1} \Rightarrow q_1 \sqsupseteq q'_1$, satisfied by $q_1 = {0...1}$, and $q'_1 = q_1$, as $s$ is 
satisfied by $0...1$. Finally, 
${0...1} \cdot x :_{q_1} A$ is subtracted from $x :_{0...2} A$ to obtain the left over usage: 
${0...1} \cdot x :_{q_1} A$ is $x :_{q_1} A$, thus the subtraction yields a grade $q_2$ with 
constraint $\exists q_2 . {0...2} \sqsupseteq q_2 + q_1 $ and $q'_2$ with maximality constraint 
$\forall q'_2 . {0...2} \sqsupseteq q'_2 + q_1 \Rightarrow q_2 \sqsupseteq q'_2$, which is satisfied 
by $q_2 = 0...1$; the final output grade for $x$ in the \subBoxName rule.



Synthesis of graded modality elimination, is handled by the
  \subUnboxName\ left rule:
  \begin{align*}
    \subUnbox
    \end{align*}
%
  Given an input context comprising $[[ G ]]$ and a linear assumption $[[ x1 ]]$
  of graded modal type, we can synthesise an unboxing of $[[x1]]$ if we can
  synthesise a term $[[t]]$ under $[[G]]$ extended with a graded assumption
  $[[x2 : [A] r]]$. This returns an output context that must contain $[[x2]]$
  graded by $s$ with the constraint that $s$ must approximate $0$, as in the
  following synthesis derivation where $x_1$ is an assumption with a graded
  modality of grade $0$, i.e. it \emph{must} be discarded after unboxing:
  \begin{align*}
    \inferrule*[right=\subUnboxName]
      {\inferrule*[right=\subLinVarName]
        {\quad}
        {y : A, x_2 :_0 A \vdash A \Rightarrow^- y \mid x_2 :_0 A}
      \\ 0 \sqsubseteq 0}
      {y : A, x_1 : \Box_0 A \vdash A \Rightarrow^- \textbf{let}\ [x_2] = x_1\ \textbf{in}\ y \mid \emptyset }
  \end{align*} 
  This enforces
  that $x_2$ has been used as is permitted by the grade $r$.



\subsubsection{Products}
The right rule for products \subPairIntroName\ behaves similarly to the
\subAppName\ rule, again synthesising two sub-terms, and passing the
entire input context $[[ G ]]$ to the first premise. This is in then used to
synthesise the first sub-term of the pair $[[ t1 ]]$, yielding an output context
$ [[ D1 ]]$, which is passed to the second premise. After synthesising the
second sub-term $[[ t2 ]]$, the output context for this premise becomes the
output context of the rule's conclusion.

The left rule equivalent \subPairElimName\  binds two assumptions
$[[ x1 : A ]]$ $[[ x2 : B ]]$ in the premise, representing the constituent parts
of the pair. As with \subAppName, we also ensure that these bound assumptions must not be
 present in the premise's output context $[[ D ]]$.

\begin{align*}
\begin{array}{c}
  \subPairIntro
\\[1.25em]
  \subPairElim
\end{array}
\end{align*}
\subsubsection{Sums}
The introduction rules for sum types, \subSumIntroLname\ and \subSumIntroRname, are straightforward:
\begin{align*}
  \subSumIntroL
\;\;\;
  \subSumIntroR
\end{align*}
The \subSumElimName\ rule synthesises the left and
right branches of a case statement that may use resources
differently:
\begin{align*}
\begin{array}{c}
\hspace{-3em}{\small{\subSumElim}}
\end{array}
\end{align*}
The output context therefore takes the \textit{greatest
lower bound} ($\sqcap$) of $[[ D1 ]]$ and $[[ D2 ]]$, given by definition~\ref{def:context-glb},
\begin{definition}[Partial greatest-lower bounds of
  contexts]\label{def:context-glb}
For all $[[ G1 ]]$, $[[ G2 ]], [[G1]] \sqcap [[G2]] =$
\begin{align*}
\left\{\begin{matrix}
\begin{array}{lll}
%% Both empty case
\emptyset
  & [[ G1 ]] = \emptyset & \wedge \; [[ G2 ]] = \emptyset
\\
%
%% Left empty
(\emptyset \sqcap [[ G2' ]]), [[ x : [ A ] {glb 0 s} ]]
  & [[ G1 ]] = \emptyset & \wedge \; [[G2]] = [[ G2',x : [A] s]]
\\
%
%% Left is left linear
([[G1']] \sqcap [[(G2',G2'')]]), [[x : A]]
 & [[G1]] = [[{G1', x : A} ]] & \wedge \; [[ G2 ]] = [[ {G2', x : A}, G2'' ]]
\\
%
%% Left is graded
([[G1']] \sqcap [[(G2',G2'')]]), [[x : [A] {glb r s}]]\;\;
 & [[G1]] = [[ G1',x : [A] r]] & \wedge \; [[ G2 ]] = [[{G2', x : [A] s}, G2'']]
\end{array}
\end{matrix}\right.
\end{align*}
where $r\!\sqcap\!s$ is the greatest-lower bound of grades $[[r]]$ and $[[s]]$
if it exists, derived from the pre-order $\sqsubseteq$ (given by
Definition~\ref{def:glb}). If the greatest lower bound of two grades does not
exist, then the operation fails, terminating the branch of synthesis. 
\end{definition}
%
%

This operation is the dual to least-upper bound
(Definition~\ref{def:context-lub}) operation used in the typing rule for
$\textsc{Case}$, as here we are subtracting usages rather than adding them. As
an example of $\sqcap$, consider the semiring of intervals over natural numbers
and two judgements that could be used as premises for the (\subSumElimName)
rule:
%
\begin{align*}
& [[ G, y : [A'] Intrv 0 5, x2 : A |- C =>- t1 ; y : [A'] Intrv 2 5 ]] \\
& [[ G, y : [A'] Intrv 0 5, x3 : B |- C =>- t2 ; y : [A'] Intrv 3 4 ]]
\end{align*}
%
where $t_1$ uses $y$ such that there are $2$-$5$ uses remaining
and $t_2$ uses $y$ such that there are $3$-$4$
uses left. To synthesise $\textbf{case} \ x_{1}\ \textbf{of}\ \textbf{inl}\ x_{2} \rightarrow t_{1};\ \textbf{inr}\ x_{3} \rightarrow t_{2}$
the output context must be pessimistic about what resources are left,
thus we take the greatest-lower bound yielding the interval $
2\dots4 $ here: we know $y$ can be used at least twice and at most
$4$ times in the rest of the synthesised program.

\subsubsection{Unit}
The right and left rules for units are then
self-explanatory following the subtractive resource model:
%
\begin{align*}
  \hspace{-3em}
\begin{array}{c}
  \subUnitIntro
  \;\;
  \subUnitElim
\end{array}
\end{align*}


\begin{figure}[t]
\begin{gather*}
{\footnotesize{
  \hspace{-3em}\begin{array}{c}
    \subLinVar
    \;\;\;
    \subGrVar
    \\[1.25em]
    \subAbs
    \\[1.25em]
    \subApp
    \\[1.25em]
    \subDer
    \\[1.25em]
    \subBox
    \\[1.25em]
    \subUnbox
    \\[1.25em]
    \subPairIntro
    \\[1.25em]
    \subPairElim
    \\[1.25em]
    \subSumIntroL
    \;\;\;
    \subSumIntroR
    \\[1.25em]
    \subSumElim
    \\[1.25em]
    \subUnitIntro
    \;\;\;
    \subUnitElim
  \end{array}
}}
\end{gather*}
\caption{Collected rules of the subtractive synthesis calculus}
\label{fig:sub-rules}
  \end{figure}
%

\subsubsection{Soundness of Subtractive Synthesis}
\label{sec:linear-base-sub-sound}
This completes subtractive synthesis calculus. We conclude
with a key result, that synthesised terms are well-typed at the type from which they
were synthesised:
\label{soundness-ref}
%
\begin{restatable}[Subtractive synthesis soundness]{lemma}{subSynthSound}
\label{lemma:subSynthSound}
For all $[[ G ]]$ and $[[ A ]]$
then:
\begin{align*}
[[ G |- A =>- t ; D ]] \quad \implies \quad [[ G - D |- t : A ]]
\end{align*}
i.e. $[[ t ]]$ has type $[[ A ]]$
under context $[[ G - D ]]$,
that contains just those linear and
graded variables with grades reflecting their use in $[[ t ]]$.
\end{restatable}
The proof of soundness can be found in Section~\ref{proof:linear-sub-sound} of Appendix~\ref{appendix:proofs}. 

The proof is fairly straightforward by induction. As a simple example, consider the 
case of the $\multimap_\textsc{R}^-$ rule. By induction on the premise of the 
rule we have:
\begin{align*}
  [[ (G, x : A) - D |- t : B ]]
\end{align*}
Then, since $[[x notin D]]$ then by the definition of context
    subtraction we have that $[[ (G, x : A) - D ]] = [[ (G - D), x : A ]]$.
    From this, we can construct the following derivation, matching the
    conclusion:
    %
    \begin{align*}
    \inferrule*[Right=Abs]
    {[[ (G - D) , x : A |- t : B]]}{[[ G - D |- \x. t : A -o B ]]}
    \end{align*}

\subsection{Additive Resource Management}
\label{subsec:additive}
We now present the dual to subtractive resource management --- the
\emph{additive} approach.
Additive synthesis also uses the input-output context approach, but where
output contexts describe exactly which assumptions were used to synthesise
a term, rather than which assumptions are still available. As with subtractive, additive
   synthesis rules are best read bottom-up, with $[[G |- A =>+ t; D]]$
  meaning that from the type $[[A]]$ we synthesise a term $[[t]]$ using
  exactly the assumptions $[[D]]$ that originate from the input
  context $[[G]]$. The rules are collected in Figure~\ref{fig:add-rules}. 

  \subsubsection{Variables}
  We unpack the rules, starting with variables:
%
\begin{align*}
  \begin{array}{c}
  \hspace{-3em}\addLinVar
  \;\;\;
  \addGrVar
  \end{array}
  \end{align*}
%
For a linear assumption, the output context contains just the variable that was
synthesised. For a graded assumption $[[x : [A] r]]$, the output context
contains the assumption graded by $1$. 

\subsubsection{Graded modalities}
The subtractive approach handled the \textsc{GrVar$^{-}$}
by a constraint $[[ exists s . r >= s + 1]]$. Here however, the
point at which we check that a graded assumption has been used
according to the grade takes place in the \addUnboxName\ rule, where graded
assumptions are bound:
%
\begin{align*}
  \addUnbox
  \end{align*}
%
Here, $[[t]]$ is synthesised under a fresh graded assumption $[[ x2 : [A] r]]$.
This produces an output context containing $[[x2]]$ with some grade $s$ that
describes how $[[x2]]$ is used in $[[t]]$ (if it was used at all). An additional
premise requires that the original grade $r$ approximates either $s$ if $[[x2]]$
appears in $[[D]]$ or $0$ if it does not, ensuring that $[[x2]]$ has been used
correctly. For the $\mathbb{N}$-semiring with equality as the ordering, this
would ensure that a variable has been used exactly the number of times specified
by the grade.

The synthesis of a promotion is considerably simpler in the additive
approach. In subtractive resource management it was necessary to calculate how
resources were used in the synthesis of $[[t]]$ before then applying the
scalar context multiplication by the grade $r$ and subtracting this from the
original input $[[G]]$. In additive resource management, however, we can simply
apply the multiplication directly to the output context $[[D]]$ to obtain how
our assumptions are used in $[[ [t] ]]$:
%
\begin{align*}
  \addBox
\end{align*}

\subsubsection{Functions}
Synthesis rules for $\multimap$ have a similar shape to the
subtractive calculus:
%
\begin{align*}
\begin{array}{c}
\addAbs
\\[1.25em]
\addApp
\end{array}
\end{align*}
%
Synthesising an abstraction (\addAbsName) requires that $[[x : A]]$ is in the
output context of the premise, ensuring that linearity is preserved. Likewise
for application (\addAppName), the output context of the first premise must
contain the linearly bound $[[x2 : B]]$ and the final output context must
contain the assumption being used in the application $[[ x1 : A -o B ]]$. This
output context computes the \emph{context addition} (Def.~\ref{def:contextAdd}
on page\pageref{def:contextAdd}) of both output contexts of the premises $[[D1 +
D2]]$. If $[[D1]]$ describes how assumptions were used in $[[t1]]$ and $[[D2]]$
respectively for $[[t2]]$, then the addition of these two contexts describes the
usage of assumptions for the entire subprogram. Recall, context addition ensures
that a linear assumption may not appear in both $[[D1]]$ and $[[D2]]$,
preventing us from synthesising terms that violate linearity.



\subsubsection{Dereliction}
As in the subtractive calculus,
%the additive equivalent of dereliction ($\textsc{Der}$) also allows us to
%reuse graded assumptions in a left rule:
we avoid duplicating left rules to
match graded assumptions by giving a synthesising version of dereliction:
\begin{align*}
  \addDer
  \end{align*}
%
The fresh linear assumption $[[ y : A ]]$ must
appear in the output context of the premise, ensuring it is used. The final
context therefore adds to $[[ D ]]$ an assumption of $[[x]]$ graded by
$1$, accounting for this use of $[[ x ]]$ (which was temporarily renamed to
$y$).
As with the subtractive case, $\addDerName$ makes $\addGrVarName$ admissible. 

\subsubsection{Products}
The right rule for products \addPairIntroName\ follows the same structure as its
subtractive equivalent, however, here $[[ G ]]$ is passed to both premises.
The conclusion's output context is then formed by taking the context addition of
the $[[ D1 ]]$ and $[[ D2 ]]$. The left rule, \addPairElimName\ follows fairly
straightforwardly from the resource scheme.
\begin{align*}
\begin{array}{c}
  \addPairIntro
\\[0.8em]
  \addPairElim
\end{array}
  \end{align*}

\subsubsection{Sums}
In contrast to the subtractive rule, the rule \addSumElimName\ takes the least-upper bound of
the premise's output contexts (see definition~\ref{def:context-lub}). Otherwise,
the right and left rules for synthesising programs from sum types are straightforward.
\begin{align*}
\begin{array}{c}
  \hspace{-2em}\addSumIntroL
  \;\;\;
  \addSumIntroR
\\[1.25em]
\hspace{-2em}{\footnotesize{\!\!\addSumElim}}
\end{array}
  \end{align*}

\subsubsection{Unit}
As in the subtractive approach, the right and left rules for unit types, are
as expected.
\begin{align*}
\begin{array}{c}
  \hspace{-3em}\addUnitIntro
  \;\;\;
  \addUnitElim
\end{array}
  \end{align*}


\begin{figure}[t]
{\footnotesize{
\begin{gather*}
\hspace{-3em}\begin{array}{c}
  \addLinVar
  \;\;\;
  \addGrVar
\\[1.25em]
  \addDer
\\[1.25em]
  \addAbs
\\[1.25em]
  \addApp
\\[1.25em]
  \addBox
\\[1.25em]
  \addUnbox
\\[1.25em]
  \addPairIntro
\\[1.25em]
  \addPairElim
\\[1.25em]
  \addSumIntroL
\;\;\;
  \addSumIntroR
\\[1.25em]
  \addSumElim
\\[1.25em]
  \addUnitIntro
\;\;\;
  \addUnitElim
      \end{array}
  \end{gather*}
}}
\caption{Collected rules of the additive synthesis calculus}
\label{fig:add-rules}
  \end{figure}


  \subsubsection{Soundness of Additive Synthesis}
  \label{subsec:add-sound}
  Thus concludes the rules for additive synthesis. As with subtractive, we
  have prove that this calculus is sound.
  \begin{restatable}[Additive synthesis soundness]{lemma}{addSynthSound}
\label{lemma:addSynthSound} 
Given a particular pre-ordered semiring $\mathcal{R}$ parametrising the calculi,
then, for all contexts $[[ G ]]$ and $[[ D ]]$, types $[[ A ]]$ and terms $[[ t ]]$:
%
\begin{align*}
[[ G |- A =>+ t ; D ]] \quad \implies \quad [[ D |- t : A ]]
\end{align*}
\end{restatable}
Thus, the synthesised term $[[ t ]]$ is well-typed
at $[[ A ]]$ using only the assumptions $[[ D ]]$ whose grades capture variable use 
in $[[ t ]]$.
i.e., synthesised terms are well typed at the type from which they
were synthesised.

In the additive calculus, the soundness on its own does not
guarantee that a synthesised program $t$ is \emph{well resourced}, i.e., the
grades in $[[ D ]]$ may not be approximated by the grades in $[[ G ]]$. For
example, a valid judgement under
semiring $\mathbb{N}_\equiv$ is:

\begin{align*}
[[ x : [A] 2 |- A =>+ x ; x : [A] 1 ]]
\end{align*}

i.e., for goal $A$, if $x$ has type $A$ in the context then we synthesis $x$ as
the result program, regardless of the grades. A synthesis judgement such as this
may be part of a larger derivation in which the grades eventually match, i.e.,
this judgement forms part of a larger derivation which has a further
sub-derivation in which $x$ is used again and thus the total usage for $x$ is
eventually $2$ as prescribed by the input context. However, at the level of an
individual judgement we do not guarantee that the synthesised term is
well-resourced. A reasonable \emph{pruning condition} that could be used to
assess whether any synthesis judgement is \emph{potentially} well-resourced is
$\exists [[ D' ]] . [[ (D + D') <<= G ]]$, i.e., there is some additional usage
$[[ D' ]]$ (that might come from further on in the synthesis process) that
`fills the gap' in resource use to produce $[[ D + D' ]]$ which is
overapproximated by $[[ G ]]$. In this example, $[[ D' ]] = [[ x : [A] 1 ]]$
would satisfy this constraint, explaining that there is some further possible
single usage which will satisfy the incoming grade. 

We apply this pruning condition at the synthesis of terms which are binders. 
Therefore, synthesised closed terms are always well-resourced.

The proof of soundness can be found in Section~\ref{proof:linear-add-sound} of
Appendix~\ref{appendix:proofs}.

As in Section~\ref{sec:linear-base-sub-sound}, the proof is fairly
straightforward, by induction. Again, consider the $\multimap_\textsc{R}^+$ rule 
as an example. By induction we then have that:
%
\begin{align*}
  [[ (G, x : A) - D |- t : B ]]
\end{align*}
%
Since $[[x notin D]]$ then by the definition of context
subtraction we have that $[[ (G, x : A) - D ]] = [[ (G - D), x : A ]]$.
From this, we can construct the following derivation, matching the
conclusion:
%
\begin{align*}
\inferrule*[Right=Abs]
{[[ (G - D) , x : A |- t : B]]}{[[ G - D |- \x. t : A -o B ]]}
\end{align*}

\subsubsection{Additive pruning}
%
As seen above, the additive approach delays checking whether a variable is used
according to its linearity/grade until it is bound, i.e. in the \addAbsName,
\addAppName, \addDerName, and \addUnboxName\ rules. We hypothesise that this can
lead additive synthesis to explore many ultimately ill-typed (or
\emph{ill-resourced}) paths for too long. For example, say we have a partial
synthesis derivation for synthesising a pair introduction form of type $A
\otimes A$, but our context contains only one linear assumption of type $A$.
Clearly we can use this assumption to synthesise a term for the left part of the
pair: 
\begin{align*}
  \inferrule*[right=\addPairIntroName]
    {\inferrule*[right=\addLinVarName]{\quad}{x : A \vdash A \Rightarrow^{+} x \mid x : A} \\ x : A \vdash A \Rightarrow^+\ ? \mid ?}
    {x : A \vdash A \otimes A \Rightarrow^+ ? \mid ?}
\end{align*}
However, after synthesising the left part we no longer the available usage of
$x$ to synthesise a term for right part of the pair. Currently,
\addPairIntroName\ will allow the synthesis of the right part of the pair to
take place using $x$, which will then be discarded when the context addition in the 
rule's output context fails: 
conclusion\begin{align*}
  \tag{invalid}
  \inferrule*[right=\addPairIntroName]
    {\inferrule*[right=\addLinVarName]{\quad}{x : A \vdash A \Rightarrow^{+} x \mid x : A} \\ x : A \vdash A \Rightarrow^{+}\ x \mid x : A}
    {x : A \vdash A \otimes A \Rightarrow^+ (x, x) \mid x : A + x : A}
\end{align*}
Subsequently, we define a ``pruning''
variant of any additive rules with multiple sequenced premises. For
\addPairIntroName\ this is:
%
\begin{align*}
  \begin{array}{c}
    \addPrunePairIntroHighlight
  \end{array}
\end{align*}
%
Instead of passing $[[G]]$ to both premises, $[[G]]$ is the input only for the
first premise. This premise outputs context $[[D1]]$ that is subtracted from
$[[G]]$ (highlighted in $\rulehighlight{\text{yellow}}$) to give the input
context of the second premise. This provides an opportunity to terminate the
current branch of synthesis early if $[[ G - D1 ]]$ does not contain the
necessary resources to attempt the second premise, based on
Definition~\ref{def:contextSub} which may fail. The \addAppName\ rule is
similarly adjusted:

\begin{align*}
  \begin{array}{c}
    \addPruneAppHighlight
  \end{array}
\end{align*}

\begin{restatable}[Additive pruning synthesis soundness]{lemma}{addPruningSynthSound}
\label{lemma:addPruningSynthSound} For all $[[ G ]]$ and $[[ A ]]$:
%
\begin{align*}
\Gamma \vdash A \Rightarrow^{\pm} t \mid \Delta \quad \implies \quad [[ D |- t : A ]]
\end{align*}
\end{restatable}
The proof of soundness can be found in Section~\ref{sec:add-pruning-sound} of
Appendix~\ref{appendix:proofs}

\section{Focusing}
\label{sec:linear-base-focusing}

The additive and subtractive calculi presented in
Sections~\ref{subsec:subtractive} and~\ref{subsec:additive} provide the
foundations for the implementations of a synthesis tool for Granule programs.
Implementing the rules as they currently stand, however, would yield a highly
inefficient tool. In their current form, the rules of both calculi exhibit a
high degree of non-determinism with regard to order in which rules
can be applied. 
% Examples too wide!!! Need a slimmer one
% For example, consider the following scenario in the additive system:
% \begin{align*} 
% %     x : (A \otimes B) \otimes (C \otimes D) \vdash (D \otimes C) \otimes (B \otimes A) \Rightarrow^{+}\ ? \mid\ ?
%     x :_{0 .. 5} A \otimes B \vdash A \multimap (A \otimes B) \Rightarrow^{+}\ ?\ |\ ? 
% \end{align*}
% Here $x$ is a linear assumption containing a pair of pairs. To reach the goal, $x$ needs to be 
% broken down into its constituent parts using the \addPairElimName rule, and then reconstructed 
% using the \addPairIntroName rule. The exact order in which these rules are applied is irrelevant: the 
% final synthesised terms will be behaviourally equivalent: 
% \begin{align*}
% \inferrule*[right=\addPairElimName] 
%   {\inferrule*[right=\addPairElimName]
%     {\inferrule*[right=\addPairElimName]
%       {} 
%       {y_1 : A,\ y_2 : B,\ x_2 : (C \otimes D)}}
%     {x_1 : (A \otimes B),\ x_2 : (C \otimes D) \vdash (D \otimes C) \otimes (B \otimes A) \Rightarrow^{+}\ \textbf{let}\ (x_1,\ x_2) = x\ \textbf{in}\ ? \mid\ ? }}
%   {x : (A \otimes B) \otimes (C \otimes D) \vdash (D \otimes C) \otimes (B \otimes A) \Rightarrow^{+}\ t \mid\ ?}
% \end{align*}
% where $t$ = 
% \begin{align*}
%   \begin{array}{ll}
%   & \textbf{let}\ (x_1,\ x_2) = x\ \textbf{in} \\ 
%   & \;\;\;\;\;\; \textbf{let}\ (y_1,\ y_2) = x_1\ \textbf{in} \\
%   & \;\;\;\;\;\;\;\;\;\;\; \textbf{let}\ (z_1,\ z_2) = x_2\ \textbf{in}\ ((z_2,\ z_1),\ (y_2,\ y_1))
%   \end{array}
% \end{align*}


This leads to us exploring a large number of redundant search branches:
something which can be avoided through the application of a technique from
linear logic proof theory called \textit{focusing}~\citep{focusing}. Focusing is
based on the observation that some of the synthesis rules are invertible, i.e.
whenever the conclusion of the rule is derivable, then so are its premises. In
other words, the order in which we apply invertible rules doesn't matter. By
fixing a particular ordering on the application of these rules, we eliminate
much of the non-determinism that arises from trying branches which differ only
in the order in which invertible rules are applied. This focused approach to
synthesis ensures that we are restricted to generating programs in
$\beta$-normal form.

We take both of our calculi and apply this focusing technique to them,
yielding two \textit{focusing} calculi. To do so, we augment our previous
synthesis judgement with an additional input context $\Omega$:
\begin{align*}
\Gamma ; \Omega \vdash [[ A ]] \Rightarrow [[t ]]\ |\ \Delta
\end{align*}
Unlike $\Gamma$ and $\Delta$, $\Omega$ is an \textit{ordered} context, which
behaves like a stack. Assumptions with types that can be broken down further are
then bound into $\Omega$. 

Using the terminology of~\citet{focusing}, we refer to rules that are invertible
as \textit{asynchronous} and rules that are not as \textit{synchronous}. The
intuition is that of asynchronous communication: asynchronous rules can be
applied eagerly, while the non-invertible synchronous rules require us to
\textit{focus} on a particular part of the judgement: either on the assumption
(if we are in an elimination rule) or on the goal (for an introduction rule).
When focusing we apply a chain of synchronous rules until:
\begin{itemize}
  \item We reach a position where no rules may be applied (at which point the branch terminates).
  \item We have synthesised a term for our goal.
  \item We have exposed an asynchronous connective at which point we switch
\end{itemize}
  back to applying asynchronous rules.
We divide our synthesis rules into four categories, each with their own
judgement form, which refines the focusing judgement above with an arrow
indicating which part of the judgement is currently in focus. An $\Uparrow$
indicates an asynchronous phase, while a $\Downarrow$ indicates a synchronous
(focused) phase. The location of the arrow in the judgement indicates whether we
are focusing on the left or right:
\begin{enumerate}
  \item \textsc{Right Async}: $\multimap_{\textsc{R}}$ rule with the judgement:
        \begin{align*}\Gamma ; \Omega \vdash A \Uparrow\ \Rightarrow t\ |\ \Delta \end{align*}
        \item \textsc{Left Async}:  $\otimes_{\textsc{L}}$, $\oplus_{\textsc{L}}$, $\mathsf{Unit}_{\textsc{L}}$, $\textsc{Der}$, and $\Box_{\textsc{L}}$ rules with the judgement:
        \begin{align*}\Gamma ; \Omega \Uparrow\ \vdash A \Rightarrow t\ |\ \Delta \end{align*}
        \item \textsc{Right Sync}:  $\otimes_{\textsc{R}}$, $\oplus1_{\textsc{R}}$, $\oplus2_{\textsc{R}}$, $\mathsf{Unit}_{\textsc{R}}$, and $\Box_{\textsc{R}}$ rules with the judgement:
        \begin{align*}\Gamma ; \Omega \vdash A \Downarrow\ \Rightarrow t\ |\ \Delta \end{align*}
        \item \textsc{Left Sync}:   $\multimap_{\textsc{L}}$ rule with the judgement:
        \begin{align*}\Gamma ; \Omega \Downarrow\ \vdash A \Rightarrow t\ |\ \Delta \end{align*}
\end{enumerate}

The complete calculi of focusing synthesis rules are given in
Figures~\ref{fig:focus-sub-right-async}-\ref{fig:focus-sub-left-sync} for the
subtractive calculus,
and~\ref{fig:focus-add-right-async}-\ref{fig:focus-add-left-sync} for the
additive, divided into focusing phases. The focusing rules for the additive
pruning calculus are identical to the additive calculus, save for the
$\otimes^{+}_{\textsc{R}}$ and $\multimap^{+}_{\textsc{L}}$ rules, which are
given in Figure~\ref{fig:focus-add-pruning}.

For the most part, the translation from non-focused to focused rules is
straightforward. The most notable change occurs in rules in which assumptions
are bound. In the cases where a fresh assumption's type falls into the \textsc{Left
Async} category (i.e. $\otimes$, $\oplus$, etc.), then it is bound in the ordered
context $\Omega$ instead of $\Gamma$. \textsc{Left Async} rules operate on assumptions in
$\Omega$, rather than $\Gamma$. This results in invertible elimination rules
being applied as fully as possible before \textit{focusing} on non-invertible
rules when $\Omega$ is empty.  

In addition to the focused forms of the original synthesis calculi, each
calculus has a set of rules which determine which part of the synthesis
judgement will be focused on: the \textsc{Focus} rules. These rules are given by
Figures~\ref{fig:focus-sub-focus}, and~\ref{fig:focus-add-focus} for the
subtractive and additive calculi, respectively.

We briefly describe each set of rules for the focusing phases of the subtractive
synthesis calculus.


\begin{figure}[H]
  \begin{align*}
    {\footnotesize{
\begin{array}{c}
  \fSubAbsRuleNoLabel
  \\[1.25em]
  \fSubRAsyncTransitionRule
  \end{array}
    }}
  \end{align*}
  \caption{$\textsc{Right Async}$ rules of the focused subtractive synthesis calculus}
  \label{fig:focus-sub-right-async}
\end{figure}
The $\textsc{Right Async}$ phase of focusing (Figure~\ref{fig:focus-sub-right-async})
contains the focused form of the $\multimap_\textsc{R}^-$ rule, which simply
binds the variable from the $\lambda$ term in $\Omega$ instead of $\Gamma$. The
$\Uparrow_\textsc{R}^-$ rule handles the transition from a $\textsc{Right Async}$ phase to
a $\textsc{Left Async}$ phase. It is applied when the goal type $C$ is no longer right
asynchronous (i.e. not a $\multimap$), thus we switch to breaking down the
assumptions in $\Omega$ by applying a sequence of $\textsc{Left Async}$ rules.

\begin{figure}[H]
  \begin{align*}
    {\footnotesize{
\hspace{-3.5em}\begin{array}{c}
  \fSubPairElimRuleNoLabel
  \\[1.25em]
  \fSubSumElimRule
  \\[1.25em]
  \fSubUnboxRule
  \\[1.25em]
  \fSubUnitElimRule
  \\[1.25em]
  % \fSubDerRule
  % \\[1.25em]
  \fSubLAsyncTransitionRule
  \end{array}
    }}
  \end{align*}
  \caption{$\textsc{Left Async}$ rules of the focused subtractive synthesis calculus}
  \label{fig:focus-sub-left-async}
\end{figure}

The $\textsc{Left Async}$ phase of focusing
(Figure~\ref{fig:focus-sub-left-async}) breaks down the assumptions in $\Omega$
via the application of left rules. The $\Uparrow_\textsc{L}^-$ rule moves an
assumption from $\Gamma$ to $\Omega$ if the assumption's type is not left
asynchronous. 

\begin{figure}[H]
  \begin{align*}
    \hspace{-3em}
    {\footnotesize{
\begin{array}{c}
  \fSubFocusRRuleNoLabel
  \;\;\;
  % \\[1.25em]
  \fSubFocusLRule
  \end{array}
    }}
  \end{align*}
  \caption{$\textsc{Focus}$ rules of the focused subtractive synthesis calculus}
  \label{fig:focus-sub-focus}
\end{figure}

The next set of rules are the $\textsc{Focus}$ rules
(Figure~\ref{fig:focus-sub-focus}) which are used to determine what synchronous
phase should be entered based on the type. In both rules, $\Omega$ is empty as a
consequence of repeatedly applying left rules in the $\textsc{Left Async}$ phase
until each assumption is broken into assumptions without left asynchronous
types. The $\textsc{Focus}_\textsc{R}^-$ rule requires that the goal type not be
atomic (i.e. a type variable), as the $\textsc{Right Sync}$ phase comprises
right rules for types. Similarly, the $\textsc{Focus}_\textsc{L}^-$ requires
that $\Gamma$ not be empty.  

\begin{figure}[H]
  \begin{align*}
    {\footnotesize{
\begin{array}{c}
\fSubPairIntroRuleNoLabel
  \\[1.25em]
  \fSubSumIntroRuleR
  \,
  \fSubSumIntroRuleL
  \\[1.25em]
  \fSubBoxRule
  \\[1.25em]
  \fSubUnitIntroRule
  \,
  \fSubRSyncTransitionRule
  \end{array}
    }}
  \end{align*}
  \caption{$\textsc{Right Sync}$ rules of the focused subtractive synthesis calculus}
  \label{fig:focus-sub-right-sync}
\end{figure}

The $\textsc{Right Sync}$ rules are given by
Figure~\ref{fig:focus-sub-right-sync}, comprising the right rules for $\otimes$,
$\oplus$, $\Box$, and $\mathsf{Unit}$. The rule $\Downarrow_\textsc{R}^-$
switches back to a $\textsc{Right Async}$ phase if the other rules cannot be
applied.

\begin{figure}[H]
  \begin{align*}
    {\footnotesize{
\begin{array}{c}
  \fSubAppRuleNoLabel
  \\[1.25em]
  \fSubDerRule
  \\[1.25em]
  \fSubLinVarRule
  \,
  \fSubGrVarRule
  \\[1.25em]
  \fSubLSyncTransitionRule
  \end{array}
    }}
  \end{align*}
  \caption{$\textsc{Left Sync}$ and $\textsc{Var}$ rules of the focused subtractive synthesis calculus}
  \label{fig:focus-sub-left-sync}
\end{figure}

Finally the $\textsc{Left Sync}$ set (Figure~\ref{fig:focus-sub-left-sync})
contains the focused $\multimap_\textsc{L}^-$ rule while the $\textsc{Var}$ set
contains the focused forms of $\textsc{LinVar}^-$ and $\textsc{GrVar}^-$. Both
sets allow for transition to $\textsc{Left Async}$ via the
$\Downarrow_\textsc{L}^-$ rule.

This focused form of the additive synthesis calculi follow an identical scheme,
given by Figures \ref{fig:focus-add-right-async}-\ref{fig:focus-add-left-sync}. 


\begin{figure}[H]
  \begin{align*}
    {\footnotesize{
\hspace{-2em}\begin{array}{c}
  \fAddAbsRuleNoLabel
  \;\;\;
  \fAddRAsyncTransitionRule
  \end{array}
    }}
  \end{align*}
  \caption{$\textsc{Right Async}$ rules of the focused additive synthesis calculus}
  \label{fig:focus-add-right-async}
\end{figure}

\begin{figure}[H]
  \begin{align*}
    {\footnotesize{
\hspace{-3em}\begin{array}{c}
  \fAddPairElimRuleNoLabel
  \\[1.25em]
  \fAddSumElimRule
  \\[1.25em]
  \fAddUnboxRule
  \\[1.25em]
  % \fAddDerRule
  % \\[1.25em]
  \fAddUnitElimRule
  \\[1.25em]
  \fAddLAsyncTransitionRule
  \end{array}
    }}
  \end{align*}
  \caption{$\textsc{Left Async}$ rules of the focused additive synthesis calculus}
  \label{fig:focus-add-left-async}
\end{figure}

\begin{figure}[H]
  \begin{align*}
    {\footnotesize{
\begin{array}{c}
  \fAddFocusRRuleNoLabel
  \\[1.25em]
  \fAddFocusLRule
  \end{array}
    }}
  \end{align*}
  \caption{$\textsc{Focus}$ rules of the focused additive synthesis calculus}
  \label{fig:focus-add-focus}
\end{figure}

\begin{figure}[H]
  \begin{align*}
    {\footnotesize{
\begin{array}{c}
  \fAddPairIntroRuleNoLabel
  \\[1.25em]
  \fAddSumIntroRuleL
  \,
  \fAddSumIntroRuleR
  \\[1.25em]
  \fAddBoxRule
  \,
  \fAddUnitIntroRule
  \\[1.25em]
  \fAddRSyncTransitionRule
  \end{array}
    }}
  \end{align*}
  \caption{$\textsc{Right Sync}$ rules of the focused additive synthesis calculus}
  \label{fig:focus-add-right-sync}
\end{figure}

\begin{figure}[H]
  \begin{align*}
    {\footnotesize{
\hspace{-2em}\begin{array}{c}
  \fAddAppRule
  \\[1.25em]
  \fAddDerRule
  \\[1.25em]
  \fAddLinVarRule
  \;\;\;
  \fAddGrVarRule
  \\[1.25em]
  \fAddLSyncTransitionRule
  \end{array}
    }}
  \end{align*}
  \caption{$\textsc{Left Sync}$ and $\textsc{Var}$ rules of the focused additive
  synthesis calculus}
  \label{fig:focus-add-left-sync}
\end{figure}

\begin{figure}[H]
  \begin{align*}
    {\footnotesize{
\begin{array}{c}
\fAddAltAppRule
  \\[1.25em]
\fAddAltPairIntroRule
  \end{array}
    }}
  \end{align*}
  \caption{Rules of the focused additive pruning synthesis calculus}
  \label{fig:focus-add-pruning}
\end{figure}

One way to view focusing is in terms of a finite state machine given in
Figure~\ref{fig:focusingFSM}. States comprise the four phases of focusing, plus
two additional states, \textsc{Focus}, and \textsc{Var}. Edges are then the
synthesis rules that direct the transition between focusing phases. The
transitions between these focusing phases are handled by dedicated focusing
rules for each transition. For the asynchronous phases, the $\Uparrow_{R}$
handles the transition between $\textsc{Right Async}$ to $\textsc{Left Async}$
phases, while the $\Uparrow_{L}$ handles the transition from $\textsc{Left
Async}$ to $\textsc{Focus}$ phases. Conversely, the $\Downarrow{R}$ rule deals
with the transition from a $\textsc{Right Sync}$ phase back to a $\textsc{Right
Async}$ phase, with the $\Downarrow{L}$ rule likewise transitioning to a
$\textsc{Left Async}$ phase. Depending on the current phase of focusing, these
rules consider the goal type, the type of the assumption currently being focused
on, as well as the size of $\Omega$, to decide when to transition between
$\textsc{Focus}$ phases.  

\tikzset{ ->,
% makes the edges directed
node distance=5cm, % specifies the minimum distance between two nodes. Change if necessary.
every state/.style={thick, fill=gray!10}, % sets the properties for each state node
initial text=$ $, % sets the text that appears on the start arrow
}

\begin{figure}[H] % ht tells LaTeX to place the figure here or at the top of the page
\centering 

\scalebox{0.80}{
\begin{tikzpicture}[every text node part/.style={align=center}]
\node[state, initial, draw] (RA) {\textsc{Right Async} \\ $ \Gamma ; \Omega \vdash A \Uparrow\ \Rightarrow t\ |\ \Delta $};
\node[state] at (10,  0) (LA) { \textsc{Left Async} \\ $\Gamma ; \Omega \Uparrow\ \vdash A \Rightarrow t\ |\ \Delta $};
\node[state] at (5, -5) (F) { \textsc{Focus} \\ $\Gamma ; \emptyset \Uparrow\ \vdash A \Rightarrow t\ |\ \Delta $};
\node[state] at (0, -10) (RS) { \textsc{Right Sync} \\ $\Gamma ; \emptyset \vdash A\ \Downarrow\ \Rightarrow t\ |\ \Delta $};
\node[state] at (10, -10) (LS) { \textsc{Left Sync} \\ $\Gamma ; [[ x : B ]] \Downarrow\ \vdash A \Rightarrow t\ |\ \Delta $};
\node[state, accepting] at (5, -12) (V) { \textsc{Var} \\ $\Gamma ; [[ x : A ]] \Downarrow\ \vdash A \Rightarrow t\ |\ \Delta $};
\draw (RA) edge[loop above] node{$\multimap_{\textsc{R}}$} (RA)
(RA) edge[above, bend left] node{$\Uparrow_{\textsc{R}}$} (LA)
(LA) edge[loop above] node{$\otimes_{\textsc{L}}$, $\oplus_{\textsc{L}}$, \\ $\mathsf{Unit}_\textsc{L}$, \\ $\Box_{\textsc{L}}$, $\Uparrow_{\textsc{L}}$ } (LA)
% (LA) edge[loop right] node{$\Uparrow_{L}$} (LA)
(LA) edge[left, bend right] node{$\otimes_{\textsc{L}}$, $\oplus_{\textsc{L}}$, \ \ \ \\ $\mathsf{Unit}_\textsc{L}$, \ \ \ \ \\ $\Box_{\textsc{L}}$, $\Uparrow_\textsc{L}$ \ \ \ \ \\  } (F)
% (LA) edge[right, bend left] node{$\Uparrow_{L}$} (F)
(F) edge[below, bend right] node{\ \ \ \  $\textsc{F}_{\textsc{R}}$} (RS)
(RS) edge[left, bend left] node{$\Downarrow_{\textsc{L}}$} (RA)
(F) edge[below, bend left] node{$\textsc{F}_{\textsc{L}}$\ \ } (LS)
(LS) edge[right, bend right] node{$\Downarrow_{\textsc{L}}$} (LA)
(LS) edge[below, bend right] node{$\multimap_{\textsc{L}}$} (RS)
(LS) edge[loop below] node{$\multimap_{\textsc{L}}, \textsc{Der}$} (LS)
(RS) edge[loop below] node{$\otimes_{\textsc{R}}$, $\oplus1_{\textsc{R}}$, $\oplus2_{\textsc{R}}$, \\ $\mathsf{Unit}_\textsc{R}$, $\Box_{\textsc{R}}$} (RS)
(LS) edge[below, bend left] node{\\[0.1em] \ \ \ $\textsc{LinVar}$, \\ \ \ \ $\textsc{GrVar}$ } (V)
;
\end{tikzpicture}
}
\caption{Focusing State Machine}
\label{fig:focusingFSM}
\end{figure}

We conclude with three key results: that applying focusing is sound for the
subtractive (Lemma~\ref{lemma:fSubSynthSound}) and additive
(Lemma~\ref{lemma:fAddSynthSound}), and additive pruning
(Lemma~\ref{lemma:fAddPruningSynthSound}) synthesis calculi. The proofs are
contained in Sections~\ref{proof:focusSubSound},~\ref{proof:focusAddSound},
and~\ref{proof:focusAddPruningSound} of Appendix~\ref{appendix:proofs},
respectively.

\begin{restatable}[Soundness of focusing for subtractive synthesis]{lemma}{focusSoundSub}
  \label{lemma:fSubSynthSound}
For all contexts $[[ G ]]$, $[[ O ]]$ and types $[[ A ]]$, $[[ B]]$
then:
\begin{align*}
  {\footnotesize{
\begin{array}{lll}
 1.\ \textsc{Right Async:} & [[ G ; O |- A async =>- t ; D ]] \quad &\iff \quad [[ G , O |- A =>- t ; D ]]\\
 2.\ \textsc{Left Async:} & [[ G ; O async |- B =>- t ; D ]] \quad &\iff \quad [[ G , O |- B =>- t ; D ]]\\
 3.\ \textsc{Right Sync:} & [[ G ; . |- A sync =>- t ; D ]] \quad &\iff \quad [[ G |- A =>- t ; D ]]\\
 4.\ \textsc{Left Sync:} & [[ G ; {x : A }sync |- B =>- t ; D ]] \quad &\iff \quad [[ G, x : A |- B =>- t ; D ]]\\
 5.\ \textsc{Focus Right:} & [[ G ; . async |- B =>- t ; D ]] \quad &\iff \quad [[ G |- B =>- t ; D ]]\\
 6.\ \textsc{Focus Left:} & [[ G, x : A ; . async |- C =>- t ; D ]] \quad &\iff \quad [[ G, x : A |- B =>- t ; D ]]
\end{array}
  }}
\end{align*}
i.e. $[[ t ]]$ has type $[[ A ]]$
under context $[[ D ]]$,
which contains assumptions with grades reflecting their use in $[[ t ]]$.
\end{restatable}

\begin{restatable}[Soundness of focusing for additive synthesis]{lemma}{focusSoundAdd}
  \label{lemma:fAddSynthSound}
For all contexts $[[ G ]]$, $[[ O ]]$ and types $[[ A ]]$, $[[ B ]]$
then:
\begin{align*}
  {\footnotesize{
\begin{array}{lll}
 1.\ \textsc{Right Async:} & \Gamma ; \Omega \vdash A \Uparrow\ \Rightarrow^{+} t \mid \Delta \quad &\iff \quad \Gamma, \Omega \vdash A \Rightarrow^{+} t \mid \Delta \\
 2.\ \textsc{Left Async:} & \Gamma ; \Omega \Uparrow\ \vdash A \Rightarrow^{+} t \mid \Delta \quad &\iff \quad \Gamma, \Omega \vdash B \Rightarrow^{+} t \mid \Delta \\
 3.\ \textsc{Right Sync:} & \Gamma ; \emptyset \vdash A \Downarrow\ \Rightarrow^{+} t \mid \Delta  \quad &\iff \quad \Gamma \vdash A \Rightarrow^{+} t \mid \Delta\\
 4.\ \textsc{Left Sync:} & \Gamma ; x : A \Downarrow\ \vdash  B \Rightarrow^{+} t \mid \Delta \quad &\iff \quad \Gamma, x : A \vdash B \Rightarrow^{+} t \mid \Delta \\
 5.\ \textsc{Focus Right:} & \Gamma ; \emptyset \Uparrow\ \vdash A \Rightarrow^{+} t \mid \Delta \quad &\iff \quad \Gamma \vdash B \Rightarrow^{+} t \mid \Delta \\
 6.\ \textsc{Focus Left:} & \Gamma, x : A ; \emptyset \Uparrow\ \vdash B \Uparrow\ \Rightarrow^{+} t \mid \Delta \quad &\iff \quad \Gamma, x : A \mid B \Rightarrow^{+} t \mid \Delta 
\end{array}
  }}
\end{align*}
i.e. $[[ t ]]$ has type $[[ A ]]$
under context $[[ D ]]$,
which contains assumptions with grades reflecting their use in $[[ t ]]$.
\end{restatable}

\begin{restatable}[Soundness of focusing for additive pruning synthesis]{lemma}{focusSoundAddPruning}
\label{lemma:fAddPruningSynthSound}
For all contexts $[[ G ]]$, $[[ O ]]$ and types $[[ A ]]$, $[[ B ]]$
then:
\begin{align*}
  {\footnotesize{
\begin{array}{lll}
 1.\ \textsc{Right Async:} & \Gamma ; \Omega \vdash A \Uparrow\ \Rightarrow^{\pm} t \mid \Delta \quad &\iff \quad \Gamma, \Omega \vdash A \Rightarrow^{\pm} t \mid \Delta \\
 2.\ \textsc{Left Async:} & \Gamma ; \Omega \Uparrow\ \vdash A \Rightarrow^{\pm} t \mid \Delta \quad &\iff \quad \Gamma, \Omega \vdash B \Rightarrow^{\pm} t \mid \Delta \\
 3.\ \textsc{Right Sync:} & \Gamma ; \emptyset \vdash A \Downarrow\ \Rightarrow^{\pm} t \mid \Delta  \quad &\iff \quad \Gamma \vdash A \Rightarrow^{\pm} t \mid \Delta\\
 4.\ \textsc{Left Sync:} & \Gamma ; x : A \Downarrow\ \vdash  B \Rightarrow^{\pm} t \mid \Delta \quad &\iff \quad \Gamma, x : A \vdash B \Rightarrow^{\pm} t \mid \Delta \\
 5.\ \textsc{Focus Right:} & \Gamma ; \emptyset \Uparrow\ \vdash A \Rightarrow^{\pm} t \mid \Delta \quad &\iff \quad \Gamma \vdash B \Rightarrow^{\pm} t \mid \Delta \\
 6.\ \textsc{Focus Left:} & \Gamma, x : A ; \emptyset \Uparrow\ \vdash B \Uparrow\ \Rightarrow^{\pm} t \mid \Delta \quad &\iff \quad \Gamma, x : A \mid B \Rightarrow^{\pm} t \mid \Delta 
\end{array}
  }}
\end{align*}
i.e. $[[ t ]]$ has type $[[ A ]]$
under context $[[ D ]]$,
which contains assumptions with grades reflecting their use in $[[ t ]]$.
\end{restatable}


\section{Implementation}
\label{sec:linear-base-implementation}
We implemented our approach as a synthesis tool for Granule, integrated with its
core tool. Granule features ML-style polymorphism (rank-1 polymorphism) but we
do not address polymorphism here (Chapter~\ref{chapter:extended} offers a treatment of 
synthesis of polymorphic programs). Instead, programs are synthesised from type
schemes treating universal type variables as logical atoms. Multiplicative
products are primitive in Granule, although additives coproducts are provided
via ADTs, from which we define a core sum type to use defined: 
\begin{granule}
  data Either a b = Left a | Right b
\end{granule}
Constraints on resource usage are handled via Granule's existing
symbolic engine, which compiles constraints on grades (for various semirings)
to the SMT-lib format for Z3~\citep{z3}.
In the case of graded variable synthesis in the subtractive
scheme, the kind of the assumption's grade (i.e., what semiring it
belongs to) is inferred using Granule's type
checker, which is used to generate an existential variable representing
the remaining available usage of the graded assumption.

We use the LogicT monad for backtracking \emph{enumerative} search of the 
space of candidate programs~\citep{logict} and the Scrap
Your Reprinter library for splicing synthesised code into syntactic ``holes''
(represented by \granin{?} in Granule), preserving the rest of the program
text~\citep{clarke2017scrap}.

\subsection{Post-Synthesis Refactoring}
\label{sec:linear-base-refactoring}
A synthesised term often contains some artefacts of the fact that it was
constructed automatically. The structure of our synthesis rules means aspects of
our synthesised programs are not representative in some stylistic ways of the
kind of programs functional programmers typically write. We consider two
examples of these below using Granule code, and show how we apply a refactoring
procedure to any synthesised term to rewrite them in a more idiomatic style. 

\subsubsection{Abstractions}
A function definition synthesised from a function type using the
$\multimap_{\textsc{R}}$ will take the form of a sequence of nested abstractions which
bind the function's arguments, with the sub-term of the innermost abstraction
containing the function body, e.g.

\begin{granule}
pair : forall { a b : Type } . a -> b -> (a, b)
pair = \x -> \y -> (x, y)
\end{granule}

In most cases, a programmer would write a function definition as a series of
equations with the function arguments given as patterns. Our refactoring
procedure collects the outermost abstractions of a synthesised term and
transforms them into equation-level patterns with the innermost abstraction
body forming the equation body:

\begin{granule}
pair : forall { a b : Type } . a -> b -> (a, b)
pair x y = (x, y)
\end{granule}

\subsubsection{Unboxing}
An unboxing term is synthesised via the \subUnboxName\ abd \addUnboxName\ rules
rule as a let expression which pattern matches over a box pattern, yielding an
assumption with the grade's usage. Such terms can also be refactored both into
function equations and to avoid nested let bindings. For example, we may write
the $k$ combinator using an explicit graded modality:

\begin{granule}
k : forall { a b : Type } . a -> b [0] -> a
k x y = let [z] = y in x
\end{granule}
which we can then refactor into
\begin{granule}
k : forall { a b : Type } . a -> b [0] -> a
k x [z] = z
\end{granule}
This procedure includes programs which perform a nested unboxing, refactoring:
\begin{granule}
comp : forall {k : Semiring, n m : k, a b c : Type} 
     . (a [m] -> b) [n] 
     -> (b [n] -> c) 
     -> a [n * m] 
     -> c
comp x y z = let [u] = x in let [v] = z in y [ u [v] ]
\end{granule}
into:
\begin{granule}
comp : forall {k : Semiring, n m : k, a b c : Type}
     . (a [m] -> b) [n] 
     -> (b [n] -> c) 
     -> a [n * m] 
     -> c
comp [u] y [v] = y [ u [v] ]
\end{granule}



\section{Evaluating the Synthesis Calculi}
\label{sec:linear-base-evaluation}

\newcommand{\stderr}[1]{\textcolor{gray}{${#1}$}} % \pm{#1}
\newcommand{\fail}{\textcolor{mypink3}{$\times$}}
\newcommand{\success}{\checkmark}
\newcommand{\highlight}[1]{%
{\setlength{\fboxsep}{0pt}\colorbox{yellow!50}{$\displaystyle#1$}}}

Prior to evaluation, we made the following hypotheses about the
relative performance of the additive versus subtractive approaches:
%
\begin{enumerate}
\item[H1.] (\textbf{Solving; Additive requires less}) Additive synthesis should make fewer calls to the solver, with lower
complexity theorems (fewer quantifiers). Dually,
subtractive synthesis makes more calls to the solver with
higher complexity theorems.

\item[H2.] (\textbf{Paths; Subtractive explores fewer}) For complex problems, additive will
explore more paths as it cannot tell whether a variable is not
well-resourced until closing a binder; additive pruning and subtractive will
explore fewer paths as they can fail sooner.

\item[H3.] (\textbf{Performance; additive faster on simpler examples}) A
corollary of the above two: simple examples will likely be faster in additive
mode, but more complex examples will be faster in subtractive.
\end{enumerate}

\subsection{Methodology}

To evaluate our synthesis tool we developed a suite of benchmarks comprising
Granule type schemes for a variety of operations using linear and graded modal
types. We divide our benchmarks into several classes of problem:
%
\begin{itemize}[itemsep=0em,leftmargin=1.1em]
\item \textbf{Hilbert}: the Hilbert-style axioms of
  intuitionistic logic (including SKI combinators), with appropriate $\mathbb{N}$ and $\mathbb{N}$-intervals
  grades where needed (see, e.g., $S$ combinator in
  Example~\ref{ex:s-comb} or coproduct elimination in Example~\ref{exm:or3}).

\item \textbf{Comp}: various translations of function composition into linear
logic: multiplicative, call-by-value and call-by-name using
$!$~\citep{girard1987linear}, 0/1 translation of intuitionistic logic using
$!$~\citep{liang2009focusing}, and coKleisli composition over $\mathbb{N}$ and
arbitrary semirings: e.g. $\forall r, s \in \mathcal{R}$:
%
\begin{equation*}
\textit{comp-}\textit{coK}_{\mathcal{R}} : [[ {[] r ({[] s A} -o B)} -o {({[] r B} -o C) -o {{[] {r * s} A} -o C}} ]]
\end{equation*}
%
\item \textbf{Dist}: distributive laws of various graded
modalities over functions, sums, and products,
e.g., $\forall r \in \mathbb{N}$, or
$\forall r \in \mathcal{R}$ in any semiring, or $r = [[ Intrv 0 Inf ]]$:
%
\begin{equation*}
\textit{pull}_\oplus : [[ (Sum {[] r A} {[] r B}) -o [] r (Sum A B) ]]
\end{equation*}
\begin{equation*}
\textit{push}_\multimap : [[ {[] r (A -o B)} -o {{[] r A} -o [] r B} ]]
\end{equation*}
%
%% data type and vice vera as our second class of programs. In the latter case, we have programs
%which \textit{pull} natural number, !, and general modalities out of sum,
%product, and vector data types. In the former case, we have programs
%which distribute (\textit{push}) natural number, !, and general graded modalities over a
%function type. The ability to synthesise \textit{push} over other data types is
%further work (see section \ref{sec:futurework}).

\item \textbf{Vec}: map operations on
fixed size vectors encoded as products, e.g.:
\begin{align*}
\begin{array}{rl}
\textit{vmap}_5 :& [[ {[] 5 (A -o B)} ]] \\ \multimap & [[(Tup (Tup (Tup (Tup A A) A) A) A) ]] \\ \multimap & [[ (Tup (Tup (Tup (Tup B B) B) B) B) ]]
\end{array}
\end{align*}
%

\item \textbf{Misc}: includes Example~\ref{exm:security} (information-flow
  security) and functions which must share resources between graded
  modalities, e.g.:
%one problem requires two graded values (the first two parameters)
%to be shared between two applications of a function given as the third input:
%
\begin{align*}
\begin{array}{rl}
\textit{share} :& [[ {[] 4 A}]] \\ \multimap & [[ [] 6 A ]] \\ \multimap & [[ [] 2 ((Tup (Tup (Tup (Tup A A) A) A) A) -o B)]] \\ \multimap & [[ (Tup B B) ]]
\end{array}
\end{align*}
%
\end{itemize}

\begin{table}
\scalebox{0.85}{\parbox{.5\linewidth}{%
\begin{align*}
\hspace{-2em}
\setlength{\arraycolsep}{0.2em}
\begin{array}{r|rll}
\textbf{Hilbert} & \\ \hline
\text{$\otimes{}$Intro} & \otimes_i : \forall a , b . & a \multimap b \multimap (a \otimes b) \\
\text{$\otimes{}$Elim} & \otimes_{e1} : \forall a , b . & (a \otimes \Box_0 b) \multimap a \\
                       & \otimes_{e2} : \forall a , b . & (\Box_0 a \otimes b) \multimap b \\
\text{$\oplus{}$Intro} & \oplus_{i1} : \forall a , b . & a \multimap a \oplus b \\
                       & \oplus_{i2} : \forall a , b . & b \multimap a \oplus b \\
\text{$\oplus{}$Elim}  & \oplus_{e} : \forall a , b, c . & \Box_{0...1}(a \multimap c) \multimap \Box_{0...1}(b \multimap c) \multimap (a \oplus b) \multimap c \\
\text{SKI} & s : \forall a, b, c . & (a \multimap (b \multimap c)) \multimap (a \multimap b) \multimap (\Box_2 a \multimap c) \\
& k : \forall a, b . & a \multimap \Box_0 b \multimap a \\
& i : \forall a . & a \multimap a \\ \hline
\textbf{Comp} & \\ \hline
\text{0/1} & \circ_{I/O} :\forall a, b, c . & \Box (\Box a \multimap \Box b) \multimap \Box (\Box b \multimap \Box c) \multimap \Box (\Box a \multimap c) \\
\text{CBN} & \circ_{\textsc{cbn}} : \forall a, b, c . & \Box (\Box a \multimap b) \multimap \Box (\Box b \multimap c) \multimap \Box a \multimap c \\
\text{CBV} & \circ_{\textsc{cbv}} : \forall a, b, c . & \Box (\Box a \multimap \Box b) \multimap \Box (\Box b \multimap \Box c) \multimap \Box \Box a \multimap \Box c\\
\text{coK-$\mathcal{R}$} & \circ_{\mathcal{R}} :
\forall \mathcal{R}, r, s \in \mathcal{R}, a, b, c . &
      \Box_r (\Box_s a \multimap b) \multimap (\Box_r b \multimap c) \multimap \Box_{r \cdot s} a \multimap c
\\
\text{lin} & \circ : \forall a, b, c . & (a \multimap b) \multimap (b \multimap c) \multimap (a \multimap c) \\
\text{coK-$\mathbb{N}$} & \circ_{\mathbb{N}} :
\forall r, s \in \mathbb{N}, a, b, c . &
      \Box_r (\Box_s a \multimap b) \multimap (\Box_r b \multimap c) \multimap \Box_{r \cdot s} a \multimap c
\\ \hline
\textbf{Dist} & \\ \hline
\text{$\oplus$-$\mathbb{N}$} & \textit{pull}_\oplus : \forall r :
                               \mathbb{N}, a, b . & (\Box_r a \oplus \Box_r b) \multimap \Box_r (a \oplus b) \\
\text{$\oplus$-!} &  \textit{pull}_\oplus : \forall a, b . & (\Box a \oplus \Box b) \multimap \Box (a \oplus b) \\
\text{$\oplus$-$\mathcal{R}$} & \textit{pull}_\oplus : \forall
                                \mathcal{R}, r \in \mathcal{R}, a, b
                                . &  (\Box_r a \oplus \Box_r b) \multimap \Box_r (a \oplus b) \\
\text{$\otimes$-$\mathbb{N}$} & \textit{pull}_\otimes : \forall r :
                                \mathbb{N}, a, b . & (\Box_r a \otimes \Box_r b) \multimap \Box_r (a \otimes b) \\
\text{$\otimes$-!} & \textit{pull}_\otimes : \forall a, b . & (\Box a \otimes \Box b) \multimap \Box (a \otimes b) \\
\text{$\otimes$-$\mathbb{R}$} & \textit{pull}_\otimes : \forall
                                  \mathcal{R}, r,
                                  a, b . & (\Box_r a \otimes \Box_r b) \multimap \Box_r (a \otimes b)  \\
\text{$\multimap$-$\mathbb{N}$} & \textit{push}_\multimap : \forall r
                                  : \mathbb{N}, a, b . & \Box_r (a \multimap b) \multimap \Box_r a \multimap \Box_r b \\
\text{$\multimap$-!} & \textit{push}_\multimap : \forall a, b . & \Box (a \multimap b) \multimap \Box a \multimap \Box b \\
\text{$\multimap$-$\mathcal{R}$} & \textit{push}_\multimap : \forall
                                   \mathcal{R}, r : \mathcal{R}, a, b
                                   .& \Box_r (a \multimap b) \multimap \Box_r a \multimap \Box_r b \\ \hline
\textbf{Vec} \\ \hline
\text{vec5} & \textit{vmap}_5 : \forall a, b . & \Box_5 (a \multimap b) \multimap ((((a \otimes a) \otimes a) \otimes a) \otimes a) \\
& & \multimap ((((b \otimes b) \otimes b) \otimes b) \otimes b) \\
\text{vec10} & \textit{vmap}_{10} : \forall a, b . & \textit{as above but for 10-tuples} \\
\text{vec15} & \textit{vmap}_{15} : \forall a, b . & \textit{as above but for 15-tuples} \\
\text{vec20} & \textit{vmap}_{20} : \forall a, b . & \textit{as above but for 20-tuples}\\ \hline
\textbf{Misc} \\ \hline
\text{split$\oplus$} & \multicolumn{2}{l}{ \textit{split} : \forall a, b, c .
\Box_{2...3} b \multimap (a \oplus c) \multimap (( a \otimes \Box_{2..2} b) \oplus (c \otimes \Box_{3...3} b))}
 \\
\text{split$\otimes$} & \multicolumn{2}{l}{\textit{split} : \forall a, b . 
\Box_{0...2} (a \multimap a \multimap a) \multimap \Box_{10...10} a \multimap (\Box_{2...2} a \otimes \Box_{6...6} a) 
}
\\
share & \multicolumn{2}{l}{\textit{share} : \forall a, b .  \Box_4 a \multimap \Box_6 a \multimap \Box_2 ((((( a \otimes a) \otimes a) \otimes a) \otimes a) \multimap b) \multimap (b \otimes b)}
\\
Exm.~\ref{exm:security}
& \multicolumn{2}{l}{\textit{noLeak} : \forall a, b . (\Box_{[[ Public ]]} a \otimes \Box_{[[ Private]]} a) \multimap (\Box_{[[ Public ]]}(a \otimes 1) \multimap b) \multimap b }
\end{array}
\end{align*}
}}
\caption{List of benchmark synthesis problems}
\label{fig:list-of-types}
\end{table}
% }
%
Table~\ref{fig:list-of-types} provides the complete list of Granule type schemes
used for these synthesis problems (32 in total). Note that these are type
schemes which quantify over type variables ($a$, and $b$), however, we simply
treat each type variable as a logical atom, unifiable only with itself.
Chapter~\ref{chapter:extended} provides a proper treatment of synthesis from
Rank-1 polymorphic type schemes. Note also that $\Box A$ is used as shorthand
for $\square_{0...\infty} A$ (graded modality with indices drawn from intervals
over $\mathbb{N} \cup \{\infty$\}). The compete synthesised program code for the
benchmarking problems can be found in Section~\ref{sec:linear-benchmarks} of
Appendix~\ref{appendix:benchmarks}


We found that Z3 is highly variable in its solving time, so timing
measurements are computed as the mean of 20 trials. We used
Z3 version 4.8.8 on a Linux laptop with an Intel i7-8665u @ 4.8 Ghz
and 16 Gb of RAM.

\begin{table}[H]
{\small{
\begin{center}
\setlength{\tabcolsep}{0.3em}
\scalebox{0.9}{
% this is wide enough to show 4 modes worth of data
\begin{tabular}{p{2.5em}r|p{0.75em}rr|p{0.5em}rr|p{0.5em}rr}
 & & \multicolumn{3}{c|}{Additive}&\multicolumn{3}{c|}{Additive (pruning)}&\multicolumn{3}{c|}{Subtractive}\\ \hline
\multicolumn{2}{c|}{{Problem}} &  & \multicolumn{1}{c}{$\mu{T}$ (ms)} & \multicolumn{1}{r|}{\textsc{N}} & & \multicolumn{1}{c}{$\mu{T}$ (ms)} & \multicolumn{1}{r|}{\textsc{N}} & & \multicolumn{1}{c}{$\mu{T}$ (ms)} & \multicolumn{1}{r|}{\textsc{N}} \\ \hline\hline
\multirow{5}{*}{{\rotatebox{90}{\textbf{Hilbert}}}}
& $\otimes{}$Intro & \success{} &   {\highlight{$6.69 (\stderr{  0.05})$}} &   2  &    \success{}
                                             &   9.66 (\stderr{  0.23}) &   2
                                                                     &     \success{}   &  10.93 (\stderr{  0.31}) &   2 \\
& $\otimes{}$Elim                     & \success{} &   0.22 (\stderr{  0.01}) &   0       & \success{} &   {\highlight{$0.05 (\stderr{  0.00})$}} &   0       & \success{} &   0.06 (\stderr{  0.00}) &   0      \\
& $\oplus{}$Intro & \success{} &   0.08 (\stderr{  0.00}) &   0    &  \success{}
                                          &   {\highlight{$0.07 (\stderr{  0.00})$}} &   0
                                                                   &    \success{}
                                                                                 &
                                                                                   {\highlight{$0.07
                                                                                   (\stderr{
                                                                                   0.00})$}}
                                                                                                                   &   0 \\
& $\oplus{}$Elim                       & \success{} &   {\highlight{$7.26 (\stderr{  0.30})$}} &   2       & \success{} &  13.25 (\stderr{  0.58}) &   2       & \success{} & 204.50 (\stderr{  8.78}) &  15      \\ %SmtT/T = 93.85304985917998%, 91.73667044352199%, 99.1533306157868%
& SKI & \success{} &   {\highlight{$8.12 (\stderr{  0.25})$}} &   2  &    \success{} &  24.98
                                                                      (\stderr{
                                                                      1.19}) &
                                                                               2
                                                                             &  \success{}
                                                                                 &
                                                                                   41.92
                                                                                   (\stderr{
                                                                                   2.34})
                                                                                                                   &
                                                                                                                     4
                 \\
\hline
\multirow{6}{*}{{\rotatebox{90}{\textbf{Comp}}}}
& 01                        & \success{} &  {\highlight{$28.31 (\stderr{  3.09})$}} &   5       & \success{} &  41.86 (\stderr{  0.38}) &   5  & \fail{} & Timeout & -    \\
& cbn                       & \success{} &  {\highlight{$13.12 (\stderr{  0.84})$}} &   3       & \success{} &  26.24 (\stderr{  0.27}) &   3  & \fail{} & Timeout & -    \\
& cbv                       & \success{} &  {\highlight{$19.68 (\stderr{  0.98})$}} &   5       & \success{} &  34.15 (\stderr{0.98}) &   5   & \fail{} & Timeout & -   \\
& $\circ\textit{coK}_\mathcal{R}$                 & \success{} &  33.37 (\stderr{  2.01}) &   2       & \success{} &  {\highlight{$27.37$}} (\stderr{  0.78}) &   2       & \fail{}  &  92.71 (\stderr{  2.37}) &   8      \\  % SmtT/T = 98.56608024136683%, 98.56541178986083%, 98.70614421865372%
& $\circ\textit{coK}_\mathbb{N}$                 & \success{} &  27.59 (\stderr{  0.67}) &   2       & \success{} &  {\highlight{$21.62$}} (\stderr{  0.59}) &   2       & \fail{}  &  95.94 (\stderr{  2.21}) &   8      \\ % SmtT/T = 98.36019681512148%, 98.35210841341276%, 98.63478123527435%
& mult                      & \success{} &   0.29 (\stderr{  0.02}) &   0       & \success{} &   0.12 (\stderr{  0.00}) &   0       & \success{} &   {\highlight{$0.11 (\stderr{  0.00})$}} &   0      \\     % SmtT/T = 0.0%, 0.0%, 0.0%
\hline
\multirow{9}{*}{{\rotatebox{90}{\textbf{Dist}}}}
& $\otimes$-!                 & \success{} &  {\highlight{$12.96 (\stderr{  0.48})$}} &   2       & \success{} &  32.28 (\stderr{  1.32}) &   2       & \success{} & 10487.92 (\stderr{  4.38}) &   7      \\ % SmtT/T = 96.84305139487837%, 99.16605247629178%, 99.98233353925326%
& $\otimes$-$\mathbb{N}$                  & \success{} &  {\highlight{$24.83 (\stderr{  1.01})$}} &   2       & \fail{}  &  32.18 (\stderr{  0.80}) &   2       & \fail{}  &  31.33 (\stderr{  0.65}) &   2      \\  % SmtT/T = 99.16249211706345%, 98.19366472714205%, 97.68400765522344
& $\otimes$-$\mathcal{R}$                  & \success{} &  {\highlight{$28.17 (\stderr{  1.01})$}} &   2       & \fail{}  &  29.72 (\stderr{  0.90}) &   2       & \fail{}  &  31.91 (\stderr{  1.02}) &   2      \\ % SmtT/T = 99.26176085615197%, 97.2013820814111%, 97.92618319348946%
& $\oplus$-!                & \success{} &   {\highlight{$7.87 (\stderr{  0.23})$}} &   2       & \success{} &  16.54 (\stderr{  0.43}) &   2       & \success{} & 160.65 (\stderr{  2.26}) &   4      \\ % SmtT/T = 96.52232766433309%, 96.54651122326587%, 99.69538508877449
& $\oplus$-$\mathbb{N}$                & \success{} &  {\highlight{$22.13 (\stderr{  0.70})$}} &   2       & \success{} &  30.30 (\stderr{  1.02}) &   2       & \fail{}  &  23.82 (\stderr{  1.13}) &   1      \\  % SmtT/T = 99.00528362933007%, 99.08548651040508%, 98.49944141606836%
& $\oplus$-$\mathcal{R}$                & \success{} &  {\highlight{$22.18 (\stderr{  0.60})$}} &   2       & \success{} &  31.24 (\stderr{  1.40}) &   2       & \fail{}  &  16.34 (\stderr{  0.40}) &   1      \\ % SmtT/T = 99.08179689945528%, 99.16435051551996%, 98.4221935356417%
& $\multimap$-!                & \success{} &   {\highlight{$6.53 (\stderr{  0.16})$}} &   2       & \success{} &  10.01 (\stderr{  0.25}) &   2       & \success{} & 342.52 (\stderr{  2.64}) &   4      \\% SmtT/T = 96.72718832940333%, 96.60077128502638%, 99.698687842732%
& $\multimap$-$\mathbb{N}$                  & \success{} &  29.16 (\stderr{  0.82}) &   2       & \success{} &  {\highlight{$28.71 (\stderr{  0.67})$}} &   2       & \fail{}  &  54.00 (\stderr{  1.53}) &   4      \\% SmtT/T = 99.14821593847735%, 99.13267964321408%, 99.00055370249412%
& $\multimap$-$\mathcal{R}$                  & \success{} &  29.31 (\stderr{  1.84}) &   2       & \success{} &  {\highlight{$27.44 (\stderr{  0.60})$}} &   2       & \fail{}  &  61.33 (\stderr{  2.28}) &   4      \\% SmtT/T = 99.22644411928067%, 99.20991868100477%, 99.01094953081872%
\hline
\multirow{4}{*}{{\rotatebox{90}{\textbf{Vec}}}}
& vec5                      & \success{} &   {\highlight{$4.72 (\stderr{  0.07})$}} &   1       & \success{} &  14.93 (\stderr{  0.21}) &   1       & \success{} &  78.90 (\stderr{  2.25}) &   6      \\% SmtT/T = 80.52034229302626%, 95.98937410635435%, 98.89325740352668%
& vec10                     & \success{} &   {\highlight{$5.51 (\stderr{  0.36})$}} &   1       & \success{} &  20.81 (\stderr{  0.77}) &   1       & \success{} & 142.87 (\stderr{  5.86}) &  11      \\  % SmtT/T = 57.23088868771888%, 91.78964923101526%, 98.76365835388931%
& vec15                     & \success{} &   {\highlight{$9.75 (\stderr{  0.25})$}} &   1       & \success{} &  22.09 (\stderr{  0.24}) &   1       & \success{} & 195.24 (\stderr{  3.20}) &  16      \\ % SmtT/T = 37.28829884651822%, 88.79604744779657%, 98.11179611018657%
& vec20                     & \success{} &  {\highlight{$13.40 (\stderr{  0.46})$}} &   1       & \success{} &  30.18 (\stderr{  0.20}) &   1       & \success{} & 269.52 (\stderr{  4.25}) &  21      \\ % SmtT/T = 25.881885439958207%, 82.374721744386%, 97.48691949245291%
\hline
\multirow{4}{*}{{\rotatebox{90}{\textbf{Misc}}}}
& split$\oplus$            & \success{} &   {\highlight{$3.79 (\stderr{  0.04})$}} &   1       & \success{} &   5.10 (\stderr{  0.16}) &   1       & \success{} & 10732.65 (\stderr{  8.01}) &   6      \\ % SmtT/T = 94.19385210967563%, 94.14708972520134%, 99.97155844397255%
& split$\otimes$                      & \success{} &  {\highlight{$14.07 (\stderr{  1.01})$}} &   3       & \success{} &  46.27 (\stderr{  2.04}) &   3       & \fail{} & Timeout & -                            \\ % SmtT/T = 87.40012646315907%, 97.15716368724911%
& share                     & \success{} & 292.02 (\stderr{ 11.37}) &  44       & \success{} & {\highlight{$100.85 (\stderr{  2.44})$}} &   6       & \success{} & 193.33 (\stderr{  4.46}) &  17      \\ % SmtT/T = 94.3058504587738%, 97.15508002763923%, 99.19701142373343%
& Exm.~\ref{exm:security}                 & \success{} &   {\highlight{$8.09 (\stderr{  0.46})$}} &   2       & \success{} &  26.03 (\stderr{  1.21}) &   2       & \success{} & 284.76 (\stderr{  0.31}) &   3      \\ % SmtT/T = 96.70245619318075%, 99.14227225428877%, 99.83905641669199%
\end{tabular}
}
\end{center}}}

\caption{Results. $\mu{T}$ in \emph{ms} to 2 d.p.
with standard sample error in brackets}
\label{tab:results}
\end{table}


\subsection{Results and Analysis}
%
For each synthesis problem, we recorded whether synthesis was successful or not
(denoted $\success$ or \fail), the mean total synthesis time ($\mu{T}$), and the
number of calls made to the SMT solver (\textsc{N}). Table~\ref{tab:results}
summarises the results with the fastest case for each benchmark highlighted. For
all benchmarks that used the SMT solver, the solver accounted for
$91.73\%-99.98\%$ of synthesis time, so we report only the mean total synthesis
time $\mu{T}$, rather than showing the SMT solver time separately as this gives
a good proxy of the solver time. We set a timeout of 120 seconds.

\subsubsection{Additive versus subtractive}

As conjectured in H1, the additive approach generally synthesises programs with
fewer calls to the solver than subtractive. Our first hypothesis holds for
almost all benchmarks, with the subtractive approach often far exceeding the
number made by the additive. This is explained by the difference in graded
variable synthesis between approaches. In the additive, a constant grade $1$ is
given for graded assumptions in the output context, whereas in the subtractive,
a fresh grade variable is created with a constraint on its usage which is
checked immediately. As the total synthesis time is almost entirely spent in the
SMT solver (more than 90\%), solving constraints is by far the most costly part
of synthesis leading to the additive approach synthesising most examples in a
shorter amount of time.

Graded variable synthesis in the subtractive case also results in several
examples failing to synthesise due to timeout. In some cases, e.g., the first three
\textit{comp} benchmarks, the subtractive approach times-out as synthesis
diverges with constraints growing in size due to the maximality condition and
absorbing behaviour of $[[ Intrv 0 Inf ]]$ interval. In the case of
$\textit{coK-$\mathcal{R}$}$ and $\textit{coK-$\mathbb{N}$}$, the generated
constraints have the form $\forall r. \exists s. r \sqsupseteq s + 1 $ which is
not valid $\forall r \in \mathbb{N}$ (e.g., when $r = 0$), which suggests that
the subtractive approach does not work well for polymorphic grades. As further
work, we are considering an alternate rule for synthesising promotion with
constraints of the form $\exists s . s = s' * r$, i.e., a multiplicative inverse
constraint.
%solving approach cannot tell this holds for all semirings. Indeed, it
%is false for $\ma
%which is not satisfiable for all semirings, including \textsc{N}, causing examples
%with arbitrary \textsc{N} grades to also fail.

In more complex examples we see evidence to support our second hypothesis. The
\textit{share} problem requires a lot of graded variable synthesis which is
problematic for the additive approach, for the reason described in H2 that synthesis 
may explore many paths which are incorrect in their eventual resource usage. In
contrast, the subtractive approach performs better, with $\mu{T} =
193.3\textit{ms}$ as opposed to additive's $292.02\textit{ms}$. However,
additive pruning outperforms both.

% Not as important
Notably, on examples which are purely linear such as \textit{andElim} from
Hilbert's axioms or \textit{mult} for function composition, the subtractive
approach generally performs better. Linear programs without graded modalities
can be synthesised without the need to interface with Z3 at all, making the
differences here somewhat negligible as solver time generally makes up for the
vast proportion of total synthesis time in the graded benchmarks.

\subsubsection{Additive pruning}
The pruning variant of additive synthesis (where subtraction
takes place in the premises of multiplicative rules) had mixed results
compared to the default. In simpler examples, the overhead of pruning
(requiring SMT solving) outweighs
the benefits obtained from reducing the space. However, in more
complex examples which involve synthesising many graded variables (e.g. \textit{share}), pruning is
especially powerful, performing better than the subtractive
approach. However, additive pruning failed to synthesis two
 examples which are polymorphic in their grade
 ($\otimes$-$\mathbb{N}$) and in the semiring ($\otimes$-$\mathcal{R}$).


Overall, the additive approach outperforms the subtractive and is able to
synthesise more examples, including ones polymorphic in grades and even the
semiring. Given that the literature on linear logic theorem proving is
typically subtractive, this is an interesting result. 

\section{Related Work}
\label{sec:linear-base-related}
Before~\citet{HODAS1994327}, the problem of resource non-determinism was first
identified by~\citet{harlandpym}. Their solution delays splitting of contexts at
a multiplicative connective. They propoesed a solution where proof search is
formulated in terms of constraints on propositions. The logic programming
language Lygon~\citep{lygon} implements this approach.

Our approach to synthesis implements a \textit{backward} style of proof search:
starting from the goal, recursively search for solutions to sub-goals. In
contrast to this, \textit{forward} reasoning approaches attempt to reach the
goal by building sub-goals from previously proved sub-goals until the overall
goal is proved.~\citet{10.1007/11532231_6,10.1007/11538363_15} consider forward
approaches to proof search in linear logic using the \textit{inverse
method}~\citep{DEGTYAREV2001179} where the resource non-determinism
that is typical to backward approaches is absent.

\section{Conclusion}
\label{sec:linear-base-conclusion}

At this point we have constructed a simple program synthesis tool for Granule,
paramterised by a resource management scheme, which effectively deals with the
problems of treating data as a resource inside a program. Both schemes would be
a reasonable choice for further development of a synthesis tool for our language
based on the graded linear $\lambda$-calculus.   

Going forward, however, we focus primarily on the additive resource management
scheme, using this as the basis for our more feature-rich fully-graded synthesis
calculus in Chapter~\ref{chapter:extended}.  
The evaluation in Section~\ref{sec:linear-base-evaluation} showed that the
additive approach generally yields smaller and simpler theorems than the
subtractive, requiring less time to solve. Theorem proving becomes even more
prevalent in synthesis for a fully graded typing calculus - potentially every
rule introduces new constraints that require solving, thus the speed at which
this can be carried out is especially important.

While the tool presented in this chapter allows users to synthesise a
considerable subset of Granule programs, our language is still limited in its
expressivity when compared to the core of Granule. Data types comprise only
product, sum, and unit types, while synthesis of recursive function defintions
or functions which make use of other in-scope values such as top-level
definitions is not permitted. One notable limitation of our typing calculi is
the inability to express (and therefore synthesise) programs which perform a
deep pattern match over a graded data type. A clear example of this can be found
in the synthesis of programs which distribute a graded modality over a data
type. Consider an example of a distributive program, \textit{push}:
\begin{align*}
  push: \Box_r(A \otimes B) \multimap \Box_r A \otimes \Box_r B
\end{align*}
which takes a data type graded by $r$ (in this case the product type $A \otimes
B$), and distributes $r$ over the constituent elements of the product $A$ and
$B$. This example is representative of a common class of graded programs known
as \emph{distribute laws}, however, we are unable to express such programs in
our simplified language.

In Granule, however, we can write the above as: 
\begin{granule}
push : forall {k : Semiring, r : k, a b : Type} 
     . (a * b) [r] -> (a [r] * b [r])
push [(x, y)] = ([x], [y])
\end{granule}
by pattern matching on both the graded modality and the product in a single
nexted pattern match. The ability to perform this \emph{deep} pattern matching
greatly increases the expressivity of our language, and allows us to both type
and potentially synthesis much more interesting and realistic Granule programs.

In the following chapter, we present an alternative approach to generating
programs which exhibit this distributive behaviour using a generic programming
methodology. The approach we present in Chapter~\ref{chapter:deriving} is not
type-directed program synthesis of the kind we have seen in this chapter, i.e.
it is not based on enumerative search. This approach complements the synthesis
calculi presented here and in Chapter~\ref{chapter:extended}, providing users
with a means to automatically generate programs purely from a type for a common
class of graded programs, as well as some useful structural combinators. In
describing this mechanism, we begin to enhance our language with more advanced
features such as pattern matching and recursion, further laying the foundations for
Chapter~\ref{chapter:extended}, and bringing our typing calculus closer in line
with what can be expressed in Granule.  

% Given this goal type, how would we go about synthesising a program
% in our tool? 

% We instatiate the \addAbsName\ rule at this type, building a partial synthesis
% derivation. Although we use the additive scheme for this example, the exact same
% situation arises in the subtractive.
% \begin{align*}
%     \inferrule*[right=\addUnboxName]
%       { x_2 :_r A \otimes B \vdash \Box_r A \otimes \Box_r B \Rightarrow\ ?\ |\ ? }
%       {\inferrule*[right=\addAbsName] {x_1 : \Box_r (A \otimes B) \vdash \Box_r A \otimes \Box_r B \Rightarrow\ \textbf{let}\ [x_2] = x_1\ \textbf{in}\ ?  \ |\ ?} 
%         {\emptyset \vdash \Box_r(A \otimes B) \multimap \Box_r A \otimes \Box_r B \Rightarrow \lambda x_1 . ? \ |\ ? }}
% \end{align*}
% After applying \addAbsName\ followed by \addUnboxName, we now have the graded
% assumption $x_2 :_r A \otimes B$ in our context which we must use to construct a
% term of type $\Box_r A \otimes \Box_r B$. We might expect that the path
% synthesis should take now would be to break $x_2$ down into two graded
% assumptions with types $A$ and $B$, promote these graded assumptions using the
% \addBoxName\ rule, before finally peforming a pair introduction to yield $\Box_r
% A \otimes \Box_r B$. However, in order to apply the pair elimination rule
% \addPairElimName\  
% and break our graded assumption into two, we must perform a dereliction on
% $x_2$, to yield a linear copy: 
% \begin{align*}
%   \inferrule*[right=\addDerName]
%       {x_2 :_r A \otimes B, x_3 : A \otimes B \vdash \Box_r A \otimes \Box_r B \Rightarrow\ ?\ }
%       { x_2 :_r A \otimes B \vdash \Box_r A \otimes \Box_r B \Rightarrow\ ?\ |\ ? }
% \end{align*}
% Clearly, this cannot lead us to the goal: the \addBoxName\ rule cannot promote
% terms using linear assumptions. Therefore, \textit{push} and other types which
% exhibit this distributive behaviour are not derivable in our calculi.
