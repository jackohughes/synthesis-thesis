\chapter{Automatically Deriving Graded Combinators}         
\label{chapter:deriving}
Thus far we have considered program synthesis from the perspective of
enumerative search, using our types to guide us and pruning the space of
programs where possible. This approach yielded a synthesis tool which was highly
expressive, allowing the synthesis of a program term for each syntactic form in
our core calculus. In this chapter we present an alternative approach, which
targets a specific class of graded programs: graded \emph{distributive} and
structural combinators. We view this approach as a useful complement to the more
powerful type-directed synthesis. Much of the content of this chapter is derived
from~\citet{DBLP:journals/corr/abs-2112-14966}, our Linearity/TLLA 2020 paper. 

When programming with graded modal types, we have observed there is often a need
to `distribute' a graded modality over a type, and vice versa, in order to
compose programs. That is, we may find ourselves in possession of a $\Box_r
(\mathsf{F} \alpha)$ value (for some parametric data type $\mathsf{F}$) which
needs to be passed to a pre-existing function (of our own codebase or a library)
which requires a $\mathsf{F} (\Box_r \alpha)$ value, or perhaps vice versa. A
\emph{distributive law} (in the categorical sense,
due to~\citet{street1972formal}) provides a conversion from one to the other. In
this chapter, we present a procedure to automatically synthesise these
distributive operators, applying a generic programming
methodology~\citep{hinze2000new} to compute these operations given the base type
(e.g., $\mathsf{F} \alpha$ in the above description). This serves to ease the
use of graded modal types in practice, removing boilerplate code by
automatically generating these `interfacing functions' on-demand, for
user-defined data types as well as built-in types.

Throughout, we refer to distributive laws of the form $\Box_r (\mathsf{F}
\alpha) \rightarrow \mathsf{F} (\Box_r \alpha)$ as \emph{push} operations (as
they `push' the graded modality inside the type constructor $\mathsf{F}$), and
dually $\mathsf{F} (\Box_r \alpha) \rightarrow \Box_r (\mathsf{F} \alpha)$ as
\emph{pull} operations (as they `pull' the graded modality outside
$\mathsf{F}$). We have already encountered some examples of push and pull
operations in Chapter~\ref{chapter:core}; our benchmarks showcased pull for 
product and sum types (i.e. $\mathsf{F}$ = $\otimes$ and $\oplus$, respectively), and 
push for linear function types ($\mathsf{F} = \multimap$). 


As a standalone methodology for generating a common class of graded programs,
this ``deriving mechanism'' serves as a complement to the synthesis calculi of
Chapter~\ref{chapter:core}. Synthesis problems which exhibit this distributive
behaviour pose issues in the existing calculi. However, in many cases the
solution programs to these distributive problems are straightforwardly derivable
from the type alone, making the costly enumerative search of type-directed synthesis
unnecessary.

Thus, we present a tool for an automatic procedure which calculates distributive
laws from graded types and present a formal analysis of their properties. This
approach is realised in Granule, embedded into the compiler. In doing so, we 
extend our graded linear $\lambda$-calculus of Section
~\ref{sec:linear-base} to incorporate data constructors and pattern matching, as well 
as recursive data types. 

\paragraph{Roadmap} We begin with a motivating example in Section~\ref{sec:motivating-example}
before defining our extended calculus in Section~\ref{sec:der-calculus}
providing an idealised, simply-typed subset of Granule with which we develop the
core deriving mechanism. Section~\ref{sec:push-pull-deriv} gives the procedures
for deriving \emph{push} and \emph{pull} operators for the calculus.
Section~\ref{sec:implementation-der} describes the details of how these
procedures are realised in the Granule language. We then provide examples of how
several other structural combinators in Granule may be derived using this tool
in Section~\ref{sec:deriving-other}. Finally, Section~\ref{sec:deriving-related}
discusses related work, while Section~\ref{sec:der-conclusion} highlights future
work.

We start with a motivating example typifying the kind of software engineering
impedance problem that distributive laws solve. We do so in Granule code since
it is the main vehicle for the developments here.

\section{Motivating Example}
\label{sec:motivating-example}

Consider the situation of projecting the first element of a pair. In Granule,
this first-projection is defined and typed as the following polymorphic function
(whose syntax is reminiscent of Haskell or ML):
%
\begin{granule}
fst : forall { a b : Type } . (a, b [0]) -> a
fst (x, [y]) = x
\end{granule}
%
Linearity is the default, so this represents a linear function applied to linear
values. However, the second component of the pair is graded, allowing weakening
to be applied via unboxing in the body to discard \granin{y} of type \granin{b}.
In graded linear $\lambda$-calculus of Section~\ref{sec:linear-base}, we denote `\granin{b [0]}' as the
type $\Box_0 b$.

The type for \granin{fst} is however somewhat restrictive: what if we are trying
to use such a function with a value (call it \granin{myPair}) whose type is not
of the form \granin{(a, b [0])} but rather \granin{(a, b) [r]} for some grading
term \granin{r} which permits weakening? Such a situation readily arises when we
are composing functional code, say between libraries or between a library and
user code. In this situation, \granin{fst myPair} is ill-typed. Instead, we
could define a different first projection function for use with \granin{myPair} as:
%
\begin{granule}
fst' : forall { a b : Type, s : Semiring, r : s } 
     . {0 <= r} => (a, b) [r] -> a
fst' [(x, y)] = x
\end{granule}
%
This implementation uses various language features of Granule to make it as
general as possible. Firstly, the function is polymorphic in the grade
\granin{r} and in the semiring \granin{s} of which \granin{r} is an element.
Next, a refinement constraint \granin{0 <= r} specifies that by the pre-ordering
\granin{<=} associated with the semiring \granin{s}, that \granin{0} is
approximated by \granin{r} (essentially, that \granin{r} permits weakening). The
rest of the type and implementation looks more familiar for computing a first
projection, but now the graded modality is over the entire pair.

From a software engineering perspective, it is cumbersome to create alternate
versions of generic combinators every time we are in a slightly different
situation with regards the position of a graded modality.  Fortunately, this is
an example to which a general \emph{distributive law} can be deployed. In this
case, we could define the following distributive law of graded modalities over
products, call it \granin{pushPair}:
%
\begin{granule}
pushPair : forall { a b : Type, s : Semiring, r : s } 
         . (a, b) [r] -> (a [r], b [r])
pushPair [(x, y)] = ([x], [y])
\end{granule}
%
This `pushes' the graded modality \granin{r} into the pair (via pattern matching
on the modality and the pair inside it, and then reintroducing the modality on
the right hand side via \granin{[x]} and \granin{[y]}), distributing the graded
modality to each component. Given this combinator, we can now apply \granin{fst
(pushPair myPair)} to yield a value of type \granin{a [r]}, on which we can
apply the Granule standard library function \granin{extract}:
\begin{granule}
extract : forall { a : Type, s : Semiring, r : s }
        . {(1 : s) <= r} => a [r] -> a
extract [x] = x
\end{granule}
 to get the original \granin{a} value we desired:
%
\begin{granule}
extract (fst (pushPair myPair)) : a
\end{granule}
%
The \granin{pushPair} function could be provided by the standard library, and
thus we have not had to write any specialised combinators ourselves: we have
applied supplied combinators to solve the problem.

Now imagine we have introduced some custom data type \granin{List} on which we
have a \emph{map} function:
%
\begin{granule}
data List a = Cons a (List a) | Nil

map : forall { a b : Type } . (a -> b) [0..Inf] -> List a -> List b
map [f] Nil = Nil;
map [f] (Cons x xs) = Cons (f x) (map [f] xs)
\end{granule}
%
Note that, via a graded modality, the type of \granin{map} specifies that the
parameter function, of type \granin{a -> b} is non-linear, used between $0$ and
$\infty$ times. Imagine now we have a value \granin{myPairList : (List (a, b))
[r]} and we want to map first projection over it. But \granin{fst} expects
\granin{(a, b [0])} and even with \granin{pushPair} we require \granin{(a, b)
[r]}. \emph{We need another distributive law}, this time of the graded modality
over the \granin{List} data type. Since \granin{List} was user-defined, we now
must roll our own \granin{pushList} operation with type \granin{List ((a, b)
[r]) -> List (a [r], b [r])}, and so we are back to having to make specialised
combinators for our data types.

The crux of this chapter is that such distributive laws can be automatically
calculated given the definition of a type. With our Granule implementation of
this approach (Section~\ref{sec:implementation-der}), we can then solve this
combination problem via the following composition of combinators:
%
\begin{granule}
map (extract . fst . push @(,)) (push @List myPairList) : List a
\end{granule}
%
where the \granin{push} operations are written with their base type via
\granin{@} (a type application) and whose definitions and types are
automatically generated during type checking. Thus the \granin{push} operation
is a \textit{data-type generic function}~\citep{hinze2000new}. This generic
function is defined inductively over the structure of types, thus a programmer
can introduce a new user-defined algebraic data type and have the implementation
of the generic distributive law derived automatically. This reduces both the
initial and future effort (e.g., if an ADT definition changes or new ADTs are
introduced). Furthermore, this technique is much less expensive than a proof search 
technique as no SMT solving on grade equations is required. 

Dual to the above, there are situations where a programmer may wish to
\emph{pull} a graded modality out of a structure. This is possible with a dual
distributive law, which could be written by hand as:
%
\begin{granule}
pullPair : forall { a b : Type, s : Semiring, m n : s } 
         . (a [n], b [m]) -> (a, b) [n /\ m]
pullPair ([x], [y]) = [(x, y)]
\end{granule}
%
Note that the resulting grade is defined by the greatest-lower bound (meet) of
\granin{n} and \granin{m}, if it exists as defined by a pre-order for semiring
\granin{s} (that is, $\sqcap$ is not a total operation). This allows some
flexibility in the use of the \emph{pull} operation when grades differ in
different components but have a greatest-lower bound which can be `pulled out'.
Our approach also allows such operations to be generically derived.

% Another potential example here?
% \granin{copyFst : forall {a, b : Type} . (a, b) [0..2] -> (a, a)}
% \granin{copyFst x = let x' = pushPair x in copy (fst x')}
%
% \granin{copyPair : forall {a, b : Type} . (a [0..2], b [0..2]) -> (a, b)[0..2]} }
% \granin{copyPair x = copy (pullPair x)}

%Our approach here is reminiscent ``push'' and ``pull'' operations are defined
%throughout the standard library for sums, products, and other ADTs such as
%lists. There is a general pattern to these distributive operations which allows
%their programs to be automatically synthesised from their type structure alone
%by a simple inductive procedure.


\section{Extending the Graded Linear-$\lambda$-Calculus}
\label{sec:der-calculus}
We define here a calculus which extends the graded linear
$\lambda$-calculus of ~\ref{sec:linear-base} with data constructors, pattern
matching, and recursive data types. This language constitutes a simplified
monomorphic subset of Granule. We include notions of data constructors and their
elimination via \textbf{case} expressions as a way to unify the handling of
regular type constructors.

The full syntax of terms and types is given by:
\begin{align*}
   \hspace{-0.9em}
   [[ t ]] ::= & \;
   [[ x ]]
   \mid \hspace{0.2em}[[ t1 t2 ]]
   \hspace{0.2em}\mid \hspace{0.2em}[[ \ x . t ]] 
   \hspace{0.2em}\mid \hspace{0.2em}[[ [ t ] ]] 
   \hspace{0.2em}\mid \hspace{0.2em}[[ Con t0 ... tn ]] \\
   \hspace{0.2em}\mid & \hspace{0.2em}[[ case t of p1 -> t1 ; * ; pn -> tn  ]] 
   \hspace{0.2em}\mid \hspace{0.2em}[[ letrec x = t1 in t2]]
 {\small{\tag{terms}}}
\end{align*}
\begin{align*}
   \hspace{-0.9em}
   [[ p ]] ::= & \;
   \hspace{0.2em} [[ x ]]
   \hspace{0.2em} \mid \hspace{0.2em} \_
   \hspace{0.2em }\mid \hspace{0.2em} [[ [ p ] ]]
   \hspace{0.2em} \mid \hspace{0.2em} [[ Con p0 ... pn ]]
{\small{\tag{patterns}}}
\end{align*}
\begin{align*}
   \hspace{-0.9em}
   [[ A ]], [[ B ]] ::= & \;
   \hspace{0.2em}[[ A -o B ]]
   \hspace{0.2em}\mid \hspace{0.2em} \alpha
   \hspace{0.2em}\mid \hspace{0.2em} [[ Tup A B ]]
   \hspace{0.2em}\mid \hspace{0.2em} [[ Sum A B ]]
   \hspace{0.2em}\mid \hspace{0.2em} [[ Unit ]]
   \hspace{0.2em} \mid \hspace{0.2em} [[ [] r A ]]
 %  \hspace{0.2em} \mid \hspace{0.2em} [[ A B ]]
 %  \hspace{0.2em} \mid \hspace{0.2em} D
   \hspace{0.2em}\mid \hspace{0.2em} [[ mu X . A ]]
   \hspace{0.2em}\mid \hspace{0.2em} [[ X ]]
 {\small{\tag{types}}} \\
 \hspace{-0.9em}
   [[ C ]] ::= & \; () \mid \mathsf{inl} \mid \mathsf{inr}
                 \mid (,)
 {\small{\tag{data constructors}}}
 \end{align*}
Terms comprise the graded linear $\lambda$-calculus of
Section~\ref{sec:linear-base}, extended with data constructors, case statements
for pattern matching over a scrutinee term, and standard recursive let bindings.
Patterns may either be variable, wildcard, box, or constructor patterns. 
A notable difference from Section~\ref{sec:linear-base} is that unboxing here
takes place via case pattern matching, instead of a specialised let expression, i.e.
$\textbf{let}\ [x] = y\ \textbf{in}\ t$ becomes $\textbf{case}\ y\ \textbf{of}\ [x] \mapsto t$
%

The syntax of types is fairly straightforward. We make type variables explicit
in our syntax via $\alpha$ to allow distributive laws to be derived on
parametric types, and $X$ represents a set of recursion variables which are not
exposed to the user. For the most part, typing follows the calculus defined in
section~\ref{sec:linear-base-calculus}. Figure~\ref{fig:deriving-typing-rules}
gives the additional rules. We briefly explain the extensions introduced for
this chapter.

\begin{figure}[t]
    \begin{align*}
      \begin{array}{c}
    \inferrule*[right=Letrec]{[[G1, x : A |- t1 : A ]] \\ [[G2, x : A  |- t2 : B]] }
    { [[ G1 + G2 |- letrec x = t1 in t2 : B  ]]}
\\[1.25em]
    \inferrule*[right=Con]{([[ C ]] : B_1 \multimap ... \multimap B_n \multimap A) \in D}
              { \emptyset\ \vdash\ [[C]] : B_1 \multimap ... \multimap B_n \multimap A}
\\[1.25em]
\inferrule*[right=Case]
  { \Gamma \vdash t : A \quad \cdot \vdash p_i : A \triangleright \Delta_i \quad \Gamma', \Delta_i \vdash t_i : B}
  {[[ G  + G' |- case t of p1 -> t1 ; * ; pn -> tn : B ]]}
    \end{array}
 \end{align*}
\vspace{-0.8em}
 \caption{Typing rules for the extended graded linear $\lambda$-calculus}
 \label{fig:deriving-typing-rules}
\end{figure}

The \textsc{LetRec} rule provides recursive bindings in the standard
way. 

Data constructors with zero or more arguments are introduced via
the \textsc{Con} rule. Here, the constructors that concern us are
units, products, and coproducts (sums), given by $[[ Defines ]]$,
a global set of data constructors with their types, defined:
%
\begin{align*}
[[ Defines ]] & = \{() : [[ Unit ]]\}\\
& \cup \{(,) : [[ A -o {B -o Tup A B} ]] \mid \forall [[ A ]], [[ B ]] \} \\ 
& \cup \{\mathsf{inl} : [[ A -o Sum A B ]] \mid \forall [[ A ]], [[ B ]] \} \\
& \cup \{\mathsf{inr} : [[ B -o Sum A B ]] \mid \forall [[ A ]], [[ B ]] \}
\end{align*}
%
Constructors are eliminated by pattern matching via the \textsc{case} rule.
Patterns $p$ are typed by judgments of the form $?r \vdash p : A \triangleright
\Delta$ meaning that a pattern $p$ has type $A$ and produces a context of typed
binders $[[ D ]]$ (used, e.g., in the typing of the case branches). The
information to the left of the turnstile denotes optional grade information
arising from being in an unboxing pattern and is syntactically defined as
either:
%
\begin{align*}
?r ::= [[ none ]] \mid [[ r ]]
\tag{enclosing grade}
\end{align*}
%
 where $[[ none ]]$ means the present pattern is not nested inside an
unboxing pattern and $[[ r ]]$ that the present pattern is nested
inside an unboxing pattern for a graded modality with grade $[[r]]$.

\begin{figure}[t]
\begin{align*}
\setlength{\arraycolsep}{0em}
\begin{array}{ccc}
\inferrule*[right=Pvar]
 {\quad}
 {\cdot \vdash x : A \triangleright x : A}
\;\;\;
\inferrule*[right=Pcon]
{\cdot \vdash p_i : B_i \triangleright \Gamma_i}
{\cdot \vdash C p_1 .. p_n : A \triangleright \Gamma_1, .., \Gamma_n}
\\[1.25em]
\inferrule*[right={Pbox}]
{[[ r |- p : A |> G ]]}
{\cdot \vdash [p] : \Box_r A \triangleright \Gamma }
 \\[1.25em]
\inferrule*[right={[}Pcon{]}]
{[[ r |- pi : Bi |> Gi ]] \quad\quad
%[[ C ]] : [[ B1 .*. Bn -> A ]] \quad
[[ |A| > 1 ]] \Rightarrow [[ 1 <<= r ]]}
{r \vdash C p_1 .. p_n : A \triangleright \Gamma_1, .., \Gamma_n}
%
\\[1.25em]
\inferrule*[right={[}Pvar{]}]
 {\quad}
 {[[ r |- x : A |> x : [ A ] { r } ]]}
 \;\;\;\;
\inferrule*[right={[}Pwild{]}]
 {[[ 0 <<= r ]]}
 {[[ r |- _ : A |> . ]]}
\end{array}
\end{align*}
\caption{Pattern typing rules for the extended graded linear $\lambda$-calculus}
\label{fig:deriving-pattern-rules}
\end{figure}

The rules of pattern typing are given in
Figure~\ref{fig:deriving-pattern-rules}.
The rule (\textsc{PBox}) provides
graded modal elimination (an `unboxing' pattern),
propagating grade information into the typing of the
sub-pattern. Thus $[[ case t of [p] -> t' ]]$ can be used to eliminate
a graded modal value. Variable patterns are typed via two
rules depending on whether the variable occurs inside an unbox pattern
(\textsc{[Pvar]}) or not (\textsc{Pvar}),
with the \textsc{[Pvar]} rule producing a binding with the grade of
the enclosing boxâ€™s grade $[[ r ]]$.
As with variable patterns, constructor patterns are split
between rules for patterns which either occur inside an unboxing
pattern or not. In the former case, the grade information is
propagated to the sub-pattern(s), with the additional constraint that
if there is more than one data constructor for the type $[[ A ]]$ (written
$|A| > 1$), then the grade $r$ must approximate 1 (written $[[ 1 <<= r
]]$) as pattern matching
incurs a usage to inspect the constructor. The
operation $|[[ A ]]|$ counts the number of data constructors
for a type:
%
\begin{align*}
  \begin{array}{ll}
|[[ Unit ]]| &= 1 \\
|[[ A -o B ]]| &= 1 \\
|[[ [] r A ]]| &= |[[  A ]]| \\
|[[ Sum A B ]]| &= |A| + |B| \\
|[[ Tup A B ]]| &= |A| |B| \\
|[[ mu X . A ]]| &= |A [ [[ mu X . A ]] / X ]|
  \end{array}
\end{align*}
%
and $|[[ X ]]|$ is undefined (or effectively 0) since we do not
allow unguarded recursion variables in types.
A type $A$ must therefore involve a sum type for $|A| > 1$.

Since a wildcard pattern $[[ _ ]]\,$ discards a value, this is only
allowed inside an unboxing pattern where the enclosing grade
permits weakening, captured via $[[ 0 <<= r ]]$ in rule \textsc{[Pwild]}.

\section{Automatically Deriving \emph{push} and \emph{pull}}
\label{sec:push-pull-deriv}
Now that we have established the language, we describe the algorithmic
calculation of distributive laws. Universal quantification over type variables
($\alpha$) takes place implicitly prior to running the deriving mechanism, thus
 type variables may be treated as logical atoms.

%
\subsection{Notation}
Let $\tcF : \mathsf{Type}^n \rightarrow \mathsf{Type}$
be an $n$-ary type constructor (i.e. a constructor which takes $n$ type arguments), whose free type variables
provide the $n$ parameter types. We write $\tcF \overline{\alpha_i}$ for
the application of $\tcF$ to
type variables $\alpha_i$ for all $1 \leq i \leq n$.

%\dnote{Split and do one at time? Might be better.}
%\paragrapho{Pull}
%
\subsection{Push}
We automatically
calculate \emph{push} for $\mathsf{F}$
applied to $n$ type variables
% $\{\alpha_i\}^n_i$
$\overline{\alpha_i}$
as the operation:
%
\begin{align*}
\deriv{push}{\tcF \overline{\alpha_i}} : \Box_{r} \tcF \overline{\alpha_i}
  \multimap  \tcF  (\overline{\Box_{r} \alpha_i})
 \end{align*}
where we require $[[ 1 <<= r ]]\ \textit{if}\ |\tcF \overline{\alpha_i}| > 1$
due to the $[\textsc{Pcon}]$ rule (e.g., if $\tcF$ contains a sum).

%and $\deriv{pull}{A} = \lambda z. \deriv{pull}{A}^\emptyset z$
%
%\dnote{put very short descriptions like comments on the right hand side?}
\begin{figure}
\begin{align*}
\hspace{-1em}\deriv{push}{[[ Unit ]]}^\Sigma \ z & = [[ case z of [ () ] -> () ]]
\\
\hspace{-1em}\deriv{push}{\alpha}^\Sigma       \ z & = z
  \\
  \deriv{push}{X}^\Sigma     \ z & = \Sigma(X)\ z
\\
\hspace{-1em}\deriv{push}{A \oplus B}^\Sigma \ z & =
\derCaseTwo{z}{[\mathsf{inl}\ x]}{\mathsf{inl}\ \deriv{push}{A}^{\Sigma}[x]}{[\mathsf{inr}\ y]}
                            {\mathsf{inr}\
                                      \deriv{push}{B}^{\Sigma}[y]}
\\
\hspace{-1em}\deriv{push}{A \otimes B}^\Sigma \ z & =
\derCaseOne{z}{[(x, y)]}
   {(\deriv{push}{A}^{\Sigma}[x], \deriv{push}{B}^{\Sigma}[y])}
% \\
%   \deriv{push}{A \multimap B}^\Sigma \ z & =
%                                 \lambda y .
%                                 \derCaseOne{z}{[f]}{\derCaseOne{\deriv{pull}{A}^{\Sigma}\
%                                     y}{[u]}{\deriv{push}{B}^{\Sigma}[(f
%                                 \ u)]}}
\\
\hspace{-1em}\deriv{push}{A \multimap B}^\Sigma \ z & = \lambda y . \textbf{case}\ z\ \textbf{of}\ [f] \rightarrow \\ 
  & \;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \textbf{case}\ \deriv{pull}{A}^{\Sigma}\ y\ \textbf{of}\ [u] \rightarrow 
  {\deriv{push}{B}^{\Sigma}[(f\ u)]}
\\
%%%%%
\hspace{-1em}\deriv{push}{\mu X . A}^\Sigma \ z & =
 \derLetRec{f}{\deriv{push}{A}^{\Sigma, X \mapsto
f : [[ {mu X . {[] r A}} -o {{(mu X . A)} [ [] r ai /* ai ]} ]] }}{f\ z}
\end{align*}
\caption{Interpretation rules for $\deriv{push}{A}$}
\label{fig:push-interp}
\end{figure}

For types $A$ closed with respect to recursion variables, let $\deriv{push}{A} =
\lambda z . \deriv{push}{A}^\emptyset\ z$ given by an intermediate
interpretation $\deriv{push}{A}^\Sigma$ where $\Sigma$ is a context of
\textit{push} combinators for the recursive type variables. This interpretation
is defined by Figure~\ref{fig:push-interp}.
%
In the case of \emph{push} on a value of type $[[ Unit ]]$, we pattern match on
the value, eliminating the graded modality via the unboxing pattern match and
returning the unit value. For type variables $\alpha$, \emph{push} is simply the
identity of the value, while for recursion variables $X$ we lookup the
variables's binding in $\Sigma$ and apply it to the value $z$. For sum and
product types, \emph{push} works by pattern matching on the type's
constructor(s) and then inductively applying \emph{push} to the promoted
arguments, re-applying them to the constructor(s). Unlike \emph{pull} below, the
\emph{push} operation can be derived for function types, with a contravariant
use of \emph{pull}. For recursive types, we inductively apply \emph{push} to the
value with a fresh recursion variable bound in $\Sigma$, representing a
recursive application of push. There is no derivation of a distributive law for
types which are themselves graded modalities, as this would depend on the
particular graded modalities being distributed
(see~\citet{DBLP:conf/icfp/GaboardiKOBU16}). 

Section~\ref{proof:deriving-type-soundness} in Appendix~\ref{appendix:proofs}
gives the proof that $\deriv{push}{A}$ is type sound, i.e., its derivations are
well-typed: 

\begin{restatable}[Type soundness of $\deriv{push}{\tcF\ \overline{\alpha_i}}^{\Sigma}$]{proposition}{pushSound}
\label{prop:soundness-push}
$\deriv{push}{\tcF \overline{\alpha_i}}^{\Sigma} : \Box_{r} \tcF \overline{\alpha_i}
  \rightarrow  \tcF  (\overline{\Box_{r} \alpha_i})$
\end{restatable}

\subsection{Pull}
We automatically
calculate \emph{pull} for $\mathsf{F}$
applied to $n$ type variables
% $\{\alpha_i\}^n_i$
$\overline{\alpha_i}$
as the operation:
\begin{align*}
\deriv{pull}{\tcF\ \overline{\alpha_i}} : \tcF\ (\overline{\Box_{r_i} \alpha_i})
\multimap \Box_{\bigsqcap^n_{i = 1} r_i} (\tcF\  \overline{\alpha_i})
 \end{align*}
Type constructor $\tcF$ here is applied to $n$ arguments each of the form
$\Box_{r_i} \alpha_i$, i.e., each with a different grading of which the
greatest-lower bound $\bigsqcap^n_{i = 1} r_i$ (derived from the semiring's
pre-order, as per Definition~\ref{def:glb}) is the resulting grade (see \granin{pullPair} from
Section~\ref{sec:motivating-example}).

% (for recursion variables $\Sigma$):
%and $\deriv{pull}{A} = \lambda z. \deriv{pull}{A}^\emptyset z$
\begin{figure*}
\begin{align*}
\hspace{-2em}\deriv{pull}{[[ Unit ]]}^\Sigma            \ z & = [[ case z of () -> [ () ] ]]
\\
\hspace{-2em}\deriv{pull}{\alpha}^\Sigma      \ z & = z
                                       \\
\hspace{-2em}\deriv{pull}{X}^\Sigma           \ z & = \Sigma(X)\ z
                                    \\
\hspace{-2em}\deriv{pull}{A \oplus B}^\Sigma  \ z & =\derCaseTwo{z}{\mathsf{inl}\
                                       x}{\derCaseOne{\deriv{pull}{A}^{\Sigma}\ x}{[u]}{[\mathsf{inl}\ u]}}{\mathsf{inr}\ y}
                            {\derCaseOne{\deriv{pull}{B}^{\Sigma}\
                                       y}{[v]}{[\mathsf{inr}\ v]}}
\\
\hspace{-2em}\deriv{pull}{A \otimes B}^\Sigma \ z & =
\derCaseOne{z}{(x, y)}
   {\\ & \;\;\;\;\;\;\;\;\;\;\; \derCaseOne{(\deriv{pull}{A}^{\Sigma}\ x, \deriv{pull}{B}^{\Sigma}\ y)}
                {([u], [v])}{[(u, v)]}} \\
%%%
\hspace{-2em}\deriv{pull}{\mu X . A}^\Sigma \ z & =
   \derLetRec{f}{\deriv{pull}{A}^{\Sigma, X \mapsto f : [[ {mu X . {A [ [] ri ai /* ai ]}}
                                 -o [] {BigMeet ri} (mu X . A) ]] }}{f\ z}
 \end{align*}
\caption{Interpretation rules for $\deriv{pull}{A}$}
\label{fig:pull-interp}
\end{figure*}

For types $A$ closed with respect to recursion variables, let $\deriv{pull}{A} =
\lambda z . \deriv{pull}{A}^\emptyset\ z$ given by an intermediate
interpretation $\deriv{pull}{A}^\Sigma$ where $\Sigma$ is a context of
\textit{pull} combinators for the recursive type variables. This interpretation
is defined by Figure~\ref{fig:pull-interp}.
%\paragrapho{Push}

%
%
Just like \emph{push}, we cannot apply \emph{pull} to graded modalities
themselves. Unlike \emph{push}, we cannot apply \emph{pull} to function types.
That is, we cannot derive a distributive law of the form $(\Box_r A \multimap
\Box_r B) \multimap \Box_r (A \multimap B)$. This is because introducing the concluding
$\Box_r$ would require the incoming function $(\Box_r A \multimap \Box_r B)$ to
itself be inside $\Box_r$ due to the promotion rule (\textsc{pr}), which does
not match the type scheme for \emph{pull}.

The rest of the derivation above is similar but dual to that of \emph{push}.

Section~\ref{proof:deriving-type-soundness} in Appendix~\ref{appendix:proofs}
gives the proof that $\deriv{pull}{A}$ is type sound, i.e., its derivations are
well-typed:

\begin{restatable}[Type soundness of $\deriv{pull}{\tcF\ \overline{\alpha_i}}$]{proposition}{pullSound}
\label{prop:soundness-pull}
 $\tcF\ (\overline{\Box_{r_i} \alpha_i})
\rightarrow \Box_{\bigsqcap^n_{i = 1} r_i} (\tcF\  \overline{\alpha_i})$
\end{restatable}

\begin{example}
  \label{ex:push-fun}
  To illustrate the above procedures, the derivation of
  $\lambda z . \deriv{push}{(\alpha \otimes \alpha) \multimap \beta}\ z : \Box_r ((\alpha \otimes \alpha) \multimap \beta) \multimap
             ((\Box_r \alpha \otimes \Box_r \alpha) \multimap \Box_r \beta) $ is:
  %
{\small{
  \begin{align*}
       & \lambda z . \deriv{push}{(\alpha \otimes \alpha) \multimap \beta}^\emptyset \ z
\\
  = \; & \lambda z . \lambda y . \derCaseOne{z}{[f]}{\derCaseOne{\deriv{pull}{\alpha
      \otimes \alpha}^{\emptyset}\
         y}{[u]}{\deriv{push}{\beta}^{\emptyset}[(f \ u)]}}
    \\
  = \; & \lambda z . \lambda y . \derCaseOne{z}{[f]}{\\ & \hspace{2em} \derCaseOne{(\derCaseOne{y}{(x', y')}
   {\\ & \hspace{5em} \derCaseOne{(\deriv{pull}{\alpha}^{\emptyset}\ x', \deriv{pull}{\alpha}^{\emptyset}\ y')}
                {([u], [v])}{[(u, v)]}})}{\\ & \;\;\;\;\;\;\;\;\;\;\;\;\; [u]}{\deriv{push}{\beta}^{\emptyset}[(f \ u)]}}
%%
    \\
  = \; & \lambda z . \lambda y . \derCaseOne{z}{[f]}{\\ & \hspace{2em} \derCaseOne{(\derCaseOne{y}{(x', y')}
   { \\ & \hspace{5em} \derCaseOne{(x', y')}
                {([u], [v])}{[(u, v)]}})}{[u]}{[(f \ u)]}} 
  \end{align*}
}}
  %
\end{example}

\begin{remark}
One might ponder whether linear logic's exponential $!
A$~\citep{girard1987linear} is modelled by the graded necessity modality over
$\mathbb{N}_{\infty}$ intervals, i.e., with $! A \triangleq
\Box_{\textcolor{coeffectColor}{[[ IntervalSyn 0 Inf ]] }} A$. This is a
reasonable assumption, but $\Box_{\textcolor{coeffectColor}{[[ IntervalSyn 0 Inf
]] }} A$ has a slightly different meaning to $! A$, exposed here: whilst
$\deriv{push}{A \otimes B} : \Box_{\textcolor{coeffectColor}{[[ IntervalSyn 0
Inf ]]}} (A \otimes B) \multimap (\Box_{\textcolor{coeffectColor}{[[ IntervalSyn
0 Inf ]] }} A \otimes \Box_{\textcolor{coeffectColor}{[[ IntervalSyn 0 Inf ]]}}
B)$ is derivable in our language, linear logic does not permit $!(A \otimes B)
\multimap (!A \otimes !B)$. Models of $!$ provide only a monoidal functor
structure which gives \emph{pull} for $\otimes$, but not
\emph{push}~\citep{benton1992linear}. This structure can be recovered in Granule
through the introduction of a partial type-level operation which selectively
disallows \emph{push} for $\otimes$ in semirings which model the $!$ modality of
linear logic. An example of such a semiring is the \{$\mathsf{Zero}$,\
$\mathsf{One}$,\ $\omega$\} semiring we encountered in
Example~\ref{example:01omega} on page~\pageref{example:01omega}, which is
intended to specifically mode linear logic's $!$ modality.\footnote{The work in
~\citet{hughes:lirmm-03271465} arose as a result of this work.}
\end{remark}

The algorithmic definitions of `push' and `pull' can be leveraged
in a programming context to automatically yield these combinators for
practical purposes. We discuss how this is leveraged inside the
Granule compiler in Section~\ref{sec:implementation-der}.
Before that, we study the algebraic behaviour of the derived distributive laws.

\subsection{Properties}
\label{subsection:properties}

%We consider here some properties of the distributive laws: that they
%are mutually inverse (under some restriction) and that they behave
%well with respect to structure associated with the graded modalities,
%specifically that they yield distributive laws of endofunctors over
%graded comonads. Proofs are given in Appendix~\ref{app:properties}.

We note that these distributive laws are mutually inverse:

\begin{restatable}[Pull is right inverse to push]{prop}{pushPullInverse}
  For all $n$-arity types $\tcF$ which do not contain function types,
  then for type variables $(\alpha_i)_{i \in 1 \leq i \leq n}$
  and for all grades $r \in \mathcal{R}$ where $[[1 <<= r]]$ if $|\tcF
    \overline{\alpha_i}| > 1$, then:
    %
  \begin{align*}
\deriv{pull}{\tcF\
  \overline{\alpha_i}}(\deriv{push}{\tcF\
  \overline{\alpha_i}}) = id\ : \Box_{r} \tcF \overline{\alpha_i}
\multimap \Box_{r} \tcF \overline{\alpha_i}
    \end{align*}
\end{restatable}

\begin{restatable}[Pull is left inverse to push]{prop}{pullPushInverse}
  For all $n$-arity types $\tcF$ which do not contain function types,
  then for type variables $(\alpha_i)_{i \in 1 \leq i \leq n}$
  and for all grades $r \in \mathcal{R}$ where $[[1 <<= r]]$ if $|\tcF
    \overline{\alpha_i}| > 1$, then:
  \begin{align*}
\deriv{push}{\tcF\
  \overline{\alpha_i}}(\deriv{pull}{\tcF\
  \overline{\alpha_i}}) = id\ : \tcF (\Box_{r} \overline{\alpha_i})
\multimap \tcF (\Box_{r} \overline{\alpha_i})
    \end{align*}
  \end{restatable}

\noindent
Section~\ref{proof:inverse-proofs} of Appendix~\ref{appendix:proofs} gives the
proofs, leveraging a typed equational theory for our lanugage. This equational
theory is defined in Section~\ref{sec:typed-eq} of
Appendix~\ref{appendix:proofs}.

Additional properties of these distributive laws can be found in
\citet{DBLP:journals/corr/abs-2112-14966}. Prima facie, the above \emph{push}
and \emph{pull} operations are simply distributive laws between two (parametric)
type constructors $\tcF$ and $\Box_r$, the latter being the graded modality.
However, both $\tcF$ and $\Box_r$ have additional structure. If the mathematical
terminology of `distributive laws' is warranted, then such additional structure
should be preserved by \emph{push} and \emph{pull} (e.g., as in how a
distributive law between a monad and a comonad must preserve the behaviour of
the monad and comonad operations after applying the distributive
law~\citep{power2002combining}). We choose to omit discussion of these
properties here for brevity. 

% Applying a mathematical perspective, $\Box_r$ is also an endofunctor with
% its object mapping provided by the type constructor itself and its
% morphism mapping behaviour defined as follows:
% %
% \begin{definition}[$\Box_r$ functor]
%   Given a function $f : \alpha \multimap \beta$ (a closed function
%   term) then $\Box_r f : \Box_r \alpha \multimap \Box_r \beta$ is the
%   morphism mapping of the endofunctor $\Box_r$ defined:
%   %
%   \begin{align*}
%     \Box_r\ f = \lambda x. [[ case x of [y] -> [f y] ]]
%   \end{align*}
%   %
% \end{definition}
% %
% \noindent
% For types $\tcF \alpha$ we can also automatically derive the
% morphism mapping of a covariant functor, which we write as $\deriv{fmap}{\tcF \alpha}$
% whose definition is standard (e.g., applied in Haskell~\cite{generic-deriving})  given in
% the appendix~\cite{appendix}. Distributive laws between
% endofunctors should be natural transformations, which is indeed the case for our
% derivations:

% \begin{restatable}[Naturality of push]{prop}{pushNatural}
%   For all unary type constructors $\tcF$ such that $\deriv{push}{\tcF \alpha}$ is defined, and given a closed function term $f : \alpha \multimap \beta$, then: $ {\deriv{fmap}{\tcF} \Box_{r}f} \circ \deriv{push}{\tcF {\alpha}} = \deriv{push}{\tcF {\beta}} \circ  \Box_{r} \deriv{fmap}{\tcF} f $, i.e.:
% \begin{align*}
% \xymatrix@C=3.5em{
% {\alpha} \ar[d]_{f}
% \\
% {\beta} &
% }
% \quad
% \xymatrix@C=5em{
% {\Box_{r} \tcF {{\alpha}}} \ar[d]_{\Box_{r} \deriv{fmap}{\tcF } f } \ar[r]^{\deriv{push}{\tcF {\alpha}}}  &   {\tcF \Box_{r} {\alpha}}
% \ar[d]^{\deriv{fmap}{\tcF } \Box_{r}f}   \\
% {\Box_{r} \tcF {{\beta}}}  \ar[r]_{\deriv{push}{\tcF {\beta}}} &  {\tcF \Box_{r} {\beta}}}
% \end{align*}
% \end{restatable}

% \begin{restatable}[Naturality of pull]{prop}{pullNatural}
%   For all unary type constructors $\tcF$ such that $\deriv{pull}{\tcF \alpha}$ is defined, and given a closed function term $f : \alpha \multimap \beta$, then: $\Box_{r} \deriv{fmap}{\tcF} f \circ \deriv{pull}{\tcF {\alpha}} = \deriv{pull}{\tcF {\beta}} \circ {\deriv{fmap}{\tcF} \Box_{r}f}$, i.e.:
% \begin{align*}
% \xymatrix@C=3.5em{
% {\alpha} \ar[d]_{f}
% \\
% {\beta} &
% }
% \quad
% \xymatrix@C=5em{
% {\tcF \Box_{r} {\alpha}} \ar[d]_{\deriv{fmap}{\tcF}\Box_{r}f} \ar[r]^{\deriv{pull}{\tcF {\alpha}}}  & {\Box_{r} \tcF {{\alpha}}}
% \ar[d]^{ \Box_{r}\deriv{fmap}{\tcF} f}   \\
% { \tcF \Box_{r} {\beta}}  \ar[r]_{\deriv{pull}{\tcF {\beta}}} &  {\Box_{r} \tcF {{\beta}}}}
% \end{align*}
% \end{restatable}
%
% \noindent
% The appendix~\cite{appendix} gives the proofs.
% Note that the naturality results here are for cases of unary type
% constructors $\tcF$ that are covariant functors, written with a single parameter $\alpha$. This can
% easily generalise to $n$-ary type constructors.

% Not only is $\Box_r$ an endofunctor but it also has the structure
% of a \emph{graded comonad}~\cite{combining2016,DBLP:conf/fossacs/Katsumata18,petricek2014coeffects,DBLP:conf/icalp/PetricekOM13}.
% %

% \begin{definition}[Graded comonadic operations]
% The \grminip{} calculus (and Granule) permits the derivation of graded
% comonadic
% operations~\cite{DBLP:journals/pacmpl/OrchardLE19}
% for the semiring graded necessity $\Box_r$, defined:
% %
% \begin{align*}
% \varepsilon_A & : \Box_1 A \multimap A = \lambda x . [[ case x of [z] -> z ]] \\
% \delta_{r,s,A} & : \Box_{r \ast{} s} A \multimap \Box_r \Box_s A
% = \lambda x . [[ case x of [z] -> [ [z] ] ]]
%  \end{align*}
%  \end{definition}
% %
% \noindent
%  The derived distributive laws preserve these graded comonadic
%  operations i.e., the distributive laws are
%  well-behaved with respect to the graded comonadic structure of
%  $\Box_r$, captured by the following properties:

% \begin{restatable}[Push preserves graded comonads]{prop}{pushPreserve}
% %
% For all $\tcF$ such that $\deriv{push}{\tcF \overline{\alpha_i}}$ is defined
% and $\tcF$ does not contain $\multimap$ (to avoid issues of
% contravariance in $\tcF$) then:
% %
% \begin{align*}
% \xymatrix@C=3.5em@R=1.4em{
% \Box_{1} \tcF \overline{\alpha_i} \ar[r]^{\deriv{push}{\tcF
%   \overline{\alpha_i}}} \ar[d]_{\varepsilon}
% & \tcF \overline{\Box_{1} \alpha_i}
% \ar[dl]^{\tcF \varepsilon}
% \\
% \tcF \overline{\alpha_i} &
% }
% \quad
% \xymatrix@C=5em@R=1.4em{
% \Box_{r\ast{}s}
% \tcF \overline{\alpha_i} \ar[d]_{\delta_{r,s}} \ar[rr]^{\deriv{push}{\tcF \overline{\alpha_i}}}
% & & \tcF \overline{\Box_{r\ast{}s} \alpha_i} \ar[d]^{\tcF \delta_{r,s}}
% \\
% \Box_{r} \Box_{s} \tcF \overline{\alpha_i} \ar[r]_{\Box_r \deriv{push}{\tcF \overline{\alpha_i}}}&
% \Box_{r}\tcF \overline{\Box_{s} \alpha_i} \ar[r]_{\deriv{push}{\tcF \overline{\alpha_i}}} &
% \tcF \overline{\Box_{r} \Box_{s} \alpha_i}
% &
% }
% \end{align*}
% \end{restatable}

% \begin{restatable}[Pull preserves graded comonads]{prop}{pullPreserve}
% \label{prop:pullPreserve}
% %
% % TODO: DAO 23/10/2020
% % There might be a more general thing we can do here with taking the
% % meet of many grades, but it's too complicated for me to think about
% % right now!
% %
% For all $\tcF$ such that $\deriv{pull}{\tcF \overline{\alpha_i}}$ is defined then:
% %
% \begin{align*}
% \xymatrix@C=3.5em@R=1.4em{
% \Box_{1} \tcF \overline{\alpha_i} \ar[d]_{\varepsilon}
% &  \ar[l]_{\deriv{pull}{\tcF \overline{\alpha_i}}} \tcF
%   \overline{\Box_{1} \alpha_i}
% \ar[dl]^{\tcF \varepsilon}
% \\
% \tcF \overline{\alpha_i} &
% }
% \quad
% \xymatrix@C=5em@R=1.4em{
% \Box_{r\ast{}s}
% \tcF \overline{\alpha_i} \ar[d]_{\delta_{r,s}}
% & & \ar[ll]_{\deriv{pull}{\tcF \overline{\alpha_i}}} \tcF
%     \overline{\Box_{r\ast{}s} \alpha_i}
% \ar[d]^{\tcF \delta_{r,s}}
% \\
% \Box_{r} \Box_{s} \tcF \overline{\alpha_i} & \ar[l]^{\Box_r \deriv{pull}{\tcF \overline{\alpha_i}}}
% \Box_{r}\tcF \overline{\Box_{s} \alpha_i} & \ar[l]^{\deriv{pull}{\tcF \overline{\alpha_i}}}
% \tcF \overline{\Box_{r} \Box_{s} \alpha_i}
% &
% }
% \end{align*}
% \end{restatable}
% %
% \noindent
% The appendix~\cite{appendix} gives the proofs.

\section{Implementation in Granule}
\label{sec:implementation-der}
The Granule type checker implements the algorithmic derivation of \emph{push}
 and \emph{pull} distributive laws as covered in the previous section. Whilst the
 syntax of our language types had unit, sum, and product types as primitives, in
 Granule these are provided by a more general notion of type constructor which
 can be extended by user-defined, generalized algebraic data types (GADTs). The
 procedure outlined in Section~\ref{sec:push-pull-deriv} is therefore generalised
 slightly so that it can be applied to any data type: the case for $A \oplus B$
 is generalised to types with an arbitrary number of data constructors.
 
 Our deriving mechanism is exposed to programmers via explicit (visible) type
 application (akin to that provided in GHC Haskell~\citep{eisenberg2016visible})
 on reserved names \granin{push} and \granin{pull}. Written \granin{push @T} or
 \granin{pull @T}, this signals to the compiler that we wish to derive the
 corresponding distributive laws at the type \granin{T} (where \granin{T} is an
 n-ary type constructor). For example, for the \granin{List : Type -> Type} data
 type from the standard library, we can write the expression \granin{push @List}
 which the type checker recognises as a function of type:

 %
\begin{granule}
push @List : forall { a : Type, s : Semiring, r : s } 
           . {1 <= r} => (List a) [r] -> List (a [r])
\end{granule}
 %
 Note this function is not only polymorphic in the grade, but polymorphic in the
 semiring itself. Granule identifies different graded modalities by their
 semirings, and thus this operation is polymorphic in the graded modality. When
 the type checker encounters such a type application, it triggers the derivation
 procedure of Section~\ref{sec:push-pull-deriv}, which also calculates the type. The
 result is then stored in the state of the frontend to be passed to the
 interpreter (or compiler) after type checking. The derived operations are
 memoized so that they need not be re-calculated if a particular distributive
 law is required more than once. Otherwise, the implementation largely follows
 Section~\ref{sec:push-pull-deriv} without surprises, apart from some additional
 machinery for specialising the types of data constructors coming from
 (generalized) ADTs.
 
 \subsection{Examples}
 Earlier, we motivated the crux of the work in this chapter with a concrete
 example, which we can replay here in Granule, using its type
 application technique for triggering the automatic derivation of the
 distributive laws. Previously, we defined \granin{pushPair} by hand which can
 now be replaced with:
 %
\begin{granule}
push @(,) : forall { a b : Type, s : Semiring, r : s } 
          . (a, b) [r] -> (a [r], b [r])
\end{granule}
 %
 Note that in Granule \granin{(,)} is an infix type constructor for product
 types as well as terms. We can then replace the previous \granin{fst'} with:
 %
\begin{granule}
fst' : forall { a b : Type, r : Semiring } 
     . {0 <= r} => (a, b) [r] -> a
fst' = let [x'] = fst (push @(,) x) in x'
\end{granule}
 %
 The point however in the example is that we need not even define this
 intermediate combinator, but can instead write the following
 wherever we need to compute the first projection
 of \granin{myPair : (a, b) [r]}:
 %
\begin{granule}
extract (fst (push @(,) myPair)) : a
\end{granule}
 %
 We already saw that we can then generalise this by applying this first
 projection inside of the list directly using \granin{push @List}:
\begin{granule}
map (extract . fst . push @(,)) (push @List myPairList) : List a
\end{granule}
where \granin{myPairList : (List (a, b)) [r]}.   
 
 In a slightly more elaborate example, we can use the \granin{pull} combinator
 for pairs to implement a function that duplicates a pair (given that both elements
 can be consumed twice):
 %
\begin{granule}
copyPair : forall { a, b : Type } 
         . (a [0..2], b [2..4]) -> ((a, b), (a, b))
-- where, copy : a [2] -> (a, a)
copyPair x = copy (pull @(,) x) 
\end{granule}
 %
 Note \granin{pull} computes the greatest-lower
 bound of intervals \granin{0..2} and \granin{2..4} which is
 \granin{2..2}, i.e., we can provide a pair of \granin{a} and \granin{b}
 values which can each be used exactly twice: exactly what is required
 for \granin{copy}.
 
 As another example, interacting with Granule's indexed types
 (GADTs), consider a simple programming task of taking the head of a sized-list (vector)
 and duplicating it into a pair. The \granin{head} operation
 is typed: 
\begin{granule}
head : forall { a : Type, n : Nat } 
     . (Vec (n + 1) a) [0..1] -> a
\end{granule}
 which has a graded modal input with grade \granin{0..1} meaning
 the input vector is used 0 or 1 times:
 the head element is used once (linearly) for the return
 but the tail is discarded.
 
 This head element can then be copied via a graded
 modality, e.g., a value of type \granin{(Vec (n + 1) (a [2])) [0..1]} permits:
 %
\begin{granule}
copyHead' : forall { a : Type, n : Nat :} 
           . (Vec (n + 1) (a [2])) [0..1] -> (a, a)
-- [y] unboxes (a [2]) to y:a usable twice
copyHead' xs = let [y] = head xs in (y, y) 
\end{granule}
 %
 Here we ``unbox'' the graded modal value of type \granin{a [2]} to
 get a non-linear variable \granin{y} which we can use precisely twice.
 However, what if we are in a programming
 context where we have a value \granin{Vec (n + 1) a} with no
 graded modality on the type \granin{a}? %What requirements can we place
 %on such a value to explain we need to be able to discard the tail of
 %the vector and reuse the head twice?
 We can employ two idioms here:
 (i) take a value of type \granin{(Vec (n + 1) a) [0..2]} and
 split its modality in two: \granin{(Vec (n + 1) a) [2] [0..1]}
 , and then (ii) use \textit{push} on the inner graded modality
 \granin{[2]} to get
 \granin{(Vec (n + 1) (a [2])) [0..1]}.
 
 Using \granin{push @Vec} we can thus write the following to duplicate
 the head element of a vector:
 %
\begin{granule}
copyHead : forall { a : Type, n : Nat } 
         . (Vec (n + 1) a) [0..2] -> (a, a)
copyHead = copy . head . push@(->) [push @Vec] . disject
\end{granule}
 %
 which employs the combinator \granin{disject} from the standard library and
 two derived distributive laws, of type:
 %
\begin{granule}
disject   : forall { a : Type, s : Semiring, n m : s }   
          . a [m * n] -> (a [n]) [m]
push @Vec : forall { a : Type, n : Nat, s : Semiring, r : s } 
          . (Vec n a) [r] -> Vec n (a [r])
push@(->) : forall { a b : Type, s : Semiring, r : s } 
          . (a  -> b) [r] -> a [r] -> b [r]
\end{granule}
 %

\section{Deriving Other Useful Structural Combinators}
\label{sec:deriving-other}
So far we have motivated the use of distributive laws, and demonstrated that
they are useful in practice when programming in languages with linear and graded
modal types. The same methodology we have been discussing can also be used to
derive other useful generic combinators for programming with linear and graded
modal types. In this section, we consider two structural combinators,
\granin{drop} and \granin{copyShape}, in Granule as well as related type classes
for dropping, copying, and moving resources in Linear Haskell.

\subsection{A Combinator for Weakening (``drop'')}
\label{subsec:drop}

First, we consider a combinator for ``dropping,'' or consuming, values in
Granule: 
\begin{align*}
\deriv{drop}{\tcF \alpha} : \tcF \alpha \multimap \tcF \mathsf{Unit}
\end{align*}
The built-in type constants of Granule can be split into those which permit
structural weakening $C^{\mathsf{w}}$ such as \granin{Int}, \granin{Char}, and those which do not $C^{\mathsf{l}}$ such as \granin{Handle}
(file handles) and \granin{Chan} (concurrent channels). 

\begin{figure*}
\begin{align*}
    \deriv{drop}{C^\mathsf{w}}^\Sigma z & = \mathsf{drop}^{C^{\mathsf{w}}}\ z \\
    \deriv{drop}{\mathsf{Unit}}^\Sigma z & = [[ case z of () -> () ]] \\
    \deriv{drop}{X}^\Sigma z & = \Sigma(X) z \\
    \deriv{drop}{A \oplus B}^\Sigma z & =
    \derCaseTwoShort{z}{\mathsf{inl}\ x}{\deriv{drop}{A}(x)}{\mathsf{inr}\ y}
                                {\deriv{drop}{B}(y)}\\
    \deriv{drop}{A \otimes B}^\Sigma z & =
    \derCaseOne{z}{(x, y)}
       {\\ & \hspace{3em} \derCaseOne{\deriv{drop}{A}(x)}{()}
         {\\ & \hspace{5em} \derCaseOne{\deriv{drop}{B}(y)}{()}{()}}}
    \\
    \deriv{drop}{\mu X . A}^\Sigma & z =
                                     \derLetRec{f}{\deriv{drop}{A}^{\Sigma,
                                     X \mapsto f : A \multimap 1}}{f\ z}
    \end{align*}
    \caption{Interpretation rules for $\deriv{drop}{A}$}
    \label{fig:drop}
\end{figure*}
Those that permit weakening contain non-abstract values that can in theory be
systematically inspected in order to consume them. Granule provides a built-in
implementation of \granin{drop} for $C^{\mathsf{w}}$ types, which is then used
by the derivation procedure of Figure~\ref{fig:drop} to derive weakening on compound
types. 

Note we cannot use this procedure in a polymorphic context (over type variables
$\alpha$) since type polymorphism ranges over all types, including those which
cannot be dropped like $C^{\mathsf{l}}$.

%\dnote{An interesting thing is that a function $[[ A -o B ]]$ is
%  droppable if you can 'pick' an element $x \in [[ A ]]$ (i.e., you
%  have a canonical way of generating one) and then that $[[ B ]]$ is
%  droppable. Not sure whether we can/should get into this here though?}

\subsection{A Combinator for Copying ``shape''}

The ``shape'' of values for a parametric data types $\tcF$ can be determined by
a function $\mathit{shape} : \tcF A \rightarrow \tcF \mathsf{Unit}$, usually
derived when $\tcF$ is a functor by lifting a function $A \rightarrow
\mathsf{Unit}$ (dropping elements)~\citep{jay1994shapely}. This provides a way
of capturing the size, shape, and form of a data structure. Often when
programming with data structures which must be used linearly, we may wish to
reason about properties of the data structure (such as the length or ``shape''
of the structure) but we may not be able to drop the contained values. Instead,
we wish to extract the shape but without consuming the original data structure
itself.

This can be accomplished with a function which copies the data structure
exactly, returning this duplicate along with a data structure of the same shape,
but with the terminal nodes replaced with values of the unit type
$\mathsf{Unit}$  (the `spine'). For example, consider a pair of integers:
\granin{(1, 2)}. Then applying \granin{copyShape} to this pair would yield
\granin{(((), ()), (1, 2))}. The original input pair is duplicated and returned
on the right of the pair, while the left value contains a pair with the same
structure as the input, but with values replaced with \granin{()}. This is
useful, as it allows us to use the left value of the resulting pair to reason
about the structure of the input (e.g., its depth / size), while preserving the
original input. This is particularly useful for deriving size and length
combinators for collection-like data structures.
\noindent
As with ``drop'', we can derive such a
function automatically:
\begin{align*}
\deriv{copyShape}{\tcF \alpha} : \tcF \alpha \multimap \tcF \mathsf{Unit} \otimes \tcF \alpha
\end{align*}
defined by
$\deriv{copyShape}{A} = \lambda z . \deriv{copyShape}{A}^\emptyset z$
by an intermediate interpretation $\deriv{copyShape}{A}^\Sigma$, given by Figure~\ref{fig:copyShape}.
%
\begin{figure}
\begin{align*}
\deriv{copyShape}{C^w}^\Sigma z & = ((),\ z) \\
\deriv{copyShape}{\mathsf{Unit}}^\Sigma z & = \derCaseOne{z}{()}{((),\ ())}\\
\deriv{copyShape}{\alpha}^\Sigma z & = ((), z) \\
\deriv{copyShape}{X}^\Sigma z & = \Sigma(X) z \\
\deriv{copyShape}{A \oplus B}^\Sigma z & =
          \textbf{case}\ z\ \textbf{of} \\ 
              & \hspace{3em} \mathsf{inl}\ x \rightarrow 
              \textbf{case}\ {\llbracket A \rrbracket}^\Sigma_\mathsf{copyShape}(x)\ \textbf{of}\ (s,\ x') \rightarrow \\ 
              & \hspace{9em} (\mathsf{inl}\ s,\ \mathsf{inl}\ x') \\ 
              & \hspace{3em} \mathsf{inr}\ y \rightarrow  
              \textbf{case}\ {\llbracket B \rrbracket}^\Sigma_\mathsf{copyShape}(y)\ \textbf{of}\ (s,\ y') \rightarrow \\ 
              & \hspace{9em} (\mathsf{inr}\ s,\ \mathsf{inr}\ y') \\ 
            %                              \derCaseTwoMultiLine{z}
            %  {\mathsf{inl}\ x}
            %     {\textbf{case}\ {\llbracket A \rrbracket}^\Sigma_\mathsf{copyShape}(x)\ \textbf{of}\ (s,\ x') \rightarrow (\mathsf{inl}\ s,\ \mathsf{inr}\ x')} 
            %  {\mathsf{inr}\ y}
            %     {\textbf{case}\ {\llbracket B \rrbracket}^\Sigma_\mathsf{copyShape}(y)\ \textbf{of}\ (s,\ y') \rightarrow (\mathsf{inl}\ s,\ \mathsf{inr}\ y')} \\
\deriv{copyShape}{A \otimes B}^\Sigma z & =
\derCaseOne{z}{(x, y)}
   {     \\ & \hspace{3em} \derCaseOne{\llbracket A \rrbracket^\Sigma_\mathsf{copyShape}(x)}{ ( [[ s ]],\ [[x']] )}{
                                          \\ & \hspace{6em} \derCaseOne{\llbracket B \rrbracket^\Sigma_\mathsf{copyShape}(y)}{ \\ & \hspace{9em} ( [[ s' ]],\ [[y']] )}{
                                          (([[s]],\ [[s']]),\ ([[x']],\ [[y']]))}
                                          }}
\\
\deriv{copyShape}{\mu X . A}^\Sigma & z =
                                 \derLetRec{f}{\deriv{copyShape}{A}^{\Sigma,
                                 X \mapsto f : A \multimap 1 \otimes A}}{f\ z}
\end{align*}
\caption{Interpretation rules for $\deriv{copyShape}{A}$}
\label{fig:copyShape}
\end{figure}
%
The implementation recursively follows the structure of the type,
replicating the constructors, reaching the crucial case where a
polymorphically type $z : \alpha$ is mapped to $( (),\ z )$ in
the third equation.

\subsection{Implementation in Granule}
Granule implements both these derived combinators in a similar way to
\emph{push}/\emph{pull} providing \granin{copyShape} and \granin{drop} which can
be derived for a type \granin{T} via type application, e.g. \granin{drop @T : T
-> ()} if it can be derived. Otherwise, the type checker produces an error,
explaining why \granin{drop} is not derivable at type \granin{T}. As an example
of using \granin{copyShape} in Granule, consider the case where we want to find
the length of a list, without consuming the list itself. Without \granin{copyShape}, 
we would have to write our \granin{length} function so that it returns the input list
after computing its size, resulting in the rather cumbersome implementation:
\begin{granule}
data List = Cons a | Nil 

data N = S N | Z

length : forall { a : Type } . List a -> (N, List a)
length Nil = (Z, Nil);
length (Cons x xs) = 
  let (n, xs') = length xs in (S n, Cons x xs')
\end{granule}
Using \granin{copyShape}, we can rewrite this definition of \granin{length} into
a simpler form which doesn't have to worry about how it consumes its input:
\begin{granule}
length : forall { a : Type } List a -> N
length Nil = Z
length (Cons _ xs) = S (length xs)
\end{granule}
and then to call \granin{length} on a list, we instead pass in the shape of the 
list rather than the list itself:
\begin{granule}
let (shape, myList') = copyShape@List myList in length shape
\end{granule}

\section{Related Work}
\label{sec:deriving-related}
In this section we consider some of the wider related work as they relate to the
ideas presented in this chapter.
\subsection{Generic Programming Methodology}
The deriving mechanism for Granule is based on the methodology of generic
functional programming~\citep{hinze2000new}, where functions may be defined
generically for all possible data types in the language; generic functions are
defined inductively on the structure of the types. This technique has notably
been used before in Haskell, where there has been a strong interest in deriving
type class instances automatically. Particularly relevant to this work is the
work on generic deriving~\citep{generic-deriving}, which allows Haskell
programmers to automatically derive arbitrary class instances using standard
datatype-generic programming techniques as described above. 

\subsection{Non-graded Distributive Laws}
Distributive laws are standard components in abstract
mathematics. Distributive laws between categorical structures used for
modelling modalities (like monads and comonads) are well explored. For
example, \citet{brookes1993intensional} defined a categorical semantics using monads
combined with comonads via a distributive law capturing both
intensional and effectful aspects of a
program.~\citet{power2002combining} study in
detail different ways of combining comonads and monads via
distributive laws. Such distributive laws
have been applied in the programming languages literature, e.g., for
modelling streams of partial elements~\citep{uustalu2006essence}.

\subsection{Graded Distributive Laws}
\citet{DBLP:conf/icfp/GaboardiKOBU16} define families of graded distributive laws for graded monads
and comonads. They include the ability to interact the
grades, e.g., with operations such as $\Box_{\iota(r,f)} \Diamond_f A
\rightarrow \Diamond_{\kappa(r,f)} \Box_r A$ between a graded comonad $\Box_r$
and graded monad $\Diamond_f$ where $\iota$ and $\kappa$ capture information
about the distributive law in the grades. In comparison, our distributive laws
here are more prosaic since they involve only a graded comonad (semiring graded
necessity) distributed over a functor and vice versa. That said, the scheme of
Gaboardi et al. suggests that there might be interesting graded distributive
laws between $\Box_r$ and the indexed types, for example, $\Box_r
(\mathsf{Vec}\, n \, A) \rightarrow \mathsf{Vec}\, (r * n) \, (\Box_1 A)$ which
internally replicates a vector. However, it is less clear how useful such
combinators would be in general or how systematic their construction would be.
In contrast, the distributive laws explained here appear frequently and have a
straightforward uniform calculation.

We noted in Section~\ref{sec:push-pull-deriv} that neither of our distributive laws
can be derived over graded modalities themselves, i.e., we cannot derive
$\textit{push} : \Box_r \Box_s A \rightarrow \Box_s \Box_r A$. Such an operation
would itself be a distributive law between two graded modalities, which may have
further semantic and analysis consequences beyond the normal derivations here
for regular types. Exploring this is future work, for which the previous work on
graded distributive laws can provide a useful scheme for considering the
possibilities here. Furthermore, Granule has both graded comonads and graded
monads so there is scope for exploring possible graded distributive laws between
these in the future following~\citet{DBLP:conf/icfp/GaboardiKOBU16}.

\subsection{Typed Analysis of Consumption in Pattern Matching}
\label{subsec:matching-and-consumption}

\newcommand{\abname}{$\Lambda^p$}
\newcommand{\gradname}{\textsc{GraD}}

This chapter's study of distributive laws provides
an opportunity to consider design decisions for the \emph{typed
  analysis of pattern matching} since the operations of
Section~\ref{sec:push-pull-deriv} are derived by pattern matching in concert
with grading. We compare here the choices made surrounding the typing of
pattern matching in four works (1) Granule and
its core calculus~\citep{DBLP:journals/pacmpl/OrchardLE19} (2) the graded
modal calculus \abname{} of~\citet{DBLP:journals/pacmpl/AbelB20} (3) the dependent graded system
\gradname{} of~\citet{DBLP:journals/pacmpl/ChoudhuryEEW21} and (4) Linear
Haskell~\citep{DBLP:journals/pacmpl/BernardyBNJS18}.

\paragraph{Granule}
%
Pattern matching against a graded modality $\Box_r A$ (with pattern
$[p]$) in Granule is provided by the \textsc{Pbox} rule
(Figure~\ref{fig:deriving-pattern-rules}) which triggers typing pattern $p$
`under' a grade $r$ at type $A$. This was denoted via the optional grade information
$[[ r |- p : A |> G ]]$ which then pushes grading down onto the
variables bound within $p$. Furthermore, it is only under such a
pattern that wildcard patterns are allowed (\textsc{[Pwild]}),
requiring $[[ 0 <<= r]]$, i.e., $[[ r ]]$ can approximate
$0$ (where $0$ denotes weakening).
None of the other systems considered here have such a facility
for weakening via pattern matching.

For a type $A$ with more than one
  constructor ($[[ | A | > 1 ]]$), pattern matching its
  constructors underneath an $r$-graded box requires $[[ 1 <<=
  r]]$. For example, eliminating sums inside an $[[r]]$-graded box
$[[ [] r (Sum A B) ]]$ requires $[[ 1 <<= r ]]$ as
distinguishing $\mathsf{inl}$ or $\mathsf{inr}$
constitutes a \emph{consumption} which reveals information (i.e.,
pattern matching on the `tag' of the data constructors).
By contrast, a type with only one constructor cannot convey any information
by its constructor and so matching on it is not counted as a consumption:
eliminating $[[ [] r (Tup A B) ]]$ places no
requirements on $r$.
The idea that unary data types do not incur consumption (since no
information is conveyed by its constructor) is a refinement here to the
original Granule paper as described by~\citet{DBLP:journals/pacmpl/OrchardLE19}, which for \textsc{[Pcon]}
had only the premise $[[ 1 <<= r]]$ rather than
$[[ | A | > 1 ]] \implies [[ 1 <<= r ]]$ here (although the
implementation already reflected this idea).

\paragraph{The \abname{} calculus} Abel and Bernardy's unified modal system
\abname{} is akin to Granule, but with pervasive grading (rather than base
linearity) akin to the coeffect calculus~\citep{petricek2014coeffects} and Linear
Haskell~\citep{DBLP:journals/pacmpl/BernardyBNJS18}. Similarly to the situation
in Granule, \abname{} also places a grading requirement when pattern matching on
a sum type, given by the following typing rule in their syntax~\citep[Fig 1,
p.4]{DBLP:journals/pacmpl/AbelB20}:
%
\begin{align*}
\dfrac{\gamma \Gamma \vdash t : A_1 + A_2 \qquad \delta\Gamma, x_i :^q
  A_i \vdash u_i : C \qquad q \leq 1}
      {(q\gamma + \delta)\Gamma \vdash \mathsf{case}^q\ t\ \mathsf{of}\
  \{\mathsf{inj}_1 x_1 \mapsto u_1; \mathsf{inj}_2 x_2 \mapsto u_2 \}
  : C}\textsc{$+$-elim}
\end{align*}
%
The key aspects here are that variables $x_i$ bound in the case are used
with grade $q$ as denoted by the graded assumption $x_i :^q A_i$ in the context of
typing $u_i$ and then that $q \leq 1$ which is exactly our
constraint that $[[ 1 <<= r ]]$ (their ordering just runs in the
opposite direction to ours). For the elimination of pair and unit
types in \abname{} there is no such constraint, matching our idea that arity
affects usage, captured in Granule by $[[ | A | > 1 ]] \implies [[ 1
<<= r ]]$. Their typed-analysis of patterns is motivated
by their parametricity theorems.

\paragraph{\gradname{}}
The dependent graded type system \gradname{} of Choudhury et al. also considers
the question of how to give the typing of pattern matching on sum
types, with a rule in their system~\cite[p.8]{DBLP:journals/pacmpl/ChoudhuryEEW21} which
closely resembles the \textsc{$+$-elim} rule for \abname{}:
%
\begin{align*}
  {\footnotesize{
\dfrac{
  \Delta ; \Gamma_1 \vdash q : A_1 \oplus A_2
  \qquad
  \Delta ; \Gamma_2 \vdash b_1 : A_1 \xrightarrow{q} B
  \qquad
  \Delta ; \Gamma_2 \vdash b_2 : A_2 \xrightarrow{q} B
  \qquad
  1 \leq q
}
{\Delta ; q \cdot \Gamma_1 + \Gamma_2 \vdash \mathbf{case}_q\ a\
  \mathbf{of}\ b_1; b_2 : B}
\textsc{STcase}
  }}
\end{align*}
%
The direction of the preordering in \gradname{} is the same as that in Granule
but, modulo this ordering and some slight restructuring, the case rule captures
the same idea as \abname{}: ``both branches of the base analysis \emph{must} use
the scrutinee at least once, as indicated by the $1 \leq q$
constraint.''~\citep[p.8]{DBLP:journals/pacmpl/ChoudhuryEEW21}. Choudhury et al., also provide a
heap-based semantics which serves to connect the meaning of grades with a
concrete operational model of usage, which then motivates the grading on sum
elimination here. % Choudhury et
In the simply-typed version of \gradname{}, matching on the components
of a product requires that each component is consumed linearly.


\paragraph{Linear Haskell}
The paper on Linear Haskell by~\citet{DBLP:journals/pacmpl/BernardyBNJS18} has a \textbf{case}
expression for eliminating arbitrary data constructors, with grading similar to
the rules seen above. Initially, this rule is for the setting of a semiring over
$\mathcal{R} = \{1, \omega\}$ and has no requirements on the grading to
represent the notion of inspection, consumption, or usage due to matching on
(multiple) constructors. This is reflected in the current implementation where
we can define the following sum elimination:
%
\begin{haskell}
match :: (Either a b) %r -> (a %1 -> c) -> (b %1 -> c) -> c
match (Left x) f _  = f x
match (Right x) _ g = g x
\end{haskell}
%
However, later when considering the generalisation to other semirings they state
that ``\emph{typing rules are mostly unchanged with the caveat that
$\mathsf{case}_\pi$ must exclude $\pi = 0$}''~\citep{DBLP:journals/pacmpl/BernardyBNJS18} where $\pi$ is the grade of the $\textbf{case}$ guard.
This appears a more coarse-grained restriction than the other three systems,
excluding even the possibility of Granule's weakening wildcard pattern which
requires $0 \leq \pi$. Currently, such a pattern must be marked as
\haskin{'Many} in Linear Haskell (i.e., it cannot explain that first projection
on a pair does not use the pair's second component). Furthermore, the condition
$\pi \neq 0$ does not require that $\pi$ actually represents a consumption,
unlike the approaches of the other three systems.
%
%Note that this multiplicity polymorphic (variable \haskin{r}), leading to the
%question of whether $\texttt{r} = 0$ is a possible unification. The rest of the
%literature as described above would all prevent $\texttt{r} = 0$ here, requiring
%instead $\texttt{r} = 1$ or $\texttt{r} = \omega$, and this is briefly touched
%on by Bernardy et al.~\cite{linear-haskell} as well, where the authors note that
%if a multiplicity of $0$ were allowed, pattern match statements would need to be
%restricted.
The argument put forward by Abel and Bernardy for their restriction to mark a
consumption ($q \leq 1$) for the sake of parametricity is a compelling one, and
the concrete model of Choudhury et al. gives further confidence that this
restriction captures well an operational model. Thus, it seems there is a chance
for fertilisation between the works mentioned here and Linear Haskell's vital
work, towards a future where grading is a key tool in the typed-functional
programmer's toolbox.


\section{Conclusion}
\label{sec:der-conclusion}

The work described here addresses the practical aspects of applying these
techniques in real-world programming. Our hope is that this aids the development
of the next generation of programming languages with rich type systems for
high-assurance programming.

The calculus of this chapter serves somewhat as a bridge between the systems in
Chapters~\ref{chapter:core} and~\ref{chapter:extended}. We build upon the graded
linear $\lambda$-calculus introduced in Section~\ref{sec:linear-base} and the
extended types introduced in~\ref{chapter:core}, introducing pattern matching
and recursion. This allowed us to write and subsequently automatically derive
programs which express the \emph{push} behaviour that we saw in
Section~\ref{sec:linear-base-conclusion} of Chapter~\ref{chapter:core}. 

In the next chapter, we take these iterations and apply them to the fully graded
``graded base'' calculus of Section~\ref{sec:graded-base}. We return to the
type-directed synthesis approach of~\ref{chapter:core}, designing a synthesis
calculus for a fully graded type system using the additive resource management
scheme. This system, however, is significantly more expressive than that of
Chapter~\ref{chapter:core}, incorporating ADTs, pattern matching, recursion, and  
polymorphism. 