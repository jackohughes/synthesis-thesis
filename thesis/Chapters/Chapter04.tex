\chapter{Automatically Deriving Graded Combinators}         
\label{chapter:deriving}
Thus far we have considered program synthesis from the perspective of
enumerative search, using our types to guide us and pruning the space of
programs where possible. This approach yielded a synthesis tool which was highly
expressive, allowing the synthesis of a program term for each syntactic form in
our core calculus. In this chapter we present an alternative approach, which
targets a specific class of graded programs: graded \emph{distributive} and
structural combinators. We view this approach as a useful complement to the more
powerful type-directed synthesis. Much of the content of this chapter is derived
from~\citet{DBLP:journals/corr/abs-2112-14966}, our Linearity/TLLA 2020 paper. 

When programming with graded modal types, we have observed there is often a need
to `distribute' a graded modality over a type, and vice versa, in order to
compose programs. That is, we may find ourselves in possession of a $\Box_r
(\mathsf{F} \alpha)$ value (for some parametric data type $\mathsf{F}$) which
needs to be passed to a pre-existing function (of our own codebase or a library)
which requires a $\mathsf{F} (\Box_r \alpha)$ value, or perhaps vice versa. A
\emph{distributive law} (in the categorical sense,
due to~\citet{street1972formal}) provides a conversion from one to the other. In
this chapter, we present a procedure to automatically synthesise these
distributive operators, applying a generic programming
methodology~\citep{hinze2000new} to compute these operations given the base type
(e.g., $\mathsf{F} \alpha$ in the above description). This serves to ease the
use of graded modal types in practice, removing boilerplate code by
automatically generating these `interfacing functions' on-demand, for
user-defined data types as well as built-in types.

Throughout, we refer to distributive laws of the form $\Box_r (\mathsf{F}
\alpha) \rightarrow \mathsf{F} (\Box_r \alpha)$ as \emph{push} operations (as
they `push' the graded modality inside the type constructor $\mathsf{F}$), and
dually $\mathsf{F} (\Box_r \alpha) \rightarrow \Box_r (\mathsf{F} \alpha)$ as
\emph{pull} operations (as they `pull' the graded modality outside $\mathsf{F}$).

As a standalone methodology for generating a common class of graded programs,
this ``deriving mechanism'' serves as a complement to the synthesis calculi of
Chapter~\ref{chapter:core}. Synthesis problems which exhibit this distributive
behaviour pose issues in the existing calculi. However, in many cases the
solution programs to these distributive problems are straightforwardly derivable
from the type alone, making the costly enumerative search of type-directed synthesis
unnecessary.

Thus, we present a tool for an automatic procedure which calculates distributive
laws from graded types and present a formal analysis of their properties. This
approach is realised in Granule, embedded into the compiler. In doing so, we 
extend our graded linear $\lambda$-calculus of Section
~\ref{sec:linear-base} to incorporate data constructors and pattern matching, as well 
as recursive data types. 

This extended calculus is defined in Section~\ref{sec:der-calculus}
providing an idealised, simply-typed subset of Granule with which we develop the
core deriving mechanism. Section~\ref{sec:push-pull-deriv} gives the procedures for
deriving \emph{push} and \emph{pull} operators for the calculus.
Section~\ref{sec:implementation-der} describes the details of how these procedures
are realised in the Granule language. We then provide examples of how several
other structural combinators in Granule may be derived using this tool in
Section~\ref{sec:deriving-other}. Finally, Section~\ref{sec:der-conclusion}
discusses more related and future work.

We start with a motivating example typifying the kind of software engineering
impedance problem that distributive laws solve. We do so in Granule code since
it is the main vehicle for the developments here.

\subsection{Motivating Example}
\label{sec:motivating-example}

Consider the situation of projecting the first element of a pair. In Granule,
this first-projection is defined and typed as the following polymorphic function
(whose syntax is reminiscent of Haskell or ML):
%
\begin{granule}
fst : forall { a b : Type } . (a, b [0]) -> a
fst (x, [y]) = x
\end{granule}
%
Linearity is the default, so this represents a linear function applied to linear
values. However, the second component of the pair has a \emph{graded modal
type}, written \granin{b [0]}, which means that we can use the value ``inside''
the graded modality $0$ times by first `unboxing' this capability via the
pattern match \granin{[y]} which allows weakening to be applied in the body to
discard \granin{y} of type \granin{b}. In calculus of
Section~\ref{sec:der-calculus}, we denote `\granin{b [0]}' as the type
$\Box_0 b$.

The type for \granin{fst} is however somewhat restrictive: what if we are trying
to use such a function with a value (call it \granin{myPair}) whose type is not
of the form \granin{(a, b [0])} but rather \granin{(a, b) [r]} for some grading
term \granin{r} which permits weakening? Such a situation readily arises when we
are composing functional code, say between libraries or between a library and
user code. In this situation, \granin{fst myPair} is ill-typed. Instead, we
could define a different first projection function for use with \granin{myPair :
(a, b) [r]} as:
%
\begin{granule}
fst' : forall { a b : Type, s : Semiring, r : s } 
     . {0 <= r} => (a, b) [r] -> a
fst' [(x, y)] = x
\end{granule}
%
This implementation uses various language features of Granule to make it as
general as possible. Firstly, the function is polymorphic in the grade
\granin{r} and in the semiring \granin{s} of which \granin{r} is an element.
Next, a refinement constraint \granin{0 <= r} specifies that by the pre-ordering
\granin{<=} associated with the semiring \granin{s}, that \granin{0} is
approximated by \granin{r} (essentially, that \granin{r} permits weakening). The
rest of the type and implementation looks more familiar for computing a first
projection, but now the graded modality is over the entire pair.

From a software engineering perspective, it is cumbersome to create alternate
versions of generic combinators every time we are in a slightly different
situation with regards the position of a graded modality.  Fortunately, this is
an example to which a general \emph{distributive law} can be deployed. In this
case, we could define the following distributive law of graded modalities over
products, call it \granin{pushPair}:
%
\begin{granule}
pushPair : forall { a b : Type, s : Semiring, r : s } 
         . (a, b) [r] -> (a [r], b [r])
pushPair [(x, y)] = ([x], [y])
\end{granule}
%
This `pushes' the graded modality \granin{r} into the pair (via pattern matching
on the modality and the pair inside it, and then reintroducing the modality on
the right hand side via \granin{[x]} and \granin{[y]}), distributing the graded
modality to each component. Given this combinator, we can now apply \granin{fst
(pushPair myPair)} to yield a value of type \granin{a [r]}, on which we can
apply the Granule standard library function \granin{extract}:
\begin{granule}
extract : forall { a : Type, s : Semiring, r : s }
        . {(1 : s) <= r} => a [r] -> a
extract [x] = x
\end{granule}
 to get the original \granin{a} value we desired:
%
\begin{granule}
extract (fst (pushPair myPair)) : a
\end{granule}
%
The \granin{pushPair} function could be provided by the standard
library, and thus we have not had to write any specialised combinators
ourselves: we have applied supplied combinators to solve the problem.

Now imagine we have introduced some custom data type \granin{List}
on which we have a \emph{map} function:
%
\begin{granule}
data List a = Cons a (List a) | Nil

map : forall { a b : Type } . (a -> b) [0..Inf] -> List a -> List b
map [f] Nil = Nil;
map [f] (Cons x xs) = Cons (f x) (map [f] xs)
\end{granule}
%
Note that, via a graded modality, the type of \granin{map} specifies that the
parameter function, of type \granin{a -> b} is non-linear, used between $0$ and
$\infty$ times. Imagine now we have a value \granin{myPairList : (List (a, b))
[r]} and we want to map first projection over it. But \granin{fst} expects
\granin{(a, b [0])} and even with \granin{pushPair} we require \granin{(a, b)
[r]}. \emph{We need another distributive law}, this time of the graded modality
over the \granin{List} data type. Since \granin{List} was user-defined, we now
have to roll our own \granin{pushList} operation, and so we are back to having
to make specialised combinators for our data types.

The crux of this chapter is that such distributive laws can be automatically
calculated given the definition of a type. With our Granule implementation of
this approach (Section~\ref{sec:implementation-der}), we can then solve this
combination problem via the following composition of combinators:
%
\begin{granule}
map (extract . fst . push @(,)) (push @List myPairList) : List a
\end{granule}
%
where the \granin{push} operations are written with their base type via
\granin{@} (a type application) and whose definitions and types are
automatically generated during type checking. Thus the \granin{push} operation
is a \textit{data-type generic function}~\citep{hinze2000new}. This generic
function is defined inductively over the structure of types, thus a programmer
can introduce a new user-defined algebraic data type and have the implementation
of the generic distributive law derived automatically. This reduces both the
initial and future effort (e.g., if an ADT definition changes or new ADTs are
introduced).

Dual to the above, there are situations where a programmer may wish to
\emph{pull} a graded modality out of a structure. This is possible with a dual
distributive law, which could be written by hand as:
%
\begin{granule}
pullPair : forall { a b : Type, s : Semiring, m n : s } 
         . (a [n], b [m]) -> (a, b) [n /\ m]
pullPair ([x], [y]) = [(x, y)]
\end{granule}
%
Note that the resulting grade is defined by the greatest-lower bound (meet) of
\granin{n} and \granin{m}, if it exists as defined by a pre-order for semiring
\granin{s} (that is, $\sqcap$ is not a total operation). This allows some
flexibility in the use of the \emph{pull} operation when grades differ in
different components but have a greatest-lower bound which can be `pulled out'.
Our approach also allows such operations to be generically derived.

% Another potential example here?
% \granin{copyFst : forall {a, b : Type} . (a, b) [0..2] -> (a, a)}
% \granin{copyFst x = let x' = pushPair x in copy (fst x')}
%
% \granin{copyPair : forall {a, b : Type} . (a [0..2], b [0..2]) -> (a, b)[0..2]} }
% \granin{copyPair x = copy (pullPair x)}

%Our approach here is reminiscent ``push'' and ``pull'' operations are defined
%throughout the standard library for sums, products, and other ADTs such as
%lists. There is a general pattern to these distributive operations which allows
%their programs to be automatically synthesised from their type structure alone
%by a simple inductive procedure.


\section{Extending the Graded Linear-$\lambda$-Calculus}
\label{sec:der-calculus}
We define here a typing calculus which extends the graded linear
$\lambda$-calculus of ~\ref{sec:linear-base} with data constructors, pattern
matching, and recursive data types. This language constitutes a simplified
monomorphic subset of Granule. We include notions of data constructors and their
elimination via \textbf{case} expressions as a way to unify the handling of
regular type constructors.

The full syntax of terms and types is given by:
\begin{align*}
   \hspace{-0.9em}
   [[ t ]] ::= & \;
   [[ x ]]
   \mid \hspace{0.2em}[[ t1 t2 ]]
   \hspace{0.2em}\mid \hspace{0.2em}[[ \ x . t ]] 
   \hspace{0.2em}\mid \hspace{0.2em}[[ [ t ] ]] 
   \hspace{0.2em}\mid \hspace{0.2em}[[ Con t0 ... tn ]] \\
   \hspace{0.2em}\mid & \hspace{0.2em}[[ case t of p1 -> t1 ; * ; pn -> tn  ]] 
   \hspace{0.2em}\mid \hspace{0.2em}[[ letrec x = t1 in t2]]
 {\small{\tag{terms}}}
\end{align*}
\begin{align*}
   \hspace{-0.9em}
   [[ p ]] ::= & \;
   \hspace{0.2em} [[ x ]]
   \hspace{0.2em} \mid \hspace{0.2em} \_
   \hspace{0.2em }\mid \hspace{0.2em} [[ [ p ] ]]
   \hspace{0.2em} \mid \hspace{0.2em} [[ Con p0 ... pn ]]
{\small{\tag{patterns}}}
\end{align*}
\begin{align*}
   \hspace{-0.9em}
   [[ A ]], [[ B ]] ::= & \;
   \hspace{0.2em}[[ A -o B ]]
   \hspace{0.2em}\mid \hspace{0.2em} \alpha
   \hspace{0.2em}\mid \hspace{0.2em} [[ Tup A B ]]
   \hspace{0.2em}\mid \hspace{0.2em} [[ Sum A B ]]
   \hspace{0.2em}\mid \hspace{0.2em} [[ Unit ]]
   \hspace{0.2em} \mid \hspace{0.2em} [[ [] r A ]]
 %  \hspace{0.2em} \mid \hspace{0.2em} [[ A B ]]
 %  \hspace{0.2em} \mid \hspace{0.2em} D
   \hspace{0.2em}\mid \hspace{0.2em} [[ mu X . A ]]
   \hspace{0.2em}\mid \hspace{0.2em} [[ X ]]
 {\small{\tag{types}}} \\
 \hspace{-0.9em}
   [[ C ]] ::= & \; () \mid \mathsf{inl} \mid \mathsf{inr}
                 \mid (,)
 {\small{\tag{data constructors}}}
 \end{align*}
%
For the most part, typing follows the calculus defined in
section~\ref{sec:linear-base-calculus}. Figure~\ref{fig:deriving-typing-rules} gives the additional rules.
We briefly explain the extensions introduced for this chapter.

\begin{figure}[H]
    \begin{align*}
      \begin{array}{c}
    \inferrule*[right=Letrec]{[[G, x : A |- t1 : A ]] \\ [[G', x : A  |- t2 : B]] }
    { [[ G + G' |- letrec x = t1 in t2 : B  ]]}
\\[1.25em]
    \inferrule*[right=Con]{([[ C ]] : B_1 \multimap ... \multimap B_n \multimap A) \in \textsc{D}}
              { \emptyset\ \vdash\ [[C]] : B_1 \multimap ... \multimap B_n \multimap A}
\\[1.25em]
\inferrule*[right=Case]
  { \Gamma \vdash t : A \quad \cdot \vdash p_i : A \triangleright \Delta_i \quad \Gamma', \Delta_i \vdash t_i : B}
  {[[ G  + G' |- case t of p1 -> t1 ; * ; pn -> tn : B ]]}
    \end{array}
 \end{align*}
\vspace{-0.8em}
 \caption{Typing rules for the extended graded linear $\lambda$-calculus}
 \label{fig:deriving-typing-rules}
\end{figure}

The \textsc{LetRec} rule provides recursive bindings in the standard
way. 

Data constructors with zero or more arguments are introduced via
the \textsc{Con} rule. Here, the constructors that concern us are
units, products, and coproducts (sums), given by $[[ Defines ]]$,
a global set of data constructors with their types, defined:
%
\begin{align*}
[[ Defines ]] & = \{() : [[ Unit ]]\}\\
& \cup \{(,) : [[ A -o {B -o Tup A B} ]] \mid \forall [[ A ]], [[ B ]] \} \\ 
& \cup \{\mathsf{inl} : [[ A -o Sum A B ]] \mid \forall [[ A ]], [[ B ]] \} \\
& \cup \{\mathsf{inr} : [[ B -o Sum A B ]] \mid \forall [[ A ]], [[ B ]] \}
\end{align*}
%
Constructors are eliminated by pattern matching via the \textsc{case} rule.
Patterns $p$ are typed by judgments of the form $?r \vdash p : A \triangleright
\Delta$ meaning that a pattern $p$ has type $A$ and produces a context of typed
binders $[[ D ]]$ (used, e.g., in the typing of the case branches). The
information to the left of the turnstile denotes optional grade information
arising from being in an unboxing pattern and is syntactically defined as
either:
%
\begin{align*}
[[ CoeffInfo ]]\ ::= [[ none ]] \mid [[ r ]]
\tag{enclosing grade}
\end{align*}
%
 where $[[ none ]]$ means the present pattern is not nested inside an
unboxing pattern and $[[ r ]]$ that the present pattern is nested
inside an unboxing pattern for a graded modality with grade $[[r]]$.

\begin{figure}[t]
\begin{align*}
\setlength{\arraycolsep}{0em}
\begin{array}{ccc}
\inferrule*[right=Pvar]
 {\quad}
 {\cdot \vdash x : A \triangleright x : A}
\;\;\;
\inferrule*[right=Pcon]
{\cdot \vdash p_i : B_i \triangleright \Gamma_i}
{\cdot \vdash C p_1 .. p_n : A \triangleright \Gamma_1, .., \Gamma_n}
\\[1.25em]
\inferrule*[right={Pbox}]
{[[ r |- p : A |> G ]]}
{\cdot \vdash [p] : \Box_r A \triangleright \Gamma }
 \\[1.25em]
\inferrule*[right={[}Pcon{]}]
{[[ r |- pi : Bi |> Gi ]] \quad\quad
%[[ C ]] : [[ B1 .*. Bn -> A ]] \quad
[[ |A| > 1 ]] \Rightarrow [[ 1 <<= r ]]}
{r \vdash C p_1 .. p_n : A \triangleright \Gamma_1, .., \Gamma_n}
%
\\[1.25em]
\inferrule*[right={[}Pvar{]}]
 {\quad}
 {[[ r |- x : A |> x : [ A ] { r } ]]}
 \;\;\;\;
\inferrule*[right={[}Pwild{]}]
 {[[ 0 <<= r ]]}
 {[[ r |- _ : A |> . ]]}
\end{array}
\end{align*}
\caption{Pattern typing rules for the extended graded linear $\lambda$-calculus}
\label{fig:deriving-pattern-rules}
\end{figure}

The rules of pattern typing are given in
Figure~\ref{fig:deriving-pattern-rules}.
The rule (\textsc{PBox}) provides
graded modal elimination (an `unboxing' pattern),
propagating grade information into the typing of the
sub-pattern. Thus $[[ case t of [p] -> t' ]]$ can be used to eliminate
a graded modal value. Variable patterns are typed via two
rules depending on whether the variable occurs inside an unbox pattern
(\textsc{[Pvar]}) or not (\textsc{Pvar}),
with the \textsc{[Pvar]} rule producing a binding with the grade of
the enclosing box’s grade $[[ r ]]$.
As with variable patterns, constructor patterns are split
between rules for patterns which either occur inside an unboxing
pattern or not. In the former case, the grade information is
propagated to the sub-pattern(s), with the additional constraint that
if there is more than one data constructor for the type $[[ A ]]$ (written
$|A| > 1$), then the grade $r$ must approximate 1 (written $[[ 1 <<= r
]]$) as pattern matching
incurs a usage to inspect the constructor. The
operation $|[[ A ]]|$ counts the number of data constructors
for a type:
%
\begin{align*}
  \begin{array}{c}
|[[ Unit ]]| = 1 \;\;\;\;
|[[ A -o B ]]| = 1 \;\;\;\,
|[[ [] r A ]]| = |[[  A ]]| \\
|[[ Sum A B ]]| = 2 (|A| + |B|) \;\;\;\;
|[[ Tup A B ]]| = |A| |B| \\
|[[ mu X . A ]]| = |A [ [[ mu X . A ]] / X ]|
  \end{array}
\end{align*}
%
and $|[[ X ]]|$ is undefined (or effectively 0) since we do not
allow unguarded recursion variables in types.
A type $A$ must therefore involve a sum type for $|A| > 1$.

Since a wildcard pattern $[[ _ ]]\,$ discards a value, this is only
allowed inside an unboxing pattern where the enclosing grade
permits weakening, captured via $[[ 0 <<= r ]]$ in rule \textsc{[Pwild]}.


\section{Automatically Deriving \emph{push} and \emph{pull}}
\label{sec:push-pull-deriv}
Now that we have established the language, we describe the algorithmic
calculation of distributive laws. Note that whilst our language is simply typed
(monomorphic), it includes type variables (ranged over by $\alpha$) to enable
the distributive laws to be derived on parametric types. In the implementation,
these will really be polymorphic type variables, but the derivation procedure
need only treat them as some additional syntactic type construct.

%
\subsection{Notation}
Let $\tcF : \mathsf{Type}^n \rightarrow \mathsf{Type}$
be an $n$-ary type constructor (i.e. a constructor which takes $n$ type arguments), whose free type variables
provide the $n$ parameter types. We write $\tcF \overline{\alpha_i}$ for
the application of $\tcF$ to
type variables $\alpha_i$ for all $1 \leq i \leq n$.

%\dnote{Split and do one at time? Might be better.}
%\paragrapho{Pull}
%
\subsection{Push}
We automatically
calculate \emph{push} for $\mathsf{F}$
applied to $n$ type variables
% $\{\alpha_i\}^n_i$
$\overline{\alpha_i}$
as the operation:
%
\begin{align*}
\deriv{push}{\tcF \overline{\alpha_i}} : \Box_{r} \tcF \overline{\alpha_i}
  \multimap  \tcF  (\overline{\Box_{r} \alpha_i})
 \end{align*}
where we require $[[ 1 <<= r ]]\ \textit{if}\ |\tcF \overline{\alpha_i}| > 1$
due to the $[\textsc{Pcon}]$ rule (e.g., if $\tcF$ contains a sum).

%and $\deriv{pull}{A} = \lambda z. \deriv{pull}{A}^\emptyset z$
%
%\dnote{put very short descriptions like comments on the right hand side?}
\begin{figure}
\begin{align*}
\hspace{-1em}\deriv{push}{[[ Unit ]]}^\Sigma \ z & = [[ case z of [ () ] -> () ]]
\\
\hspace{-1em}\deriv{push}{\alpha}^\Sigma       \ z & = z
  \\
  \deriv{push}{X}^\Sigma     \ z & = \Sigma(X)\ z
\\
\hspace{-1em}\deriv{push}{A \oplus B}^\Sigma \ z & =
\derCaseTwo{z}{[\mathsf{inl}\ x]}{\mathsf{inl}\ \deriv{push}{A}^{\Sigma}[x]}{[\mathsf{inr}\ y]}
                            {\mathsf{inr}\
                                      \deriv{push}{B}^{\Sigma}[y]}
\\
\hspace{-1em}\deriv{push}{A \otimes B}^\Sigma \ z & =
\derCaseOne{z}{[(x, y)]}
   {(\deriv{push}{A}^{\Sigma}[x], \deriv{push}{B}^{\Sigma}[y])}
% \\
%   \deriv{push}{A \multimap B}^\Sigma \ z & =
%                                 \lambda y .
%                                 \derCaseOne{z}{[f]}{\derCaseOne{\deriv{pull}{A}^{\Sigma}\
%                                     y}{[u]}{\deriv{push}{B}^{\Sigma}[(f
%                                 \ u)]}}
\\
\hspace{-1em}\deriv{push}{A \multimap B}^\Sigma \ z & = \lambda y . \textbf{case}\ z\ \textbf{of}\ [f] \rightarrow \\ 
  & \;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \textbf{case}\ \deriv{pull}{A}^{\Sigma}\ y\ \textbf{of}\ [u] \rightarrow 
  {\deriv{push}{B}^{\Sigma}[(f\ u)]}
\\
%%%%%
\hspace{-1em}\deriv{push}{\mu X . A}^\Sigma \ z & =
 \derLetRec{f}{\deriv{push}{A}^{\Sigma, X \mapsto
f : [[ {mu X . {[] r A}} -o {{(mu X . A)} [ [] r ai /* ai ]} ]] }}{f\ z}
\end{align*}
\caption{Interpretation rules for $\deriv{push}{A}$}
\label{fig:push-interp}
\end{figure}

For types $A$ closed with respect to recursion variables, let $\deriv{push}{A} =
\lambda z . \deriv{push}{A}^\emptyset\ z$ given by an intermediate
interpretation $\deriv{push}{A}^\Sigma$ where $\Sigma$ is a context of
\textit{push} combinators for the recursive type variables. This interpretation
is defined by Figure~\ref{fig:push-interp}.
%
In the case of \emph{push} on a value of type $[[ Unit ]]$, we pattern match on
the value, eliminating the graded modality via the unboxing pattern match and
returning the unit value. For type variables, \emph{push} is simply the identity
of the value, while for recursion variables we lookup the $X$'s binding in
$\Sigma$ and apply it to the value. For sum and product types, \emph{push} works
by pattern matching on the type's constructor(s) and then inductively applying
\emph{push} to the boxed arguments, re-applying them to the constructor(s).
Unlike \emph{pull} below, the \emph{push} operation can be derived for function
types, with a contravariant use of \emph{pull}. For recursive types, we
inductively apply \emph{push} to the value with a fresh recursion variable bound
in $\Sigma$, representing a recursive application of push. There is no
derivation of a distributive law for types which are themselves graded
modalities.

Section~\ref{proof:deriving-type-soundness} in Appendix~\ref{appendix:proofs}
gives the proof that $\deriv{pull}{A}$ is type sound, i.e., its derivations are
well-typed.

\subsection{Pull}
We automatically
calculate \emph{pull} for $\mathsf{F}$
applied to $n$ type variables
% $\{\alpha_i\}^n_i$
$\overline{\alpha_i}$
as the operation:
\begin{align*}
\deriv{pull}{\tcF\ \overline{\alpha_i}} : \tcF\ (\overline{\Box_{r_i} \alpha_i})
\multimap \Box_{\bigwedge^n_{i = 1} r_i} (\tcF\  \overline{\alpha_i})
 \end{align*}
Type constructor $\tcF$ here is applied to $n$ arguments each of the form
$\Box_{r_i} \alpha_i$, i.e., each with a different grading of which the
greatest-lower bound\footnote{The greatest-lower bound $\wedge$ is partial
operation which can be defined in terms of the semiring's pre-order: $r \wedge s
= t$ if $t \sqsubseteq r$, $t \sqsubseteq s$ and there exists no other $t'$
where $t' \sqsubseteq r$ and $t' \sqsubseteq s$ and $t \sqsubseteq t'$.}
$\bigwedge^n_{i = 1} r_i$ is the resulting grade (see \granin{pullPair} from
Section~\ref{sec:motivating-example}).

% (for recursion variables $\Sigma$):
%and $\deriv{pull}{A} = \lambda z. \deriv{pull}{A}^\emptyset z$
\begin{figure*}
\begin{align*}
\hspace{-2em}\deriv{pull}{[[ Unit ]]}^\Sigma            \ z & = [[ case z of () -> [ () ] ]]
\\
\hspace{-2em}\deriv{pull}{\alpha}^\Sigma      \ z & = z
                                       \\
\hspace{-2em}\deriv{pull}{X}^\Sigma           \ z & = \Sigma(X)\ z
                                    \\
\hspace{-2em}\deriv{pull}{A \oplus B}^\Sigma  \ z & =\derCaseTwo{z}{\mathsf{inl}\
                                       x}{\derCaseOne{\deriv{pull}{A}^{\Sigma}\ x}{[u]}{[\mathsf{inl}\ u]}}{\mathsf{inr}\ y}
                            {\derCaseOne{\deriv{pull}{B}^{\Sigma}\
                                       y}{[v]}{[\mathsf{inr}\ v]}}
\\
\hspace{-2em}\deriv{pull}{A \otimes B}^\Sigma \ z & =
\derCaseOne{z}{(x, y)}
   {\\ & \;\;\;\;\;\;\;\;\;\;\; \derCaseOne{(\deriv{pull}{A}^{\Sigma}\ x, \deriv{pull}{B}^{\Sigma}\ y)}
                {([u], [v])}{[(u, v)]}} \\
%%%
\hspace{-2em}\deriv{pull}{\mu X . A}^\Sigma \ z & =
   \derLetRec{f}{\deriv{pull}{A}^{\Sigma, X \mapsto f : [[ {mu X . {A [ [] ri ai /* ai ]}}
                                 -o [] {BigMeet ri} (mu X . A) ]] }}{f\ z}
 \end{align*}
\caption{Interpretation rules for $\deriv{pull}{A}$}
\label{fig:pull-interp}
\end{figure*}

For types $A$ closed with respect to recursion variables, let $\deriv{pull}{A} =
\lambda z . \deriv{pull}{A}^\emptyset\ z$ given by an intermediate
interpretation $\deriv{pull}{A}^\Sigma$ where $\Sigma$ is a context of
\textit{pull} combinators for the recursive type variables. This interpretation
is defined by Figure~\ref{fig:pull-interp}.
%\paragrapho{Push}

%
%
Just like \emph{push}, we cannot apply \emph{pull} to graded modalities
themselves. Unlike \emph{push}, we cannot apply \emph{pull} to function types.
That is, we cannot derive a distributive law of the form $(\Box_r A \multimap
\Box_r B) \multimap \Box_r (A \multimap B)$ since introducing the concluding
$\Box_r$ would require the incoming function $(\Box_r A \multimap \Box_r B)$ to
itself be inside $\Box_r$ due to the promotion rule (\textsc{pr}), which does
not match the type scheme for \emph{pull}.

The rest of the derivation above is similar but dual to that of \emph{push}.

Section~\ref{proof:deriving-type-soundness} in Appendix~\ref{appendix:proofs} gives the proof that $\deriv{pull}{A}$ is type sound,
i.e., its derivations are well-typed.

\begin{example}
  \label{ex:push-fun}
  To illustrate the above procedures, the derivation of
  $\lambda z . \deriv{push}{(\alpha \otimes \alpha) \multimap \beta}\ z : \Box_r ((\alpha \otimes \alpha) \multimap \beta) \multimap
             ((\Box_r \alpha \otimes \Box_r \alpha) \multimap \Box_r \beta) $ is:
  %
{\small{
  \begin{align*}
       & \lambda z . \deriv{push}{(\alpha \otimes \alpha) \multimap \beta}^\emptyset \ z
\\
  = \; & \lambda z . \lambda y . \derCaseOne{z}{[f]}{\derCaseOne{\deriv{pull}{\alpha
      \otimes \alpha}^{\emptyset}\
         y}{[u]}{\deriv{push}{\beta}^{\emptyset}[(f \ u)]}}
    \\
  = \; & \lambda z . \lambda y . \derCaseOne{z}{[f]}{\\ & \hspace{2em} \derCaseOne{(\derCaseOne{y}{(x', y')}
   {\\ & \hspace{5em} \derCaseOne{(\deriv{pull}{\alpha}^{\emptyset}\ x', \deriv{pull}{\alpha}^{\emptyset}\ y')}
                {([u], [v])}{[(u, v)]}})}{\\ & \;\;\;\;\;\;\;\;\;\;\;\;\; [u]}{\deriv{push}{\beta}^{\emptyset}[(f \ u)]}}
%%
    \\
  = \; & \lambda z . \lambda y . \derCaseOne{z}{[f]}{\\ & \hspace{2em} \derCaseOne{(\derCaseOne{y}{(x', y')}
   { \\ & \hspace{5em} \derCaseOne{(x', y')}
                {([u], [v])}{[(u, v)]}})}{[u]}{[(f \ u)]}}
  \end{align*}
}}
  %
\end{example}

\begin{remark}
One might ponder whether linear logic's exponential $!
A$~\citep{girard1987linear} is modelled by the graded necessity modality over
$\mathbb{N}_{\infty}$ intervals, i.e., with $! A \triangleq \Box_{\textcolor{coeffectColor}{[[
  IntervalSyn 0 Inf ]] }} A$. This is a reasonable assumption, but
$\Box_{\textcolor{coeffectColor}{[[ IntervalSyn 0 Inf ]] }} A$ has a slightly different meaning to $! A$,
exposed here: whilst $\deriv{push}{A \otimes B} : \Box_{\textcolor{coeffectColor}{[[
  IntervalSyn 0 Inf ]]}} (A \otimes B) \multimap (\Box_{\textcolor{coeffectColor}{[[ IntervalSyn 0 Inf ]] }} A \otimes \Box_{\textcolor{coeffectColor}{[[ IntervalSyn 0 Inf ]]}} B)$
is derivable in our language, linear logic does not permit $!(A \otimes B) \multimap
(!A \otimes !B)$. Models of $!$ provide only a monoidal functor
structure which gives \emph{pull} for $\otimes$, but not
\emph{push}~\citep{benton1992linear}. This structure can be recovered in Granule through
the introduction of a partial type-level operation which selectively
disallows \emph{push} for $\otimes$ in semirings which model the $!$
modality of linear logic\footnote{The work in ~\citet{hughes:lirmm-03271465} arose as a result of this work.}.
\end{remark}

The algorithmic definitions of `push' and `pull' can be leveraged
in a programming context to automatically yield these combinators for
practical purposes. We discuss how this is leveraged inside the
Granule compiler in Section~\ref{sec:implementation-der}.
Before that, we study the algebraic behaviour of the derived distributive laws.

\subsection{Properties}
\label{subsection:properties}

%We consider here some properties of the distributive laws: that they
%are mutually inverse (under some restriction) and that they behave
%well with respect to structure associated with the graded modalities,
%specifically that they yield distributive laws of endofunctors over
%graded comonads. Proofs are given in Appendix~\ref{app:properties}.

We consider here the properties of these derived operations.  Prima
facie, the above \emph{push} and \emph{pull} operations are simply
distributive laws between two (parametric) type constructors $\tcF$
and $\Box_r$, the latter being the graded modality. However, both
$\tcF$ and $\Box_r$ have additional structure. If the mathematical
terminology of `distributive laws' is warranted, then such additional structure
should be preserved by \emph{push} and \emph{pull} (e.g., as in how
a distributive law between a monad and a comonad must preserve
the behaviour of the monad and comonad operations after applying
the distributive law~\citep{power2002combining}); we explain here the relevant
additional structure and verify the distributive law properties.

We note that these distributive laws are mutually inverse:

\begin{restatable}[Pull is right inverse to push]{prop}{pushPullInverse}
  For all $n$-arity types $\tcF$ which do not contain function types,
  then for type variables $(\alpha_i)_{i \in [1..n]}$
  and for all grades $r \in \mathcal{R}$ where $[[1 <<= r]]$ if $|\tcF
    \overline{\alpha_i}| > 1$, then:
    %
  \begin{align*}
\deriv{pull}{\tcF\
  \overline{\alpha_i}}(\deriv{push}{\tcF\
  \overline{\alpha_i}}) = id\ : \Box_{r} \tcF \overline{\alpha_i}
\multimap \Box_{r} \tcF \overline{\alpha_i}
    \end{align*}
\end{restatable}

\begin{restatable}[Pull is left inverse to push]{prop}{pullPushInverse}
  For all $n$-arity types $\tcF$ which do not contain function types,
  then for type variables $(\alpha_i)_{i \in [1..n]}$
  and for all grades $r \in \mathcal{R}$ where $[[1 <<= r]]$ if $|\tcF
    \overline{\alpha_i}| > 1$, then:
  \begin{align*}
\deriv{push}{\tcF\
  \overline{\alpha_i}}(\deriv{pull}{\tcF\
  \overline{\alpha_i}}) = id\ : \tcF (\Box_{r} \overline{\alpha_i})
\multimap \tcF (\Box_{r} \overline{\alpha_i})
    \end{align*}
  \end{restatable}

\noindent
Section~\ref{proof:inverse-proofs} of Appendix~\ref{appendix:proofs} gives the
proofs, leveraging an equational theory for our lanugage.

Additional properties of these distributive laws can be found in
\citet{DBLP:journals/corr/abs-2112-14966}. These include the naturality of
\emph{push} and \emph{pull} (in a categorical sense), and their preservation of
graded comonadic structure. We choose to omit this properties here both for
brevity. 

% Applying a mathematical perspective, $\Box_r$ is also an endofunctor with
% its object mapping provided by the type constructor itself and its
% morphism mapping behaviour defined as follows:
% %
% \begin{definition}[$\Box_r$ functor]
%   Given a function $f : \alpha \multimap \beta$ (a closed function
%   term) then $\Box_r f : \Box_r \alpha \multimap \Box_r \beta$ is the
%   morphism mapping of the endofunctor $\Box_r$ defined:
%   %
%   \begin{align*}
%     \Box_r\ f = \lambda x. [[ case x of [y] -> [f y] ]]
%   \end{align*}
%   %
% \end{definition}
% %
% \noindent
% For types $\tcF \alpha$ we can also automatically derive the
% morphism mapping of a covariant functor, which we write as $\deriv{fmap}{\tcF \alpha}$
% whose definition is standard (e.g., applied in Haskell~\cite{generic-deriving})  given in
% the appendix~\cite{appendix}. Distributive laws between
% endofunctors should be natural transformations, which is indeed the case for our
% derivations:

% \begin{restatable}[Naturality of push]{prop}{pushNatural}
%   For all unary type constructors $\tcF$ such that $\deriv{push}{\tcF \alpha}$ is defined, and given a closed function term $f : \alpha \multimap \beta$, then: $ {\deriv{fmap}{\tcF} \Box_{r}f} \circ \deriv{push}{\tcF {\alpha}} = \deriv{push}{\tcF {\beta}} \circ  \Box_{r} \deriv{fmap}{\tcF} f $, i.e.:
% \begin{align*}
% \xymatrix@C=3.5em{
% {\alpha} \ar[d]_{f}
% \\
% {\beta} &
% }
% \quad
% \xymatrix@C=5em{
% {\Box_{r} \tcF {{\alpha}}} \ar[d]_{\Box_{r} \deriv{fmap}{\tcF } f } \ar[r]^{\deriv{push}{\tcF {\alpha}}}  &   {\tcF \Box_{r} {\alpha}}
% \ar[d]^{\deriv{fmap}{\tcF } \Box_{r}f}   \\
% {\Box_{r} \tcF {{\beta}}}  \ar[r]_{\deriv{push}{\tcF {\beta}}} &  {\tcF \Box_{r} {\beta}}}
% \end{align*}
% \end{restatable}

% \begin{restatable}[Naturality of pull]{prop}{pullNatural}
%   For all unary type constructors $\tcF$ such that $\deriv{pull}{\tcF \alpha}$ is defined, and given a closed function term $f : \alpha \multimap \beta$, then: $\Box_{r} \deriv{fmap}{\tcF} f \circ \deriv{pull}{\tcF {\alpha}} = \deriv{pull}{\tcF {\beta}} \circ {\deriv{fmap}{\tcF} \Box_{r}f}$, i.e.:
% \begin{align*}
% \xymatrix@C=3.5em{
% {\alpha} \ar[d]_{f}
% \\
% {\beta} &
% }
% \quad
% \xymatrix@C=5em{
% {\tcF \Box_{r} {\alpha}} \ar[d]_{\deriv{fmap}{\tcF}\Box_{r}f} \ar[r]^{\deriv{pull}{\tcF {\alpha}}}  & {\Box_{r} \tcF {{\alpha}}}
% \ar[d]^{ \Box_{r}\deriv{fmap}{\tcF} f}   \\
% { \tcF \Box_{r} {\beta}}  \ar[r]_{\deriv{pull}{\tcF {\beta}}} &  {\Box_{r} \tcF {{\beta}}}}
% \end{align*}
% \end{restatable}
%
% \noindent
% The appendix~\cite{appendix} gives the proofs.
% Note that the naturality results here are for cases of unary type
% constructors $\tcF$ that are covariant functors, written with a single parameter $\alpha$. This can
% easily generalise to $n$-ary type constructors.

% Not only is $\Box_r$ an endofunctor but it also has the structure
% of a \emph{graded comonad}~\cite{combining2016,DBLP:conf/fossacs/Katsumata18,petricek2014coeffects,DBLP:conf/icalp/PetricekOM13}.
% %

% \begin{definition}[Graded comonadic operations]
% The \grminip{} calculus (and Granule) permits the derivation of graded
% comonadic
% operations~\cite{DBLP:journals/pacmpl/OrchardLE19}
% for the semiring graded necessity $\Box_r$, defined:
% %
% \begin{align*}
% \varepsilon_A & : \Box_1 A \multimap A = \lambda x . [[ case x of [z] -> z ]] \\
% \delta_{r,s,A} & : \Box_{r \ast{} s} A \multimap \Box_r \Box_s A
% = \lambda x . [[ case x of [z] -> [ [z] ] ]]
%  \end{align*}
%  \end{definition}
% %
% \noindent
%  The derived distributive laws preserve these graded comonadic
%  operations i.e., the distributive laws are
%  well-behaved with respect to the graded comonadic structure of
%  $\Box_r$, captured by the following properties:

% \begin{restatable}[Push preserves graded comonads]{prop}{pushPreserve}
% %
% For all $\tcF$ such that $\deriv{push}{\tcF \overline{\alpha_i}}$ is defined
% and $\tcF$ does not contain $\multimap$ (to avoid issues of
% contravariance in $\tcF$) then:
% %
% \begin{align*}
% \xymatrix@C=3.5em@R=1.4em{
% \Box_{1} \tcF \overline{\alpha_i} \ar[r]^{\deriv{push}{\tcF
%   \overline{\alpha_i}}} \ar[d]_{\varepsilon}
% & \tcF \overline{\Box_{1} \alpha_i}
% \ar[dl]^{\tcF \varepsilon}
% \\
% \tcF \overline{\alpha_i} &
% }
% \quad
% \xymatrix@C=5em@R=1.4em{
% \Box_{r\ast{}s}
% \tcF \overline{\alpha_i} \ar[d]_{\delta_{r,s}} \ar[rr]^{\deriv{push}{\tcF \overline{\alpha_i}}}
% & & \tcF \overline{\Box_{r\ast{}s} \alpha_i} \ar[d]^{\tcF \delta_{r,s}}
% \\
% \Box_{r} \Box_{s} \tcF \overline{\alpha_i} \ar[r]_{\Box_r \deriv{push}{\tcF \overline{\alpha_i}}}&
% \Box_{r}\tcF \overline{\Box_{s} \alpha_i} \ar[r]_{\deriv{push}{\tcF \overline{\alpha_i}}} &
% \tcF \overline{\Box_{r} \Box_{s} \alpha_i}
% &
% }
% \end{align*}
% \end{restatable}

% \begin{restatable}[Pull preserves graded comonads]{prop}{pullPreserve}
% \label{prop:pullPreserve}
% %
% % TODO: DAO 23/10/2020
% % There might be a more general thing we can do here with taking the
% % meet of many grades, but it's too complicated for me to think about
% % right now!
% %
% For all $\tcF$ such that $\deriv{pull}{\tcF \overline{\alpha_i}}$ is defined then:
% %
% \begin{align*}
% \xymatrix@C=3.5em@R=1.4em{
% \Box_{1} \tcF \overline{\alpha_i} \ar[d]_{\varepsilon}
% &  \ar[l]_{\deriv{pull}{\tcF \overline{\alpha_i}}} \tcF
%   \overline{\Box_{1} \alpha_i}
% \ar[dl]^{\tcF \varepsilon}
% \\
% \tcF \overline{\alpha_i} &
% }
% \quad
% \xymatrix@C=5em@R=1.4em{
% \Box_{r\ast{}s}
% \tcF \overline{\alpha_i} \ar[d]_{\delta_{r,s}}
% & & \ar[ll]_{\deriv{pull}{\tcF \overline{\alpha_i}}} \tcF
%     \overline{\Box_{r\ast{}s} \alpha_i}
% \ar[d]^{\tcF \delta_{r,s}}
% \\
% \Box_{r} \Box_{s} \tcF \overline{\alpha_i} & \ar[l]^{\Box_r \deriv{pull}{\tcF \overline{\alpha_i}}}
% \Box_{r}\tcF \overline{\Box_{s} \alpha_i} & \ar[l]^{\deriv{pull}{\tcF \overline{\alpha_i}}}
% \tcF \overline{\Box_{r} \Box_{s} \alpha_i}
% &
% }
% \end{align*}
% \end{restatable}
% %
% \noindent
% The appendix~\cite{appendix} gives the proofs.

\section{Implementation in Granule}
\label{sec:implementation-der}
The Granule type checker implements the algorithmic derivation of \emph{push}
 and \emph{pull} distributive laws as covered in the previous section. Whilst the
 syntax of our language types had unit, sum, and product types as primitives, in
 Granule these are provided by a more general notion of type constructor which
 can be extended by user-defined, generalized algebraic data types (GADTs). The
 procedure outlined in Section~\ref{sec:push-pull-deriv} is therefore generalised
 slightly so that it can be applied to any data type: the case for $A \oplus B$
 is generalised to types with an arbitrary number of data constructors.
 
 Our deriving mechanism is exposed to programmers via explicit (visible) type
 application (akin to that provided in GHC Haskell~\citep{eisenberg2016visible})
 on reserved names \granin{push} and \granin{pull}. Written \granin{push @T} or
 \granin{pull @T}, this signals to the compiler that we wish to derive the
 corresponding distributive laws at the type \granin{T}. For example, for the
 \granin{List : Type -> Type} data type from the standard library, we can write
 the expression \granin{push @List} which the type checker recognises as a
 function of type:

 %
\begin{granule}
push @List : forall { a : Type, s : Semiring, r : s } 
           . {1 <= r} => (List a) [r] -> List (a [r])
\end{granule}
 %
 Note this function is not only polymorphic in the grade, but polymorphic in the
 semiring itself. Granule identifies different graded modalities by their
 semirings, and thus this operation is polymorphic in the graded modality. When
 the type checker encounters such a type application, it triggers the derivation
 procedure of Section~\ref{sec:push-pull-deriv}, which also calculates the type. The
 result is then stored in the state of the frontend to be passed to the
 interpreter (or compiler) after type checking. The derived operations are
 memoized so that they need not be re-calculated if a particular distributive
 law is required more than once. Otherwise, the implementation largely follows
 Section~\ref{sec:push-pull-deriv} without surprises, apart from some additional
 machinery for specialising the types of data constructors coming from
 (generalized) ADTs.
 
 \subsection{Examples}
 At the start of this chapter, we motivated the crux of this work with a concrete
 example, which we can replay here in concrete Granule, using its type
 application technique for triggering the automatic derivation of the
 distributive laws. Previously, we defined \granin{pushPair} by hand which can
 now be replaced with:
 %
\begin{granule}
push @(,) : forall { a b : Type, s : Semiring, r : s } 
          . (a, b) [r] -> (a [r], b [r])
\end{granule}
 %
 Note that in Granule \granin{(,)} is an infix type constructor for products as
 well as terms. We could then replace the previous \granin{fst'}
 with:
 %
\begin{granule}
fst' : forall { a b : Type, r : Semiring } 
     . {0 <= r} => (a, b) [r] -> a
fst' = let [x'] = fst (push @(,) x) in x'
\end{granule}
 %
 The point however in the example is that we need not even define this
 intermediate combinator, but can instead write the following
 wherever we need to compute the first projection
 of \granin{myPair : (a, b) [r]}:
 %
\begin{granule}
extract (fst (push @(,) myPair)) : a
\end{granule}
 %
 We already saw that we can then generalise this by applying
 this first projection inside of the list \\ \granin{myPairList : (List (a, b))
   [r]} directly, using \granin{push @List}.
 
 In a slightly more elaborate example, we can use the \granin{pull} combinator
 for pairs to implement a function that duplicates a pair (given that both elements
 can be consumed twice):
 %
\begin{granule}
copyPair : forall { a, b : Type } 
         . (a [0..2], b [2..4]) -> ((a, b), (a, b))
-- where, copy : a [2] -> (a, a)
copyPair x = copy (pull @(,) x) 
\end{granule}
 %
 Note \granin{pull} computes the greatest-lower
 bound of intervals \granin{0..2} and \granin{2..4} which is
 \granin{2..2}, i.e., we can provide a pair of \granin{a} and \granin{b}
 values which can each be used exactly twice: exactly what is required
 for \granin{copy}.
 
 As another example, interacting with Granule's indexed types
 (GADTs), consider a simple programming task of taking the head of a sized-list (vector)
 and duplicating it into a pair. The \granin{head} operation
 is typed: 
\begin{granule}
head : forall { a : Type, n : Nat } 
     . (Vec (n + 1) a) [0..1] -> a
\end{granule}
 which has a graded modal input with grade \granin{0..1} meaning
 the input vector is used 0 or 1 times:
 the head element is used once (linearly) for the return
 but the tail is discarded.
 
 This head element can then be copied via a graded
 modality, e.g., a value of type \granin{(Vec (n + 1) (a [2])) [0..1]} permits:
 %
\begin{granule}
copyHead' : forall { a : Type, n : Nat :} 
           . (Vec (n + 1) (a [2])) [0..1] -> (a, a)
-- [y] unboxes (a [2]) to y:a usable twice
copyHead' xs = let [y] = head xs in (y, y) 
\end{granule}
 %
 Here we ``unbox'' the graded modal value of type \granin{a [2]} to
 get a non-linear variable \granin{y} which we can use precisely twice.
 However, what if we are in a programming
 context where we have a value \granin{Vec (n + 1) a} with no
 graded modality on the type \granin{a}? %What requirements can we place
 %on such a value to explain we need to be able to discard the tail of
 %the vector and reuse the head twice?
 We can employ two idioms here:
 (i) take a value of type \granin{(Vec (n + 1) a) [0..2]} and
 split its modality in two: \granin{(Vec (n + 1) a) [2] [0..1]}
 (ii) then use \textit{push} on the inner graded modality
 \granin{[2]} to get
 \granin{(Vec (n + 1) (a [2])) [0..1]}.
 
 Using \granin{push @Vec} we can thus write the following to duplicate
 the head element of a vector:
 %
\begin{granule}
copyHead : forall { a : Type, n : Nat } 
         . (Vec (n + 1) a) [0..2] -> (a, a)
copyHead = copy . head . boxmap [push @Vec] . disject
\end{granule}
 %
 which employs combinators from the standard library and
 the derived distributive law, of type:
 %
\begin{granule}
boxmap    : forall { a b : Type, s : Semiring, r : s } 
          . (a  -> b) [r] -> a [r] -> b [r]
disject   : forall { a : Type, s : Semiring, n m : s }   
          . a [m * n] -> (a [n]) [m]
push @Vec : forall { a : Type, n : Nat, s : Semiring, r : s } 
          . (Vec n a) [r] -> Vec n (a [r])
\end{granule}
 %

\section{Deriving Other Useful Structural Combinators}
\label{sec:deriving-other}
So far we have motivated the use of distributive laws, and demonstrated that
they are useful in practice when programming in languages with linear and graded
modal types. The same methodology we have been discussing can also be used to
derive other useful generic combinators for programming with linear and graded
modal types. In this section, we consider two structural combinators,
\granin{drop} and \granin{copyShape}, in Granule as well as related type classes
for dropping, copying, and moving resources in Linear Haskell.

\subsection{A Combinator for Weakening (``drop'')}
\label{subsec:drop}

%First, we consider a combinator for ``dropping,'' or consuming, values in
%Granule.
%
The built-in type constants of Granule can be split into those which permit
structural weakening $C^{\mathsf{w}}$ such as \granin{Int}, \granin{Char}, and those which do not $C^{\mathsf{l}}$ such as \granin{Handle}
(file handles) and \granin{Chan} (concurrent channels). 

\begin{figure*}
\begin{align*}
    \deriv{drop}{C^w}^\Sigma z & = \text{\granin{drop} $z$} \\
    \deriv{drop}{1}^\Sigma z & = [[ case z of () -> () ]] \\
    \deriv{drop}{X}^\Sigma z & = \Sigma(X) z \\
    \deriv{drop}{A \oplus B}^\Sigma z & =
    \derCaseTwoShort{z}{\mathsf{inl}\ x}{\deriv{drop}{A}(x)}{\mathsf{inr}\ y}
                                {\deriv{drop}{B}(y)}\\
    \deriv{drop}{A \otimes B}^\Sigma z & =
    \derCaseOne{z}{(x, y)}
       {\\ & \hspace{3em} \derCaseOne{\deriv{drop}{A}(x)}{()}
         {\\ & \hspace{5em} \derCaseOne{\deriv{drop}{B}(y)}{()}{()}}}
    \\
    \deriv{drop}{\mu X . A}^\Sigma & z =
                                     \derLetRec{f}{\deriv{drop}{A}^{\Sigma,
                                     X \mapsto f : A \multimap 1}}{f\ z}
    \end{align*}
    \caption{Interpretation rules for $\deriv{drop}{A}$}
    \label{fig:drop}
\end{figure*}
Those that permit weakening contain non-abstract values that can in theory be
systematically inspected in order to consume them. Granule provides a built-in
implementation of \granin{drop} for $C^{\mathsf{w}}$ types, which is then used
by the derivation procedure of \ref{fig:drop} to derive weakening on compound
types. 

Note we cannot use this procedure in a polymorphic context (over type variables
$\alpha$) since type polymorphism ranges over all types, including those which
cannot be dropped like $C^{\mathsf{l}}$.

%\dnote{An interesting thing is that a function $[[ A -o B ]]$ is
%  droppable if you can 'pick' an element $x \in [[ A ]]$ (i.e., you
%  have a canonical way of generating one) and then that $[[ B ]]$ is
%  droppable. Not sure whether we can/should get into this here though?}

\subsection{A Combinator for Copying ``shape''}

The ``shape'' of values for a parametric data types $\tcF$ can be determined by
a function $\mathit{shape} : \tcF A \rightarrow \tcF 1$, usually derived when
$\tcF$ is a functor by mapping with $A \rightarrow 1$ (dropping
elements)~\citep{jay1994shapely}. This provides a way of capturing the size,
shape, and form of a data structure. Often when programming with data structures
which must be used linearly, we may wish to reason about properties of the data
structure (such as the length or ``shape'' of the structure) but we may not be
able to drop the contained values. Instead, we wish to extract the shape but
without consuming the original data structure itself.

This can be accomplished with a function which copies the data structure
exactly, returning this duplicate along with a data structure of the same shape,
but with the terminal nodes replaced with values of the unit type $1$  (the
`spine'). For example, consider a pair of integers: \granin{(1, 2)}. Then
applying \granin{copyShape} to this pair would yield \granin{(((), ()), (1,
2))}. The original input pair is duplicated and returned on the right of the
pair, while the left value contains a pair with the same structure as the input,
but with values replaced with \granin{()}. This is useful, as it allows us to
use the left value of the resulting pair to reason about the structure of the
input (e.g., its depth / size), while preserving the original input. This is
particularly useful for deriving size and length combinators for collection-like
data structures.
\noindent
As with ``drop'', we can derive such a
function automatically:
\begin{align*}
\deriv{copyShape}{\tcF \alpha} : \tcF \alpha \multimap \tcF 1 \otimes \tcF \alpha
\end{align*}
defined by
$\deriv{copyShape}{A} = \lambda z . \deriv{copyShape}{A}^\emptyset z$
by an intermediate interpretation $\deriv{copyShape}{A}^\Sigma$, given by Figure~\ref{fig:copyShape}.
%
\begin{figure}
\begin{align*}
\deriv{copyShape}{C^w}^\Sigma z & = ((),\ z) \\
\deriv{copyShape}{1}^\Sigma z & = \derCaseOne{z}{()}{((),\ ())}\\
\deriv{copyShape}{\alpha}^\Sigma z & = ((), z) \\
\deriv{copyShape}{X}^\Sigma z & = \Sigma(X) z \\
\deriv{copyShape}{A \oplus B}^\Sigma z & =
          \textbf{case}\ z\ \textbf{of} \\ 
              & \hspace{3em} \mathsf{inl}\ x \rightarrow 
              \textbf{case}\ {\llbracket A \rrbracket}^\Sigma_\mathsf{copyShape}(x)\ \textbf{of}\ (s,\ x') \rightarrow \\ 
              & \hspace{9em} (\mathsf{inl}\ s,\ \mathsf{inr}\ x') \\ 
              & \hspace{3em} \mathsf{inr}\ y \rightarrow  
              \textbf{case}\ {\llbracket B \rrbracket}^\Sigma_\mathsf{copyShape}(y)\ \textbf{of}\ (s,\ y') \rightarrow \\ 
              & \hspace{9em} (\mathsf{inr}\ s,\ \mathsf{inr}\ y') \\ 
            %                              \derCaseTwoMultiLine{z}
            %  {\mathsf{inl}\ x}
            %     {\textbf{case}\ {\llbracket A \rrbracket}^\Sigma_\mathsf{copyShape}(x)\ \textbf{of}\ (s,\ x') \rightarrow (\mathsf{inl}\ s,\ \mathsf{inr}\ x')} 
            %  {\mathsf{inr}\ y}
            %     {\textbf{case}\ {\llbracket B \rrbracket}^\Sigma_\mathsf{copyShape}(y)\ \textbf{of}\ (s,\ y') \rightarrow (\mathsf{inl}\ s,\ \mathsf{inr}\ y')} \\
\deriv{copyShape}{A \otimes B}^\Sigma z & =
\derCaseOne{z}{(x, y)}
   {     \\ & \hspace{3em} \derCaseOne{\llbracket A \rrbracket^\Sigma_\mathsf{copyShape}(x)}{ ( [[ s ]],\ [[x']] )}{
                                          \\ & \hspace{6em} \derCaseOne{\llbracket B \rrbracket^\Sigma_\mathsf{copyShape}(y)}{ \\ & \hspace{9em} ( [[ s' ]],\ [[y']] )}{
                                          (([[s]],\ [[s']]),\ ([[x']],\ [[y']]))}
                                          }}
\\
\deriv{copyShape}{\mu X . A}^\Sigma & z =
                                 \derLetRec{f}{\deriv{copyShape}{A}^{\Sigma,
                                 X \mapsto f : A \multimap 1 \otimes A}}{f\ z}
\end{align*}
\caption{Interpretation rules for $\deriv{copyShape}{A}$}
\label{fig:copyShape}
\end{figure}
%
The implementation recursively follows the structure of the type,
replicating the constructors, reaching the crucial case where a
polymorphically type $z : \alpha$ is mapped to $( (),\ z )$ in
the third equation.

Granule implements both these derived combinators in a similar
way to \emph{push}/\emph{pull} providing \granin{copyShape}
and \granin{drop} which can be derived for a type \granin{T} via type
application, e.g. \granin{drop @T : T -> ()} if it can be derived.
Otherwise, the type checker produces an error,
explaining why \granin{drop} is not derivable at type \granin{T}.

\section{Related Work}
\label{sec:der-related}
In this section we consider some of the wider related work as they relate to the
ideas presented in this chapter.
\subsection{Generic Programming Methodology}
The deriving mechanism for Granule is based on the methodology of generic
functional programming~\citep{hinze2000new}, where functions may be defined
generically for all possible data types in the language; generic functions are
defined inductively on the structure of the types. This technique has notably
been used before in Haskell, where there has been a strong interest in deriving
type class instances automatically. Particularly relevant to this work is the
work on generic deriving~\citep{generic-deriving}, which allows Haskell
programmers to automatically derive arbitrary class instances using standard
datatype-generic programming techniques as described above. 

\subsection{Non-graded Distributive Laws}
Distributive laws are standard components in abstract
mathematics. Distributive laws between categorical structures used for
modelling modalities (like monads and comonads) are well explored. For
example, \citet{brookes1993intensional} defined a categorical semantics using monads
combined with comonads via a distributive law capturing both
intensional and effectful aspects of a
program.~\citet{power2002combining} study in
detail different ways of combining comonads and monads via
distributive laws. Such distributive laws
have been applied in the programming languages literature, e.g., for
modelling streams of partial elements~\citep{uustalu2006essence}.

\subsection{Graded Distributive Laws}
Gaboardi et al. define families of graded distributive laws for graded monads
and comonads~\citep{combining2016}. They include the ability to interact the
grades, e.g., with operations such as $\Box_{\iota(r,f)} \Diamond_f A
\rightarrow \Diamond_{\kappa(r,f)} \Box_r A$ between a graded comonad $\Box_r$
and graded monad $\Diamond_f$ where $\iota$ and $\kappa$ capture information
about the distributive law in the grades. In comparison, our distributive laws
here are more prosaic since they involve only a graded comonad (semiring graded
necessity) distributed over a functor and vice versa. That said, the scheme of
Gaboardi et al. suggests that there might be interesting graded distributive
laws between $\Box_r$ and the indexed types, for example, $\Box_r
(\mathsf{Vec}\, n \, A) \rightarrow \mathsf{Vec}\, (r * n) \, (\Box_1 A)$ which
internally replicates a vector. However, it is less clear how useful such
combinators would be in general or how systematic their construction would be.
In contrast, the distributive laws explained here appear frequently and have a
straightforward uniform calculation.

We noted in Section~\ref{sec:push-pull-deriv} that neither of our distributive laws
can be derived over graded modalities themselves, i.e., we cannot derive
$\textit{push} : \Box_r \Box_s A \rightarrow \Box_s \Box_r A$. Such an operation
would itself be a distributive law between two graded modalities, which may have
further semantic and analysis consequences beyond the normal derivations here
for regular types. Exploring this is future work, for which the previous work on
graded distributive laws can provide a useful scheme for considering the
possibilities here. Furthermore, Granule has both graded comonads and graded
monads so there is scope for exploring possible graded distributive laws between
these in the future following Gaboardi et al.~\citep{combining2016}.

\section{Conclusion}
\label{sec:der-conclusion}

The work described here addresses the practical aspects of applying these
techniques in real-world programming. Our hope is that this aids the development
of the next generation of programming languages with rich type systems for
high-assurance programming.

The calculus of this chapter serves somewhat as a bridge between the systems in
Chapters~\ref{chapter:core} and~\ref{chapter:extended}. In the next chapter, we
continue on from Chapter~\ref{chapter:core}, presenting a synthesis calculus for
a fully graded typing calculus. This system, however, is significantly more
expressive than the calculus of Section~\ref{sec:linear-base-calculus},
incorporating ADTs, pattern matching, recursion, and  
polymorphism. 